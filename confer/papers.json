entities={

  "research506": {
    "title": "Distributed Join Algorithms on Thousands of Cores",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p517-barthels.pdf",
    "abstract": "Traditional database operators such as joins are relevant not only in the context of database engines but also as a building block in many computational and machine learning algorithms. With the advent of big data, there is an increasing demand for efficient join algorithms that can scale with the input data size and the available hardware resources. In this paper, we explore the implementation of distributed join algorithms in systems with several thousand cores connected by a low-latency network as used in high performance computing systems or data centers. We compare radix hash join to sort-merge join algorithms and discuss their implementation at this scale. In the paper, we explain how to use MPI to implement joins, show the impact and advantages of RDMA, discuss the importance of network scheduling, and study the relative performance of sorting vs. hashing. The experimental results show that the algorithms we present scale well with the number of cores, reaching a throughput of 48.7 billion input tuples per second on 4096 cores.",
    "subtype": "research",
    "authors": [
      {
        "name": "Claude Barthels",
        "affiliation": "ETH"
      },
      {
        "name": "Gustavo Alonso",
        "affiliation": "ETH"
      },
      {
        "name": "Torsten Hoefler",
        "affiliation": "ETH"
      },
      {
        "name": "Timo Schneider",
        "affiliation": "ETH"
      },
      {
        "name": "Ingo Müller",
        "affiliation": "ETH"
      }
    ],
    "type": "research",
    "id": "506"
  },
  "research512": {
    "title": "AdaptDB: Adaptive Partitioning for Distributed Joins",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p589-lu.pdf",
    "abstract": "Big data analytics often involves complex join queries over two or more tables. Such join processing is expensive in a distributed setting both because large amounts of data must be read from disk, and because of data shuffling across the network. Many techniques based on data partitioning have been proposed to reduce the amount of data that must be accessed, often focusing on finding the best partitioning scheme for a particular workload, rather than adapting to changes in the workload over time. In this paper, we present AdaptDB, an adaptive storage manager for analytical database workloads in a distributed setting. It works by partitioning datasets across a cluster and incrementally refining data partitioning as queries are run. AdaptDB introduces a novel hyper join that avoids expensive data shuffling by identifying storage blocks of the joining tables that overlap on the join attribute, and only joining those blocks. Hyper join performs well when each block in one table overlaps with few blocks in the other table, since that will minimize the number of blocks that have to be accessed. To minimize the number of overlapping blocks for common join queries, AdaptDB users smooth repartitioning to repartition small portions of the tables on join attributes as queries run. A prototype of AdaptDB running on top of Spark improves query performance by 2-3x on TPC-H as well as real-world dataset, versus a system that employs scans and shuffle-joins.",
    "subtype": "research",
    "authors": [
      {
        "name": "Yi Lu",
        "affiliation": "MIT"
      },
      {
        "name": "Anil Shanbhag",
        "affiliation": "MIT"
      },
      {
        "name": "Alekh Jindal",
        "affiliation": "Microsoft"
      },
      {
        "name": "Samuel Madden",
        "affiliation": "MIT"
      }
    ],
    "type": "research",
    "id": "512"
  },
  "research509": {
    "title": "An Evaluation of Distributed Concurrency Control",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p541-zheng.pdf",
    "abstract": "Increasing transaction volumes have led to a resurgence of interest in distributed transaction processing. In particular, partitioning data across several servers can improve throughput by allowing servers to process transactions in parallel. But executing transactions across servers limits the scalability and performance of these systems. In this paper, we quantify the effects of distribution on concurrency control protocols in a distributed environment. We evaluate six classic and modern protocols in an in-memory distributed database evaluation framework called Deneva, providing an apples-to-apples comparison between each. Our results expose severe limitations of distributed transaction processing engines. Moreover, in our analysis, we identify several protocol-specific scalability bottlenecks. We conclude that to achieve truly scalable operation, distributed concurrency control solutions must seek a tighter coupling with either novel network hardware (in the local area) or applications (via data modeling and semantically-aware execution), or both.",
    "subtype": "research",
    "authors": [
      {
        "name": "Rachael Harding",
        "affiliation": "MIT"
      },
      {
        "name": "Dana Van Aken",
        "affiliation": "CMU"
      },
      {
        "name": "Andrew Pavlo",
        "affiliation": "CMU"
      },
      {
        "name": "Michael Stonebraker",
        "affiliation": "MIT"
      }
    ],
    "type": "research",
    "id": "509"
  },
  "research572": {
    "title": "The End of a Myth: Distributed Transaction Can Scale",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p685-zamanian.pdf",
    "abstract": "The common wisdom is that distributed transactions do not scale. But what if distributed transactions could be made scalable using the next generation of networks and a redesign of distributed databases? There would no longer be a need for developers to worry about co-partitioning schemes to achieve decent performance. Application development would become easier as data placement would no longer determine how scalable an application is. Hardware provisioning would be simplified as the system administrator can expect a linear scale-out when adding more machines rather than some complex sub-linear function, which is highly application specific. In this paper, we present the design of our novel scalable database system NAM-DB and show that distributed transactions with the very common Snapshot Isolation guarantee can indeed scale using the next generation of RDMA-enabled network technology without any inherent bottlenecks. Our experiments with the TPC-C benchmark show that our system scales linearly to over 6.5 million new-order (14.5 million total) distributed transactions per second on 56 machines.",
    "subtype": "research",
    "authors": [
      {
        "name": "Erfan Zamanian",
        "affiliation": "Brown University"
      },
      {
        "name": "Carsten Binnig",
        "affiliation": "Brown University"
      },
      {
        "name": "Tim Kraska",
        "affiliation": "Brown University"
      },
      {
        "name": "Tim Harris",
        "affiliation": "Oracle Labs"
      }
    ],
    "type": "research",
    "id": "572"
  },
  "research609": {
    "title": "An Empirical Evaluation of In-Memory Multi-Version Concurrency Control ",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p781-Wu.pdf",
    "abstract": "Multi-version concurrency control (MVCC) is currently the most popular transaction management scheme in modern database management systems (DBMSs). Although MVCC was discovered in the late 1970s, it is used in almost every major relational DBMS released in the last decade. Maintaining multiple versions of data potentially increases parallelism without sacrificing serializability when processing transactions. But scaling MVCC in a multi-core and in-memory setting is non-trivial: when there are a large number of threads running in parallel, the synchronization overhead can outweigh the benefits of multi-versioning. To understand how MVCC perform when processing transactions in modern hardware settings, we conduct an extensive study of the scheme's four key design decisions: concurrency control protocol, version storage, garbage collection, and index management. We implemented state-of-the-art variants of all of these in an in-memory DBMS and evaluated them using OLTP workloads. Our analysis identifies the fundamental bottlenecks of each design choice.",
    "subtype": "research",
    "authors": [
      {
        "name": "Yingjun Wu",
        "affiliation": "NUS"
      },
      {
        "name": "Joy Arulraj",
        "affiliation": "CMU"
      },
      {
        "name": "Jiexi Lin",
        "affiliation": "CMU"
      },
      {
        "name": "Ran Xian",
        "affiliation": "CMU"
      },
      {
        "name": "Andrew Pavlo",
        "affiliation": "CMU"
      }
    ],
    "type": "research",
    "id": "609"
  },
  "research116": {
    "title": "Adaptive NUMA-aware data placement and task scheduling for analytical workloads in main-memory column-stores",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p37-psaroudakis.pdf",
    "abstract": "Complex event processing (CEP) matches patterns over a continuous stream of events to detect situations of interest. Yet, the definition of an event pattern that precisely characterises a particular situation is challenging: there are manifold dimensions to correlate events, including time windows and value predicates. In the presence of historic event data that is labelled with the situation to detect, event patterns can be learned automatically. To cope with the combinatorial explosion of pattern candidates, existing approaches work on a type-level and discover patterns based on predefined event abstractions, aka event types. Hence, discovery is limited to patterns of a fixed granularity and users face the burden to manually select appropriate event abstractions. We present IL-Miner, a system that discovers event patterns by genuinely working on the instance-level, not assuming a priori knowledge on event abstractions. In a multi-phase process, IL-Miner first identifies relevant abstractions for the construction of event patterns. The set of events explored for pattern discovery is thereby reduced, while still providing formal guarantees on correctness, minimality, and completeness of the discovery result. Experiments using real-world datasets from diverse domains show that IL-Miner discovers a much broader range of event patterns compared to the state-of-the-art in the field.",
    "subtype": "research",
    "authors": [
      {
        "name": "Iraklis Psaroudakis",
        "affiliation": "EPFL"
      },
      {
        "name": "Tobias Scheuer",
        "affiliation": "SAP SE"
      },
      {
        "name": "Norman May",
        "affiliation": "SAP SE"
      },
      {
        "name": "Abdelkader Sellami",
        "affiliation": "SAP SE"
      },
      {
        "name": "Anastasia Ailamaki",
        "affiliation": "EPFL"
      }
    ],
    "type": "research",
    "id": "116"
  },
  "research433": {
    "title": "Clay: Fine-Grained Adaptive Partitioning for General Database Schemas",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p445-serafini.pdf",
    "abstract": "Transaction processing database management systems (DBMSs) are critical for today's data-intensive applications because they enable an organization to quickly ingest and query new information. Many of these applications exceed the capabilities of a single server, and thus their database has to be deployed in a distributed DBMS. The key factor affecting such a system's performance is how the database is partitioned. If the database is partitioned incorrectly, the number of distributed transactions can be high. These transactions have to synchronize their operations over the network, which is considerably slower and leads to poor performance. Previous work on elastic database repartitioning has focused on a certain class of applications whose database schema can be represented in a hierarchical tree structure. But many applications cannot be partitioned in this manner, and thus are subject to distributed transactions that impede their performance and scalability. In this paper, we present a new on-line partitioning approach, called Clay, that supports both tree-based schemas and more complex general schemas with arbitrary foreign key relationships. Clay dynamically creates blocks of tuples to migrate among servers during repartitioning, placing no constraints on the schema but taking care to balance load and reduce the amount of data migrated. Clay achieves this goal by including in each block a set of hot tuples and other tuples co-accessed with these hot tuples. To evaluate our approach, we integrate Clay in a distributed, main-memory DBMS and show that it can generate partitioning schemes that enable the system to achieve up to 15x better throughput and 99% lower latency than existing approaches.",
    "subtype": "research",
    "authors": [
      {
        "name": "Marco Serafini",
        "affiliation": "Qatar Computing Research Institute"
      },
      {
        "name": "Rebecca Taft",
        "affiliation": "MIT"
      },
      {
        "name": "Aaron J. Elmore",
        "affiliation": "University of Chicago"
      },
      {
        "name": "Andrew Pavlo",
        "affiliation": "CMU"
      },
      {
        "name": "Ashraf Aboulnaga",
        "affiliation": "Qatar Computing Research Institute"
      },
      {
        "name": "Michael Stonebraker",
        "affiliation": "MIT"
      }
    ],
    "type": "research",
    "id": "433"
  },
  "research428": {
    "title": "Two Birds, One Stone: A Fast, yet Lightweight, Indexing Scheme for Modern Database Systems",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p385-yu.pdf",
    "abstract": "Classic database indexes (e.g., B+ Tree), though speed up queries, suffer from two main drawbacks: (1) An index usually yields 5 to 15 additional storage overhead which results in non-ignorable dollar cost in big data scenarios especially when deployed on modern storage devices. (2) Maintaining an index incurs high latency because the DBMS has to locate and update those index pages affected by the underlying table changes. This paper proposes Hippo a fast, yet scalable, database indexing approach. It significantly shrinks the index storage and mitigates maintenance overhead without compromising much on the query execution performance. Hippo stores disk page ranges instead of tuple pointers in the indexed table to reduce the storage space occupied by the index. It maintains simplified histograms that represent the data distribution and adopts a page grouping technique that groups contiguous pages into page ranges based on the similarity of their index key attribute distributions. When a query is issued, Hippo leverages the page ranges and histogram-based page summaries to recognize those pages such that their tuples are guaranteed not to satisfy the query predicates and inspects the remaining pages. Experiments based on real and synthetic datasets show that Hippo occupies up to two orders of magnitude less storage space than that of the B+ Tree while still achieving comparable query execution performance to that of the B+ Tree for 0.1 - 1 selectivity factors. Also, the experiments show that Hippo outperforms BRIN (Block Range Index) in executing queries with various selectivity factors. Furthermore, Hippo achieves up to three orders of magnitude less maintenance overhead and up to an order of magnitude higher throughput (for hybrid query/update workloads) than its counterparts.",
    "subtype": "research",
    "authors": [
      {
        "name": "Jia Yu",
        "affiliation": "Arizona State University"
      },
      {
        "name": "Mohamed Sarwat",
        "affiliation": "Arizona State University"
      }
    ],
    "type": "research",
    "id": "428"
  },
  "research602": {
    "title": "Adaptive Work Placement for Query Processing on Heterogeneous Computing Resources",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p733-karnagel.pdf",
    "abstract": "The hardware landscape is currently changing from homogeneous multi-core systems towards heterogeneous systems with many different computing units, each with their own characteristics. This trend is a great opportunity for database systems to increase the overall performance if the heterogeneous resources can be utilized efficiently. To achieve this, the main challenge is to place the right work on the right computing unit. Current approaches tackling this placement for query processing assume that data cardinalities of intermediate results can be correctly estimated. However, this assumption does not hold for complex queries. To overcome this problem, we propose an adaptive placement approach being independent of cardinality estimation of intermediate results. Our approach is incorporated in a novel adaptive placement sequence. Additionally, we implement our approach as an extensible virtualization layer, to demonstrate the broad applicability with multiple database systems. In our evaluation, we clearly show that our approach significantly improves OLAP query processing on heterogeneous hardware, while being adaptive enough to react to changing cardinalities of intermediate query results.",
    "subtype": "research",
    "authors": [
      {
        "name": "Tomas Karnagel",
        "affiliation": "TU Dresden"
      },
      {
        "name": "Dirk Habich",
        "affiliation": "TU Dresden"
      },
      {
        "name": "Wolfgang Lehner",
        "affiliation": "TU Dresden"
      }
    ],
    "type": "research",
    "id": "602"
  },
  "research415": {
    "title": "Write-Behind Logging",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p337-arulraj.pdf",
    "abstract": "The design of the logging and recovery components of database management systems (DBMSs) has always been influenced by the difference in the performance characteristics of volatile (DRAM) and non-volatile storage devices (HDD/SSDs). Until now, the key assumption has been that non-volatile storage is much slower than DRAM and only support block-oriented read/writes. But the arrival of new non-volatile memory (NVM) storage that is almost as fast as DRAM with fine-grained read/writes invalidates these previous design choices. This paper explores the changes that are required in a DBMS to leverage the unique properties of NVM in systems that still include volatile DRAM. We make the case for a new logging and recovery protocol, called write-behind logging, that enables a DBMS to recover nearly instantaneously from system failures. The key idea of our approach is that the DBMS logs what parts of the database have changed rather than how it was changed. Using this logging method, the DBMS can directly flush the changes to the database even before recording them in the log. Our evaluation using an in-memory DBMS shows that this protocol improves the transactional throughput by 1.3 times, reduces the recovery time by more than two orders of magnitude, and shrinks the storage footprint of the DBMS on NVM by 1.5 times. We also demonstrate that our logging protocol works seamlessly with standard replication schemes.",
    "subtype": "research",
    "authors": [
      {
        "name": "Joy Arulraj",
        "affiliation": "CMU"
      },
      {
        "name": "Matthew Perron",
        "affiliation": "CMU"
      },
      {
        "name": "Andrew Pavlo",
        "affiliation": "CMU"
      }
    ],
    "type": "research",
    "id": "415"
  },
  "research500": {
    "title": "An Experimental Comparison of Partitioning Strategies in Distributed Graph Processing",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p493-verma.pdf",
    "abstract": "In this paper, we study the problem of choosing among partitioning strategies in distributed graph processing systems. To this end, we evaluate and characterize both the performance and resource usage of different partitioning strategies under various popular distributed graph processing systems, applications, input graphs, and execution environments. Through our experiments, we found that no single partitioning strategy is the best fit for all situations, and that the choice of partitioning strategy has a significant effect on resource usage and application run-time. Our experiments demonstrate that the choice of partitioning strategy depends on (1) the degree distribution of input graph, (2) the type and duration of the application, and (3) the cluster size. Based on our results, we present rules of thumb to help users pick the best partitioning strategy for their particular use cases. We present results from each system, as well as from all partitioning strategies implemented in one common system (PowerLyra).",
    "subtype": "research",
    "authors": [
      {
        "name": "Shiv Verma",
        "affiliation": "University Of Illinois at Urbana-Champaign"
      },
      {
        "name": "Luke Leslie",
        "affiliation": "UIUC"
      },
      {
        "name": "Yosub Shin",
        "affiliation": "Samsara"
      },
      {
        "name": "Indranil Gupta",
        "affiliation": "UIUC"
      }
    ],
    "type": "research",
    "id": "500"
  },
  "research682": {
    "title": "Bridging the Gap between HPC and Big Data frameworks",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p901-anderson.pdf",
    "abstract": "Apache Spark is a popular framework for data analytics with attractive features such as fault tolerance and interoperability with the Hadoop ecosystem. Unfortunately, many analytics operations in Spark are an order of magnitude or more slower compared to native implementations written with high performance computing tools such as MPI. There is a need to bridge the performance gap while retaining the benefits of the Spark ecosystem such as availability, productivity, and fault tolerance. In this paper, we propose a system for integrating MPI with Spark and analyze the costs and benefits of doing so for four distributed graph and machine learning applications. We show that offloading computation to an MPI environment from within Spark provides 3.1-17.7x speedups on the four sparse applications, including all of the overheads. This opens up an easy avenue to reuse existing MPI libraries in Spark with little effort.",
    "subtype": "research",
    "authors": [
      {
        "name": "Michael Anderson",
        "affiliation": "Intel Labs"
      },
      {
        "name": "Shaden Smith",
        "affiliation": "University of Minnesota"
      },
      {
        "name": "Narayanan Sundaram",
        "affiliation": "Intel"
      },
      {
        "name": "Mihai Capotă",
        "affiliation": "Intel Labs"
      },
      {
        "name": "Zheguang Zhao",
        "affiliation": "Brown University"
      },
      {
        "name": "Subramanya Dulloor",
        "affiliation": "Intel Labs"
      },
      {
        "name": "Nadathur Satish",
        "affiliation": "Intel Labs"
      },
      {
        "name": "Theodore Willke",
        "affiliation": "Intel Labs"
      }
    ],
    "type": "research",
    "id": "682"
  },
  "industrial1091": {
    "title": "SAP HANA Adoption of Non-Volatile Memory",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p1754-andrei.pdf",
    "abstract": "Non-Volatile RAM (NVRAM) is a novel class of hardware technology which is an interesting blend of two storage paradigms: byte-addressable DRAM and block-addressable storage (e.g. HDD/SSD). Most of the existing enterprise relational data management systems such as SAP HANA have their internal architecture based on the inherent assumption that memory is volatile and base their persistence on explicit handling of block-oriented storage devices. In this paper, we present the early adoption of Non-Volatile Memory within the SAP HANA Database, from the architectural and technical angles. We discuss our architectural choices, dive deeper into a few challenges of the NVRAM integration and their solutions, and share our experimental results. As we present our solutions for the NVRAM integration, we also give, as a basis, a detailed description of the relevant HANA internals.",
    "subtype": "industrial",
    "authors": [
      {
        "name": "Mihnea Andrei",
        "affiliation": "SAP"
      },
      {
        "name": "Christian Lemke",
        "affiliation": "SAP"
      },
      {
        "name": "Günter Radestock",
        "affiliation": "SAP"
      },
      {
        "name": "Robert Schulze",
        "affiliation": "SAP"
      },
      {
        "name": "Carsten Thiel",
        "affiliation": "SAP"
      },
      {
        "name": "Rolando Blanco",
        "affiliation": "SAP"
      },
      {
        "name": "Akanksha Meghlan",
        "affiliation": "SAP"
      },
      {
        "name": "Muhammad Sharique",
        "affiliation": "SAP"
      },
      {
        "name": "Sebastian Seifert",
        "affiliation": "SAP"
      },
      {
        "name": "Surendra Vishnoi",
        "affiliation": "SAP"
      },
      {
        "name": "Daniel Booss",
        "affiliation": "SAP SE"
      },
      {
        "name": "Thomas Peh",
        "affiliation": "SAP"
      },
      {
        "name": "Ivan Schreter",
        "affiliation": "SAP"
      },
      {
        "name": "Werner Thesing",
        "affiliation": "SAP"
      },
      {
        "name": "Mehul Wagle",
        "affiliation": "SAP"
      },
      {
        "name": "Thomas Willhalm",
        "affiliation": "Intel Deutschland GmbH"
      }
    ],
    "type": "industrial",
    "id": "1091"
  },
  "research1380": {
    "title": "Fast Scans on Key-Value Stores",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p1526-bocksrocker.pdf",
    "abstract": "Key-Value Stores (KVS) are becoming increasingly popular because they scale up and down elastically, sustain high throughputs for get/put workloads and have low latencies. KVS owe these advantages to their simplicity. This simplicity, however, comes at a cost: It is expensive to process complex, analytical queries on top of a KVS because today's generation of KVS does not support an efficient way to scan the data. The problem is that there are conflicting goals when designing a KVS for analytical queries and for simple get/put workloads: Analytical queries require high locality and a compact representation of data whereas elastic get/put workloads require sparse indexes. This paper shows that it is possible to have it all, with reasonable compromises. We studied the KVS design space and built TellStore, a distributed KVS, that performs almost as well as state-of-the-art KVS for get/put workloads and orders of magnitude better for analytical and mixed workloads. This paper presents the results of comprehensive experiments with an extended version of the YCSB benchmark and a workload from the telecommunication industry.",
    "subtype": "research",
    "authors": [
      {
        "name": "Markus Pilman",
        "affiliation": "ETH"
      },
      {
        "name": "Kevin Bocksrocker",
        "affiliation": "Microsoft"
      },
      {
        "name": "Lucas Braun",
        "affiliation": "ETH"
      },
      {
        "name": "Renato Marroquín",
        "affiliation": "ETH"
      },
      {
        "name": "Donald Kossmann",
        "affiliation": "ETH"
      }
    ],
    "type": "research",
    "id": "1380"
  },
  "research1069": {
    "title": "OrpheusDB: Bolt-on Versioning for Relational Databases",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p1130-huang.pdf",
    "abstract": "Data science teams often collaboratively analyze datasets, generating dataset versions at each stage of iterative exploration and analysis. There is a pressing need for a system that can support dataset versioning, enabling such teams to efficiently store, track, and query across dataset versions. We introduce OrpheusDB, a dataset version control system that “bolts on” versioning capabilities to a traditional relational database system, thereby gaining the analytics capabilities of the database “for free”. We develop and evaluate multiple data models for representing versioned data, as well as a light-weight partitioning scheme, LyreSplit, to further optimize the models for reduced query latencies. With LyreSplit, OrpheusDB is on average 1000x faster in finding effective (and better) partitionings than competing approaches, while also reducing the latency of version retrieval by up to 20x relative to schemes without partitioning. LyreSplit can be applied in an online fashion as new versions are added, alongside an intelligent migration scheme that reduces migration time by 10x on average.",
    "subtype": "research",
    "authors": [
      {
        "name": "Silu Huang",
        "affiliation": "UIUC"
      },
      {
        "name": "Liqi Xu",
        "affiliation": "UIUC"
      },
      {
        "name": "Jialin Liu",
        "affiliation": "Peking University"
      },
      {
        "name": "Aaron J. Elmore",
        "affiliation": "University of Chicago"
      },
      {
        "name": "Aditya Parameswaran",
        "affiliation": "UIUC"
      }
    ],
    "type": "research",
    "id": "1069"
  },
  "research1278": {
    "title": "Memory Management Techniques for Large-Scale Persistent-Main-Memory Systems",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p1166-oukid.pdf",
    "abstract": "Storage Class Memory (SCM) is a novel class of memory technologies that promise to revolutionize database architectures. SCMis byte-addressable and exhibits latencies similar to those of DRAM, while being non-volatile. Hence, SCM could replace both main memory and storage, enabling a novel single-level database architecture without the traditional I/O bottleneck. Fail-safe persistent SCM allocation can be considered conditio sine qua non for enabling this novel architecture paradigm for database management systems. In this paper we present PAllocator, a fail-safe persistent SCM allocator whose design emphasizes high concurrency and capacity scalability. Contrary to previous works, PAllocator thoroughly addresses the important challenge of persistent memory fragmentation by implementing an efficient defragmentation algorithm. We show that PAllocator outperforms state-of-the-art persistent allocators by up to one order of magnitude, both in operation throughput and recovery time, and enables up to 2.39x higher operation throughput on a persistent B-Tree.",
    "subtype": "research",
    "authors": [
      {
        "name": "Ismail Oukid",
        "affiliation": "TU Dresden & SAP SE"
      },
      {
        "name": "Daniel Booss",
        "affiliation": "SAP SE"
      },
      {
        "name": "Adrien Lespinasse",
        "affiliation": "Independent"
      },
      {
        "name": "Wolfgang Lehner",
        "affiliation": "TU Dresden"
      },
      {
        "name": "Thomas Willhalm",
        "affiliation": "Intel Deutschland GmbH"
      },
      {
        "name": "Grégoire Gomes",
        "affiliation": "Grenoble INP - Ensimag"
      }
    ],
    "type": "research",
    "id": "1278"
  },
  "research1286": {
    "title": "Caribou: Intelligent Distributed Storage",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p1202-istvan.pdf",
    "abstract": "Storage in data centers is evolving very quickly: several technologies are used simultaneously, the boundaries between main-memory and persistent storage are becoming blurred, persistent storage no longer has a block-based interface, it can handle random access at high rates, and is often exposed over the network. Yet, the ever increasing amount of data being handled causes an intrinsic inefficiency: moving data around is expensive in terms of bandwidth, latency, and power consumption, especially given the low computational complexity of many database operations. In this paper we explore near-data processing in database engines, i.e., the option of offloading part of the computation directly to the storage nodes. We implement our ideas in Caribou, an intelligent distributed storage layer incorporating many of the lessons learned while building systems with specialized hardware. Caribou provides access to DRAM/NVRAM storage over the network through a simple key-value store interface, with each storage node providing high-bandwidth near-data processing at line rate and fault tolerance through transparent replication. The result is a highly efficient data processing component that can be used not only to boost performance but also to reduce power consumption and real estate usage in the data center thanks to the micro-server architecture adopted.",
    "subtype": "research",
    "authors": [
      {
        "name": "Zsolt Istvan",
        "affiliation": "ETH"
      },
      {
        "name": "David Sidler",
        "affiliation": "ETH"
      },
      {
        "name": "Gustavo Alonso",
        "affiliation": "ETH"
      }
    ],
    "type": "research",
    "id": "1286"
  },
  "research423": {
    "title": "The TileDB Array Data Storage Manager",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p349-papadopoulos.pdf",
    "abstract": "We present a novel storage manager for multi-dimensional arrays that arise in scientific applications, which is part of a larger scientific data management system called TileDB. In contrast to existing solutions, TileDB is optimized for both dense and sparse arrays. Its key idea is to organize array elements into ordered collections called fragments. Each fragment is dense or sparse, and groups contiguous array elements into data tiles of fixed capacity. The organization into fragments turns random writes into sequential writes, and, coupled with a novel read algorithm, leads to very efficient reads. TileDB enables parallelization via multi-threading and multi- processing, offering thread-/process-safety and atomicity via lightweight locking. We show that TileDB delivers comparable performance to the HDF5 dense array storage manager, while providing much faster random writes. We also show that TileDB offers substantially faster reads and writes than the SciDB array database system with both dense and sparse arrays. Finally, we demonstrate that TileDB is considerably faster than adaptations of the Vertica relational column-store for dense array storage management, and at least as fast for the case of sparse arrays.",
    "subtype": "research",
    "authors": [
      {
        "name": "Stavros Papadopoulos",
        "affiliation": "Intel Labs and MIT"
      },
      {
        "name": "Kushal Datta",
        "affiliation": "Intel Corporation"
      },
      {
        "name": "Samuel Madden",
        "affiliation": "MIT"
      },
      {
        "name": "Timothy Mattson",
        "affiliation": "Intel Labs"
      }
    ],
    "type": "research",
    "id": "423"
  },
  "research679": {
    "title": "Automatic Algorithm Transformation for Efficient Multi-Snapshot Analytics on Temporal Graphs",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p877-then.pdf",
    "abstract": "Analytical graph algorithms commonly compute metrics for a graph at one point in time. In practice it is often also of interest how metrics change over time, e.g., to find trends. For this purpose, algorithms must be executed for multiple graph snapshots. We present Single Algorithm Multiple Snapshots (SAMS), a novel approach to execute algorithms concurrently for multiple graph snapshots. SAMS automatically transforms graph algorithms to leverage similarities between the analyzed graph snapshots. The automatic transformation interleaves algorithm executions on multiple snapshots, synergistically shares their graph accesses and traversals, and optimizes the algorithm's data layout. Thus, SAMS can amortize the cost of random data accesses and improve memory bandwidth utilization---two main cost factors in graph analytics. We extensively evaluate SAMS using six well-known algorithms and multiple synthetic as well as real-world graph datasets. Our measurements show that in multi-snapshot analyses, SAMS offers runtime improvements of up to two orders of magnitude over traditional snapshot-at-a-time execution.",
    "subtype": "research",
    "authors": [
      {
        "name": "Manuel Then",
        "affiliation": "TUM"
      },
      {
        "name": "Timo Kersten",
        "affiliation": "TUM"
      },
      {
        "name": "Stephan Guennemann",
        "affiliation": "TUM"
      },
      {
        "name": "Alfons Kemper",
        "affiliation": "TUM"
      },
      {
        "name": "Thomas Neumann",
        "affiliation": "TUM"
      }
    ],
    "type": "research",
    "id": "679"
  },
  "research1398": {
    "title": "HippogriffDB: Balancing I/O and GPU Bandwidth in Big Data Analytics",
    "acm_link": "http://www.vldb.org/pvldb/vol9/p1647-li.pdf",
    "abstract": "As data sets grow and conventional processor performance scaling slows, data analytics move towards heterogeneous architectures that incorporate hardware accelerators (notably GPUs) to continue scaling performance. However, existing GPU-based databases fail to deal with big data applications efficiently: the limited memory capacity of GPUs hinders the scalability of existing GPU-accelerated databases; existing systems fail to consider the discrepancy between fast GPUs and slow storage, which can counteract the benefit of GPU accelerators. In this paper, we propose HippogriffDB, an efficient, scalable GPU-accelerated OLAP system. It tackles the bandwidth discrepancy using data compression and direct data transfer path. HippogriffDB stores tables in compressed format and uses the GPU for decompression, trading GPU cycles for improved I/O bandwidth. To improve data transfer efficiency, HippogriffDB introduces a peer-to-peer, multi-threaded data transfer mechanism, directly transferring data from the SSD to the GPU. Focusing on star schema queries, HippogriffDB adopts a query-over-block execution model that provides scalability using a stream-based query execution model. The model helps increase kernel efficiency with operator fusion and double buffering mechanism. We have implemented HippogriffDB using an NVMe SSD, which connects directly to a commercial GPU. Results on two popular benchmarks demonstrate the scalability and efficiency of HippogriffDB. HippogriffDB outperforms existing GPU-based databases (YDB) and in-memory data analytics (MonetDB) by 1-2 orders of magnitude.",
    "subtype": "research",
    "authors": [
      {
        "name": "Jing Li",
        "affiliation": "UCSD"
      },
      {
        "name": "Hung-Wei Tseng",
        "affiliation": "UCSD"
      },
      {
        "name": "Chunbin Lin",
        "affiliation": "UCSD"
      },
      {
        "name": "Steven Swanson",
        "affiliation": "UCSD"
      },
      {
        "name": "Yannis Papakonstantinou",
        "affiliation": "UCSD"
      }
    ],
    "type": "research",
    "id": "1398"
  },
  "research1403": {
    "title": "Voodoo - A Vector Algebra for Portable Database Performance on Modern Hardware",
    "acm_link": "http://www.vldb.org/pvldb/vol9/p1707-pirk.pdf",
    "abstract": "In-memory databases require careful tuning and many engineering tricks to achieve good performance. This kind of database performance engineering is hard: a plethora of both data and hardware-dependent optimization techniques create a design space that is hard for a skilled database engineer – much less a query compiler – to navigate. To facilitate performance-oriented design exploration and query plan compilation, we propose and present Voodoo, a declarative intermediate algebra that abstracts the detailed architectural properties of the hardware, such as multi- or many-core architectures, hierarchical caches and SIMD registers, without losing the ability generate highly tuned code. Because consists of a collection of declarative, vector-oriented operations, Voodoo is easier to reason about and tune than low-level C and related hardware-focused extensions (i.e., Vector Intrinsics, OpenCL, CUDA, etc.). This enables our Voodoo compiler (which actually generates OpenCL) to produce code that rivals and even outperforms the fastest state-of-the-art in memory databases for both GPUs and CPUs. In addition, Voodoo makes it possible to to express techniques as diverse as cache-conscious processing, predication and SIMD (again on both GPUs and CPUs) to be expressed with just a few lines of code. Central to our approach is a novel idea we call control vectors, which allow programmers to expose parallelism to the Voodoo compiler in a highly abstracted manner that is largely agnostic to the targeted hardware, enabling portable performance across different hardware platforms. We used Voodoo to build an alternative backend for MonetDB, a popular open-source in-memory database. Our backend allows MonetDB to perform at the same level as highly tuned in-memory databases, including HyPeR and Ocelot. We also demonstrate Voodoo’s usefulness when investigating hardware conscious tuning techniques, assessing their performance on different queries, devices and data.",
    "subtype": "research",
    "authors": [
      {
        "name": "Holger Pirk",
        "affiliation": "MIT"
      },
      {
        "name": "Oscar Moll",
        "affiliation": "MIT"
      },
      {
        "name": "Matei Zaharia",
        "affiliation": "MIT"
      },
      {
        "name": "Samuel Madden",
        "affiliation": "MIT"
      }
    ],
    "type": "research",
    "id": "1403"
  },
  "industrial910": {
    "title": "Statisticum: Data Statistics Management in SAP HANA",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p1658-nica.pdf",
    "abstract": "We introduce a new concept of leveraging traditional data statistics as dynamic data integrity constraints. These data statistics produce transient database constraints, which are valid as long as they can be proven to be consistent with the current data. We denote this type of data statistics by constraint data statistics, their properties needed for consistency checking by consistency metadata, and their implied integrity constraints by implied data statistics constraints (implied constraints for short). Implied constraints are valid integrity constraints which are powerful query optimization tools employed, just as traditional database constraints, in semantic query transformation (aka query reformulation), partition pruning, runtime optimization, and semi-join reduction, to name a few. To our knowledge, this is the _rst work introducing this novel and powerful concept of deriving implied integrity constraints from data statistics. We discuss theoretical aspects of the constraint data statistics concept and their integration into query processing. We present the current architecture of data statistics management in SAP HANA and detail how constraint data statistics are designed and integrated into this architecture. As an instantiation of this framework, we consider dynamic partition pruning for data aging scenarios. We discuss our current implementation for constraint data statistics objects in SAP HANA which can be used for dynamic partition pruning. We enumerate their properties and show how consistency checking for implied integrity constraints is supported in the data statistics architecture. Our experimental evaluations on the TPC-H benchmark and a real customer application confirm the effectiveness of the implied integrity constraints; (1) for 59% of TPC-H queries, constraint data statistics utilization results in pruning cold partitions and reducing memory consumption, and (2) we observe up to 3 orders of magnitude speed-up in query processing time, for a real customer running an S/4HANA application.",
    "subtype": "industrial",
    "authors": [
      {
        "name": "Anisoara Nica",
        "affiliation": "SAP"
      },
      {
        "name": "Reza Sherkat",
        "affiliation": "SAP"
      },
      {
        "name": "Mihnea Andrei",
        "affiliation": "SAP"
      },
      {
        "name": "Xun Chen",
        "affiliation": "SAP"
      },
      {
        "name": "Martin Heidel",
        "affiliation": "SAP"
      },
      {
        "name": "Christian Bensberg",
        "affiliation": "SAP"
      },
      {
        "name": "Heiko Gerwens",
        "affiliation": "SAP"
      }
    ],
    "type": "industrial",
    "id": "910"
  },
  "industrial1072": {
    "title": "PaxosStore: High-availability Storage Made Practical in WeChat",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p1730-lin.pdf",
    "abstract": "In this paper, we present PaxosStore, a high-availability storage system developed to support the comprehensive business of WeChat. It employs a combinational design in the storage layer to engage multiple storage engines constructed for different storage models. PaxosStore is characteristic of extracting the Paxos-based distributed consensus protocol as a middleware that is universally accessible to the underlying multi-model storage engines. This facilitates tuning, maintaining, scaling and extending the storage engines. According to our experience in engineering practice, to achieve a practical consistent read/write protocol is far more complex than its theory. To tackle such engineering complexity, we propose a layered design of the Paxos-based storage protocol stack, where PaxosLog, the key data structure used in the protocol, is devised to bridge the programming-oriented consistent read/write to the storage-oriented Paxos procedure. Additionally, we present optimizations based on Paxos that made fault-tolerance more efficient. Discussion throughout the paper primarily focuses on pragmatic solutions that could be insightful for building practical distributed storage systems.",
    "subtype": "industrial",
    "authors": [
      {
        "name": "Jianjun Zheng",
        "affiliation": "Tencent Inc."
      },
      {
        "name": "Qian Lin",
        "affiliation": "NUS"
      },
      {
        "name": "Jiatao Xu",
        "affiliation": "Tencent Inc."
      },
      {
        "name": "Cheng Wei",
        "affiliation": "Tencent Inc."
      },
      {
        "name": "Chuwei Zeng",
        "affiliation": "Tencent Inc."
      },
      {
        "name": "Pingan Yang",
        "affiliation": "Tencent Inc."
      },
      {
        "name": "Yunfan Zhang",
        "affiliation": "Tencent Inc."
      }
    ],
    "type": "industrial",
    "id": "1072"
  },
  "research1287": {
    "title": "Towards Linear Algebra over Normalized Data",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p1214-chen.pdf",
    "abstract": "Providing machine learning (ML) over relational data is a mainstream requirement for data analytics systems. While almost all the ML tools require the input data to be presented as a single table, many datasets are multi-table, which forces data scientists to join those tables first, leading to data redundancy and runtime waste. Recent works on factorized ML mitigate this issue for a few specific ML algorithms by pushing ML through joins. But their approaches require a manual rewrite of ML implementations. Such piecemeal methods create a massive development overhead when extending such ideas to other ML algorithms. In this paper, we show that it is possible to mitigate this overhead by leveraging a popular formal algebra to represent the computations of many ML algorithms: linear algebra. We introduce a new logical data type to represent normalized data and devise a framework of algebraic rewrite rules to convert a large set of linear algebra operations over denormalized data into operations over normalized data. We show how this enables us to automatically factorize several popular ML algorithms, thus unifying and generalizing several prior works. We prototype our framework in the popular ML environment R and an industrial R-over-RDBMS tool. Experiments with both synthetic and real normalized data show that our framework also yields significant speed-ups, up to 36x on real data.",
    "subtype": "research",
    "authors": [
      {
        "name": "Lingjiao Chen",
        "affiliation": "UW-Madison"
      },
      {
        "name": "Arun Kumar",
        "affiliation": "University of California"
      },
      {
        "name": "Jeffrey Naughton",
        "affiliation": "Google"
      },
      {
        "name": "Jignesh Patel",
        "affiliation": "UW-Madison"
      }
    ],
    "type": "research",
    "id": "1287"
  },
  "research335": {
    "title": "Fast In-Memory SQL Analytics on Typed Graphs",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p265-lin.pdf",
    "abstract": "We study a class of graph analytics SQL queries, which we call relationship queries. These queries involving aggregation, join, semijoin, intersection and selection are a wide superset of fixed-length graph reachability queries and of tree pattern queries. We present real-world OLAP scenarios, where efficient relationship queries are needed. However, row stores, column stores and graph databases are unacceptably slow in such OLAP scenarios. We propose a GQ-Fast database, which is an indexed database that roughly corresponds to efficient encoding of annotated adjacency lists that combines salient features of column-based organization, indexing and compression. GQ-Fast uses a bottom-up fully pipelined query execution model, which enables (a) aggressive compression (e.g., compressed bitmaps and Huffman) and (b) avoids intermediate results that consist of row IDs (which are typical in column databases). GQ-Fast compiles query plans into executable C++ source code. Besides achieving runtime efficiency, GQ-Fast also reduces main memory requirements because, unlike column databases, GQ-Fast selectively allows dense forms of compression including heavy-weight compressions, which do not support random access. We used GQ-Fast to accelerate queries for two OLAP dashboards in the biomedical field. GQ-Fast outperforms PostgreSQL by 2–4 orders of magnitude and MonetDB, Vertica and Neo4j by 1–3 orders of magnitude when all of them are running on RAM. Our experiments dissect GQ-Fast’s advantage between (i) the use of compiled code, (ii) the bottom-up pipelining execution strategy, and (iii) the use of dense structures. Other analysis and experiments show the space savings of GQ-Fast due to the appropriate use of compression methods. We also show that the runtime penalty incurred by the dense compression methods decreases as the number of CPU cores increases.",
    "subtype": "research",
    "authors": [
      {
        "name": "Chunbin Lin",
        "affiliation": "UCSD"
      },
      {
        "name": "Benjamin Mandel",
        "affiliation": "UCSD"
      },
      {
        "name": "Yannis Papakonstantinou",
        "affiliation": "UCSD"
      },
      {
        "name": "Matthias Springer",
        "affiliation": "UCSD"
      }
    ],
    "type": "research",
    "id": "335"
  },
  "research426": {
    "title": "DOCS: Domain-Aware Crowdsourcing System",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p361-zheng.pdf",
    "abstract": "Crowdsourcing is a new computing paradigm that harnesses human effort to solve computer-hard problems, such as entity resolution and photo tagging. The crowd (or workers) have diverse qualities and it is important to effectively model a worker's quality. Most of existing worker models assume that workers have the same quality on different tasks. In practice, however, tasks belong to a variety of diverse domains, and workers have different qualities on different domains. For example, a worker who is a basketball fan should have better quality for the task of labeling a photo related to 'Stephen Curry' than the one related to 'Leonardo DiCaprio'. In this paper, we study how to leverage domain knowledge to accurately model a worker's quality. We examine using knowledge base (KB), e.g., Wikipedia and Freebase, to detect the domains of tasks and workers. We develop Domain Vector Estimation, which analyzes the domains of a task with respect to the KB. We also study Truth Inference, which utilizes the domain-sensitive worker model to accurately infer the true answer of a task. We design an Online Task Assignment algorithm, which judiciously and efficiently assigns tasks to appropriate workers. To implement these solutions, we have built DOCS, a system deployed on the Amazon Mechanical Turk. Experiments show that DOCS performs much better than the state-of-the-art approaches.",
    "subtype": "research",
    "authors": [
      {
        "name": "Yudian Zheng",
        "affiliation": "Hong Kong University"
      },
      {
        "name": "Guoliang Li",
        "affiliation": "Tsinghua University"
      },
      {
        "name": "Reynold Cheng",
        "affiliation": "Hong Kong University"
      }
    ],
    "type": "research",
    "id": "426"
  },
  "research514": {
    "title": "High Performance Transactions via Early Write Visibility",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p613-faleiro.pdf",
    "abstract": "The overwhelming majority of existing concurrency control protocols make transactions' writes visible at the end of their execution. This delayed write visibility can significantly impact the performance of serializable protocols by reducing concurrency among conflicting transactions. This paper makes the observation that this delayed write visibility characteristic of existing protocols stem from an assumption that the database system within which they run may arbitrarily abort transactions at any point during their execution. Based on this observation, we propose a new serializable concurrency control protocol, piece-wise visibility (PWV), that is designed to run in a new class of database systems whose ability to abort transactions is deliberately constrained. PWV makes transactions' writes visible prior to the end of their execution, and consequently obtains significantly more concurrency than conventional serializable protocols. We evaluate PWV against state-of-the-art serializable protocols and a highly optimized implementation of read committed, and found that PWV can outperform serializable protocols by an order of magnitude and read committed by 3X on contentious workloads.",
    "subtype": "research",
    "authors": [
      {
        "name": "Jose Faleiro",
        "affiliation": "Yale University"
      },
      {
        "name": "Daniel Abadi",
        "affiliation": "Yale University"
      },
      {
        "name": "Joseph Hellerstein",
        "affiliation": "UC Berkeley"
      }
    ],
    "type": "research",
    "id": "514"
  },
  "research430": {
    "title": "PHyTM: Persistent Hybrid Transactional Memory",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p409-brown.pdf",
    "abstract": "Processors with hardware support for transactional memory (HTM) are rapidly becoming commonplace, and processor manufacturers are currently working on implementing support for upcoming non-volatile memory (NVM) technologies. The combination of HTM and NVM promises to be a natural choice for in-memory database synchronization. However, limitations on the size of hardware transactions and the lack of progress guarantees by modern HTM implementations prevent some applications from obtaining the full benefit of hardware transactional memory. In this paper, we propose a persistent hybrid TM algorithm called PHyTM for systems that support NVM and HTM. PHyTM allows hardware assisted ACID transactions to execute concurrently with pure software transactions, which allows applications to gain the benefit of persistent HTM while simultaneously accommodating unbounded transactions (with a high degree of concurrency). Experimental simulations demonstrate that PHyTM is fast and scalable for realistic workloads.",
    "subtype": "research",
    "authors": [
      {
        "name": "Hillel Avni",
        "affiliation": "Huawei"
      },
      {
        "name": "Trevor Brown",
        "affiliation": "University of Toronto"
      }
    ],
    "type": "research",
    "id": "430"
  },
  "industrial1041": {
    "title": "State Management in Apache Flink®: Consistent Stateful Distributed Stream Processing",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p1718-carbone.pdf",
    "abstract": "Stream processors are emerging in industry as an apparatus that drives analytical but also mission critical services handling the core of persistent application logic. Thus, apart from scalability and low-latency, a rising system need is first-class support for application state together with strong consistency guarantees, and adaptivity to cluster reconfigurations, software patches and partial failures. Although prior systems research has addressed some of these specific problems, the practical challenge lies on how such guarantees can be materialized in a transparent, non-intrusive manner that relieves the user from unnecessary constraints. Such needs served as the main design principles of state management in Apache Flink, an open source, scalable stream processor. We present Flink's core pipelined, in-flight mechanism which guarantees the creation of lightweight, consistent, distributed snapshots of application state, progressively, without impacting continuous execution. Consistent snapshots cover all needs for system reconfiguration, fault tolerance and version management through coarse grained rollback recovery. Application state is declared explicitly to the system, allowing efficient partitioning and transparent commits to persistent storage. We further present Flink's backend implementations and mechanisms for high availability, external state queries and output commit. Finally, we demonstrate how these mechanisms behave in practice with metrics and large-deployment insights exhibiting the low performance trade-offs of our approach and the general benefits of exploiting asynchrony in continuous, yet sustainable system deployments.",
    "subtype": "industrial",
    "authors": [
      {
        "name": "Paris Carbone",
        "affiliation": "KTH"
      },
      {
        "name": "Stephan Ewen",
        "affiliation": "Data Artisans"
      },
      {
        "name": "Gyula Fóra",
        "affiliation": "King Digital Entertainment Limited"
      },
      {
        "name": "Seif Haridi",
        "affiliation": "KTH"
      },
      {
        "name": "Stefan Richter",
        "affiliation": "Data Artisans"
      },
      {
        "name": "Kostas Tzoumas",
        "affiliation": "Data Artisans"
      }
    ],
    "type": "industrial",
    "id": "1041"
  },
  "industrial1218": {
    "title": "Dhalion:Self-Regulating Stream Processing in Heron",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p1825-floratou.pdf",
    "abstract": "In recent years, there has been an explosion of large-scale real-time analytics needs and a plethora of streaming systems have been developed to support such applications. These systems are able to continue stream processing even when faced with hardware and software failures. However, these systems do not address some crucial challenges facing their operators: the manual, time-consuming and error-prone tasks of tuning various configuration knobs to achieve service level objectives (SLO) as well as the maintenance of SLOs in the face of sudden, unpredictable load variation and hardware or software performance degradation. In this paper, we introduce the notion of self-regulating streaming systems and the key properties that they must satisfy. We then present the design and evaluation of Dhalion, a system that provides self-regulation capabilities to underlying streaming systems. We describe our implementation of the Dhalion framework on top of Twitter Heron, as well as a number of policies that automatically reconfigure Heron topologies to meet throughput SLOs, scaling resource consumption up and down as needed. We experimentally evaluate our Dhalion policies in a cloud environment and demonstrate their effectiveness. We are in the process of open-sourcing our Dhalion policies as part of the Heron project.",
    "subtype": "industrial",
    "authors": [
      {
        "name": "Avrilia Floratou",
        "affiliation": "Microsoft"
      },
      {
        "name": "Ashvin Agrawal",
        "affiliation": "Microsoft"
      },
      {
        "name": "Bill Graham",
        "affiliation": "Twitter"
      },
      {
        "name": "Sriram Rao",
        "affiliation": "Microsoft"
      },
      {
        "name": "Karthik Ramasamy",
        "affiliation": "Twitter"
      }
    ],
    "type": "industrial",
    "id": "1218"
  },
  "research139": {
    "title": "Mostly-Optimistic Concurrency Control for Highly Contended Dynamic Workloads on a Thousand Cores",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p49-wang.pdf",
    "abstract": "Future servers will be equipped with thousands of CPU cores and deep memory hierarchies. Traditional concurrency control (CC) schemes—both optimistic and pessimistic—slow down orders of magnitude in such environments for highly contended workloads. Optimistic CC (OCC) scales the best for workloads with few conflicts, but suffers from clobbered reads for high conflict workloads. Although pessimistic locking can protect reads, it floods cache-coherence backbones in deep memory hierarchies and can also cause numerous deadlock aborts. This paper proposes a new CC scheme, mostly-optimistic concurrency control (MOCC), to address these problems. MOCC achieves orders of magnitude higher performance for dynamic workloads on modern servers. The key objective of MOCC is to avoid clobbered reads for high conflict workloads, without any centralized mechanisms or heavyweight interthread communication. To satisfy such needs, we devise a native, cancellable reader-writer spinlock and a serializable protocol that can acquire, release and re-acquire locks in any order without expensive interthread communication. For low conflict workloads, MOCC maintains OCC’s high performance without taking read locks. Our experiments with high conflict YCSB workloads on a 288-core server reveal that MOCC performs 8× and 23× faster than OCC and pessimistic locking, respectively. It achieves 17 million TPS for TPC-C and more than 110 million TPS for YCSB without conflicts, 170× faster than pessimistic methods.",
    "subtype": "research",
    "authors": [
      {
        "name": "Tianzheng Wang",
        "affiliation": "University of Toronto"
      },
      {
        "name": "Hideaki Kimura",
        "affiliation": "Hewlett Packard Enterprise"
      }
    ],
    "type": "research",
    "id": "139"
  },
  "industrial778": {
    "title": "Parallel Replication across Formats in SAP HANA for Scaling Out Mixed OLTP/OLAP Workloads",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p1598-han.pdf",
    "abstract": "Modern in-memory database systems are facing the need of efficiently supporting mixed workloads of OLTP and OLAP. A conventional approach to this requirement is to rely on ETL-style, application-driven data replication between two very different OLTP and OLAP systems, sacrificing real-time reporting on operational data. An alternative approach is to run OLTP and OLAP workloads in a single machine, which eventually limits the maximum scalability of OLAP query performance. In order to tackle this challenging problem, we propose a novel database replication architecture called HANA Asynchronous Parallel Table Replication (ATR). ATR supports OLTP workloads in one primary machine, while it supports heavy OLAP workloads in replicas. Here, row-store formats can be used for OLTP transactions at the primary, while column-store formats are used for OLAP analytical queries at the replicas. ATR is designed to support elastic scalability of OLAP query performance while it minimizes the overhead for transaction processing at the primary and minimizes CPU consumption for replayed transactions at the replicas. ATR employs a novel optimistic lock-free parallel log replay scheme which exploits characteristics of multi-version concurrency control (MVCC) in order to enable real-time reporting by minimizing the propagation delay between the primary and replicas. It also supports adaptive query routing depending on its predefined max acceptable staleness range. Through extensive experiments with a concrete implementation available in a commercial product, we demonstrate that ATR achieves sub-second visibility delay even for update-intensive workloads, providing scalable OLAP performance without notable overhead to the primary.",
    "subtype": "industrial",
    "authors": [
      {
        "name": "Juchang Lee",
        "affiliation": "SAP Labs Korea"
      },
      {
        "name": "SeungHyun Moon",
        "affiliation": "POSTECH"
      },
      {
        "name": "Kyu Hwan Kim",
        "affiliation": "SAP Labs Korea"
      },
      {
        "name": "Deok Hoe Kim",
        "affiliation": "SAP Labs Korea"
      },
      {
        "name": "Sang Kyun Cha",
        "affiliation": "Seoul National University"
      },
      {
        "name": "Wook-Shin Han",
        "affiliation": "POSTECH"
      },
      {
        "name": "Chang Gyoo Park",
        "affiliation": "SAP Labs Korea"
      },
      {
        "name": "Hyoung Jun Na",
        "affiliation": "SAP Labs Korea"
      },
      {
        "name": "Joo Yeon Lee",
        "affiliation": "SAP Labs Korea"
      }
    ],
    "type": "industrial",
    "id": "778"
  },
  "research1293": {
    "title": "I’ve Seen “Enough”: Incrementally Improving Visualizations to Support Rapid Decision Making",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p1262-rahman.pdf",
    "abstract": "Data visualization is an effective mechanism for identifying trends, insights, and anomalies in data. On large datasets, however, generating visualizations can take a long time, delaying the extraction of insights, hampering decision making, and reducing exploration time. One solution is to use online sampling-based schemes to generate visualizations faster while improving the displayed estimates incrementally, eventually converging to the exact visualization computed on the entire data. However, the intermediate visualizations are approximate, and often fluctuate drastically, leading to potentially incorrect decisions. In this paper, we propose sampling-based incremental visualization algorithms that reveal the “salient” features of the eventual visualization quickly—with a 46× speedup relative to baselines—while minimizing error, thus enabling rapid and error-free decision making. We demonstrate that these algorithms are optimal in terms of sample complexity, in that given the level of interactivity, they generate approximations that take as few samples as possible to incrementally improve the accuracy. We have developed the algorithms in the context of an incremental visualization tool, titled INCVISAGE, for trendline and heatmap visualizations. We evaluate the usability of INCVISAGE via a full- fledged user study and demonstrate that users are able to make effective decisions with incrementally improving visualizations.",
    "subtype": "research",
    "authors": [
      {
        "name": "Sajjadur Rahman",
        "affiliation": "UIUC"
      },
      {
        "name": "Maryam Aliakbarpour",
        "affiliation": "MIT"
      },
      {
        "name": "Ha Kyung Kong",
        "affiliation": "UIUC"
      },
      {
        "name": "Eric Blais",
        "affiliation": "University of Waterloo"
      },
      {
        "name": "Karrie Karahalios",
        "affiliation": "UIUC"
      },
      {
        "name": "Aditya Parameswaran",
        "affiliation": "UIUC"
      },
      {
        "name": "Ronitt Rubinfeld",
        "affiliation": "MIT"
      }
    ],
    "type": "research",
    "id": "1293"
  },
  "research431": {
    "title": "Skipping-oriented Partitioning for Columnar Layouts",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p421-sun.pdf",
    "abstract": "As data volumes continue to grow, modern database systems increasingly rely on data skipping mechanisms to improve performance by avoiding access to irrelevant data. Recent work proposed a fine-grained partitioning scheme that was shown to improve the opportunities for data skipping in row-oriented systems. Modern analytics and big data systems increasingly adopt columnar storage schemes, and in such systems, a row-based approach misses important opportunities for further improving data skipping. The flexibility of column-oriented organizations, however, comes with the additional cost of tuple reconstruction. In this paper, we develop Generalized Skipping-Oriented Partitioning (GSOP), a novel hybrid data skipping framework that takes into account these row-based and column-based tradeoffs. In contrast to previous column-oriented physical design work, GSOP considers the tradeoffs between horizontal data skipping and vertical partitioning jointly. Our experiments using two public benchmarks and a real-world workload show that GSOP can significantly reduce the amount of data scanned and improve end-to-end query response times over the state-of-the-art techniques.",
    "subtype": "research",
    "authors": [
      {
        "name": "Liwen Sun",
        "affiliation": "UC Berkeley"
      },
      {
        "name": "Michael Franklin",
        "affiliation": "UC Berkeley"
      },
      {
        "name": "Jiannan Wang",
        "affiliation": "UC Berkeley"
      },
      {
        "name": "Eugene Wu",
        "affiliation": "Columbia University"
      }
    ],
    "type": "research",
    "id": "431"
  },
  "research932": {
    "title": "Slalom: Coasting Through Raw Data via Adaptive Partitioning and Indexing",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p1106-olma.pdf",
    "abstract": "The constant flux of data and queries alike has been pushing the boundaries of data analysis systems. The increasing size of raw data files has made data loading an expensive operation that delays the data-to-insight time. Hence, recent in-situ query processing systems operate directly over raw data, alleviating the loading cost. Another trend is the increasing number of queries. Each query, typically, focuses on a constantly shifting, yet small, range of the dataset. Minimizing the cumulative query latency of such a workload requires the benefits of indexing in in-situ query processing. In this paper we present an online partitioning and indexing scheme, along with a partitioning and indexing tuner tailored for in-situ querying engines. The proposed scheme improves query execution time by taking into account user query patterns, to (i) partition raw data files logically and (ii) build for each partition lightweight partition-specific indexes. We build Slalom, an in-situ query engine that follows the state of the art. Slalom accommodates workload shifts by updating partitioning and indexing decisions on-the-fly, based on query access patterns gathered by lightweight monitoring. Using this information Slalom builds non-obtrusive indexes. Due to its lightweight and adaptive nature, Slalom achieves efficient accesses to raw data with minimal memory consumption. Our experimentation with both micro-benchmarks and real-life workloads shows that Slalom outperforms state-of-the-art in-situ engines, and achieves comparable query response times with fully indexed DBMS, offering much lower cumulative query execution times for query workloads with increasing size and unpredictable access patterns.",
    "subtype": "research",
    "authors": [
      {
        "name": "Matthaios Olma",
        "affiliation": "EPFL"
      },
      {
        "name": "Manos Karpathiotakis",
        "affiliation": "EPFL"
      },
      {
        "name": "Ioannis Alagiannis",
        "affiliation": "Microsoft"
      },
      {
        "name": "Manos Athanassoulis",
        "affiliation": "Harvard University"
      },
      {
        "name": "Anastasia Ailamaki",
        "affiliation": "EPFL"
      }
    ],
    "type": "research",
    "id": "932"
  },
  "research435": {
    "title": "Effortless Data Exploration with zenvisage: An Expressive and Interactive Visual Analytics System",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p457-siddiqui.pdf",
    "abstract": "Data visualization is by far the most commonly used mechanism to explore and extract insights from datasets, especially by novice data scientists. And yet, current visual analytics tools are rather limited in their ability to operate on collections of visualizations - by composing, filtering, comparing, and sorting them - to find those that depict desired trends or patterns. The process of visual data exploration remains a tedious process of trial-and-error. We propose zenvisage, a visual analytics platform for effortlessly finding desired visual patterns from large datasets. We introduce general purpose visual exploration language, ZQL (zee-quel) for specifying the desired visual patterns, drawing from use-cases in a variety of domains, including biology, mechanical engineering, climate science, and commerce. We formalize the expressiveness of ZQL via a algebra on collections of visualizations - and demonstrate that ZQL is as expressive as that algebra. zenvisage exposes an interactive front-end that supports the issuing of ZQL queries, and also supports interactions that are short-cuts to certain commonly used ZQL queries. To execute these queries, zenvisage uses a novel ZQL graph-based query optimizer that leverages a suite of optimizations tailored to the goal of processing collections of visualizations in certain pre-defined ways. Lastly, a user survey and study demonstrates that data scientists are able to effectively use zenvisage to eliminate error-prone and tedious exploration and directly identify desired visualizations.",
    "subtype": "research",
    "authors": [
      {
        "name": "Tarique Ashraf Siddiqui",
        "affiliation": "UIUC"
      },
      {
        "name": "Albert Kim",
        "affiliation": "MIT"
      },
      {
        "name": "John Lee",
        "affiliation": "UIUC"
      },
      {
        "name": "Karrie Karahalios",
        "affiliation": "UIUC"
      },
      {
        "name": "Aditya Parameswaran",
        "affiliation": "UIUC"
      }
    ],
    "type": "research",
    "id": "435"
  },
  "research855": {
    "title": "Time Series Data Cleaning: From Anomaly Detection to Anomaly Repairing",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p1046-song.pdf",
    "abstract": "Errors are prevalent in time series data, such as GPS trajec- tories or sensor readings. Existing methods focus more on anomaly detection but not on repairing the detected anoma- lies. By simply filtering out the dirty data via anomaly detection, applications could still be unreliable over the in- complete time series. Instead of simply discarding anoma- lies, we propose to (iteratively) repair them in time series data, by creatively bonding the beauty of temporal nature in anomaly detection with the widely considered minimum change principle in data repairing. Our major contributions include: (1) a novel framework of iterative minimum re- pairing (IMR) over time series data, (2) explicit analysis on convergence of the proposed iterative minimum repairing, and (3) efficient estimation of parameters in each iteration. Remarkably, with incremental computation, we reduce the complexity of parameter estimation from O(n) to O(1). Ex- periments on real datasets demonstrate the superiority of our proposal compared to the state-of-the-art approaches. In particular, we show that (the proposed) repairing indeed improves the time series classification application.",
    "subtype": "research",
    "authors": [
      {
        "name": "Aoqian Zhang",
        "affiliation": "Tsinghua University"
      },
      {
        "name": "Shaoxu Song",
        "affiliation": "Tsinghua University"
      },
      {
        "name": "Jianmin Wang",
        "affiliation": "Tsinghua University"
      },
      {
        "name": "Philip Yu",
        "affiliation": "University of Illinois at Chicago"
      }
    ],
    "type": "research",
    "id": "855"
  },
  "research1310": {
    "title": "Query Optimization for Dynamic Imputation",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p1310-feser.pdf",
    "abstract": "Missing values are common in data analysis and present a usability challenge. Users are forced to pick between removing tuples with missing values or creating a cleaned version of their data by applying a relatively expensive imputation strategy. Our system, ImputeDB, incorporates imputation into a cost-based query optimizer, performing necessary imputations on-the-fly for each query. This allows users to immediately explore their data, while the system picks the optimal placement of imputation operations. We evaluate this approach on three real-world survey-based datasets. Our experiments show that our query plans execute between 10 and 140 times faster than first imputing the base tables. Furthermore, we show that the query results from on-the-fly imputation differ from the traditional base-table imputation approach by 0–20%. Finally, we show that while dropping tuples with missing values that fail query constraints discards 6–78% of the data, on-the-fly imputation loses only 0–21%.",
    "subtype": "research",
    "authors": [
      {
        "name": "Jose Cambronero Sanchez",
        "affiliation": "MIT"
      },
      {
        "name": "John Feser",
        "affiliation": "MIT"
      },
      {
        "name": "Micah Smith",
        "affiliation": "MIT"
      },
      {
        "name": "Samuel Madden",
        "affiliation": "MIT"
      }
    ],
    "type": "research",
    "id": "1310"
  },
  "research742": {
    "title": "Attribute-Driven Community Search",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p949-huang.pdf",
    "abstract": "Recently, community search over graphs has gained significant interest. In applications such as analysis of protein-protein interaction (PPI) networks, citation graphs, and collaboration networks, nodes tend to have attributes. Unfortunately, most previous community search algorithms ignore attributes and result in communities with poor cohesion w.r.t. their node attributes. In this paper, we study the problem of attribute-driven community search, that is, given an undirected graph $G$ where nodes are associated with attributes, and an input query $Q$ consisting of nodes $V_q$ and attributes $W_q$, find the communities containing $V_q$, in which most community members are densely inter-connected and have similar attributes. We formulate this problem as finding attributed truss communities (ATC), i.e., finding connected and close k-truss subgraphs containing $V_q$, with the largest attribute relevance score. We design a framework of desirable properties that good score function should satisfy. We show that the problem is NP-hard. However, we develop an efficient greedy algorithmic framework to iteratively remove nodes with the least popular attributes, and shrink the graph into an ATC. In addition, we also build an elegant index to maintain $k$-truss structure and attribute information, and propose efficient query processing algorithms. Extensive experiments on large real-world networks with ground-truth communities show that our algorithms significantly outperform the state of the art and demonstrates their efficiency and effectiveness.",
    "subtype": "research",
    "authors": [
      {
        "name": "Xin Huang",
        "affiliation": "Hong Kong Baptist University"
      },
      {
        "name": "Laks Lakshmanan",
        "affiliation": "UBC"
      }
    ],
    "type": "research",
    "id": "742"
  },
  "research1377": {
    "title": "DigitHist: a Histogram-Based Data Summary with Tight Error Bounds",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p1514-shekelyan.pdf",
    "abstract": "We propose DigitHist, a histogram summary for selectivity estimation on multidimensional data with tight error bounds. By combining multidimensional and one-dimensional histograms along regular grids of different resolutions, DigitHist provides an accurate and reliable histogram approach for multidimensional data. To achieve a compact summary, we use a sparse representation combined with a novel histogram compression technique that chooses a higher resolution in dense regions and a lower resolution elsewhere. For the construction of DigitHist, we propose a new error measure, termed u-error, which minimizes the width between the guaranteed upper and lower bounds of the selectivity estimate. The construction algorithm performs a single data scan and has linear time complexity. An in-depth experimental evaluation shows that DigitHist delivers superior precision and error bounds than state-of-the-art competitors at a comparable query time.",
    "subtype": "research",
    "authors": [
      {
        "name": "Michael Shekelyan",
        "affiliation": "Free University of Bozen-Bolzano"
      },
      {
        "name": "Anton Dignös",
        "affiliation": "Free University of Bozen-Bolzano"
      },
      {
        "name": "Johann Gamper",
        "affiliation": "Free University of Bozen-Bolzano"
      }
    ],
    "type": "research",
    "id": "1377"
  },
  "research1320": {
    "title": "A Forward Scan based Plane Sweep Algorithm for Parallel Interval Joins",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p1346-bouros.pdf",
    "abstract": "The interval join is a basic operation that finds application in temporal, spatial, and uncertain databases. Although a number of centralized and distributed algorithms have been proposed for the efficient evaluation of interval joins, classic plane sweep approaches have not been considered at their full potential. A recent piece of related work proposes an optimized approach based on plane sweep for modern hardware, showing that it greatly outperforms previous work. However, this approach depends on the development of a complex data structure and its parallelization has not been adequately studied. In this paper, we explore the applicability of a largely ignored forward scan (FS) based plane sweep algorithm, which is extremely simple to implement. We proposed two novel optimized versions of FS that greatly reduce its cost, making it competitive to the state-of-the-art single-threaded algorithm. In addition, we show the drawbacks of a previously proposed hash-based partitioning approach for parallel join processing and suggest a domain-based partitioning approach that does not produce duplicate results. Within our approach we propose a novel breakdown of the partition join jobs into a small number of independent mini-join jobs with varying cost and manage to avoid redundant comparisons. Finally, we show how these mini joins can be scheduled in a smaller number of CPU cores and propose an adaptive domain partitioning, aiming at load balancing. We include an experimental study that demonstrates the efficiency of our optimized FS and the scalability of our parallelization framework.",
    "subtype": "research",
    "authors": [
      {
        "name": "Panagiotis Bouros",
        "affiliation": "Aarhus University"
      },
      {
        "name": "Nikos Mamoulis",
        "affiliation": "Hong Kong University"
      }
    ],
    "type": "research",
    "id": "1320"
  },
  "research429": {
    "title": "History is a mirror to the future: Best-effort approximate complex event matching with insufficient resources",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p397-ge.pdf",
    "abstract": "Complex event processing (CEP) has proven to be a highly relevant topic in practice. As it is sensitive to both errors in the stream and uncertainty in the pattern, approximate complex event processing (ACEP) is an important direction but has not been adequately studied before. ACEP is costly, and is often performed under insufficient computing resources. We propose an algorithm that learns from the past behavior of ACEP runs, and makes decisions on what to process first in an online manner, so as to maximize the number of full matches found. In addition, we devise effective optimization techniques. Finally, we propose a mechanism that uses reinforcement learning to dynamically update the history structure without incurring much overhead. Put together, these techniques drastically improve the fraction of full matches found in resource constrained environments.",
    "subtype": "research",
    "authors": [
      {
        "name": "Zheng Li",
        "affiliation": "Oracle"
      },
      {
        "name": "Tingjian Ge",
        "affiliation": "University of Massachusetts Lowell"
      }
    ],
    "type": "research",
    "id": "429"
  },
  "research329": {
    "title": "HubPPR: Effective Indexing for Approximate Personalized PageRank",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p205-wang.pdf",
    "abstract": "{ Personalized PageRank (PPR)} computation is a fundamental operation in web search, social networks, and graph analysis. Given a graph $G$, a source $s$, and a target $t$, the PPR query $pi(s, t)$ returns the probability that a random walk on $G$ starting from $s$ terminates at $t$. Unlike global PageRank which can be effectively pre-computed and materialized, the PPR result depends on both the source and the target, rendering results materialization infeasible for large graphs. Existing indexing techniques have rather limited effectiveness; in fact, the current state-of-the-art solution, { BiPPR}, answers individual PPR queries without pre-computation or indexing, and yet it outperforms all previous index-based solutions. Motivated by this, we propose { HubPPR}, an effective indexing scheme for PPR computation with controllable tradeoffs for accuracy, query time, and memory consumption. The main idea is to pre-compute and index auxiliary information for selected hub nodes that are often involved in PPR processing. Going one step further, we extend HubPPR to answer top-$k$ PPR queries, which returns the $k$ nodes with the highest PPR values with respect to a source $s$, among a given set $T$ of target nodes. Extensive experiments demonstrate that compared to the current best solution { BiPPR}, { HubPPR} achieves up to 10x and 220x speedup for PPR and top-$k$ PPR processing, respectively, with moderate memory consumption. Notably, with a single commodity server, { HubPPR} answers a top-$k$ PPR query in seconds on graphs with billions of edges, with high accuracy and strong result quality guarantees.",
    "subtype": "research",
    "authors": [
      {
        "name": "Sibo Wang",
        "affiliation": "Nanyang Technological University"
      },
      {
        "name": "Youze Tang",
        "affiliation": "Nanyang Technological University"
      },
      {
        "name": "Xiaokui Xiao",
        "affiliation": "Nanyang Technological University"
      },
      {
        "name": "Yin Yang",
        "affiliation": "Hamad Bin Khalifa University"
      },
      {
        "name": "Zengxiang Li",
        "affiliation": "Institute of High Performance Computing"
      }
    ],
    "type": "research",
    "id": "329"
  },
  "research1375": {
    "title": "Runtime Optimization of Join Location in Parallel Data Management Systems",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p1490-chandra.pdf",
    "abstract": "Applications running on parallel systems often need to join a streaming relation or a stored relation with data indexed in a parallel data storage system. Some applications also compute UDFs on the joined tuples. The join can be done at the data storage nodes, corresponding to reduce side joins, or by fetching data from the storage system, corresponding to map side join. Both may be suboptimal: reduce side joins may cause skew, while map side joins may lead to a lot of data being transferred and replicated. In this paper, we present techniques to make runtime decisions between the two options on a per key basis, in order to improve the throughput of the join, accounting for UDF computation if any. Our techniques are based on an extended ski-rental algorithm and provide worst-case performance guarantees with respect to the optimal point in the space considered by us. Our techniques use load balancing taking into account the CPU, network and I/O costs as well as the load on clients and servers. We have implemented our techniques on Hadoop, Spark and the Muppet stream processing engine. Our experiments show that our optimization techniques provide a significant improvement in throughput over existing techniques.",
    "subtype": "research",
    "authors": [
      {
        "name": "Bikash Chandra",
        "affiliation": "IIT Bombay"
      },
      {
        "name": "S. Sudarshan",
        "affiliation": "IIT Bombay"
      }
    ],
    "type": "research",
    "id": "1375"
  },
  "research1381": {
    "title": "Finding the maximum clique in massive graphs",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p1538-lu.pdf",
    "abstract": "Cliques refer to subgraphs in an undirected graph such that vertices in each subgraph are pairwise adjacent. The maximum clique problem, to find the clique with most vertices in a given graph, has been extensively studied by researchers. Besides its theoretical value as an NP-hard problem, the maximum clique problem is known to have direct applications in various fields, such as community search in social networks and social media, team formation in expert networks, gene expression and motif discovery in bioinformatics and anomaly detection in complex networks, revealing the structure and function of networks. However, algorithms designed for the maximum clique problem are too expensive to deal with real-world networks, which are massive, sparse and associated with various characteristics. In this paper, we devise a randomized algorithm for the maximum clique problem. Different from previous algorithms that search from each vertex one after another, our approach RMC, for the randomized maximum clique problem, employs a binary search while maintains a lower bound CL and an upper bound CU of omega(G). In each iteration, RMC attempts to find a CT-clique where CT=(CL+CU)/2. As finding CT in each iteration is NP-complete, we extract a seed set S such that the problem of finding a CT-clique in G is equivalent to finding a CT-clique in S with robability guarantees (>= 1-n^{-c}). We propose a novel iterative algorithm to determine the maximum clique by searching a k-clique in S starting from k=CL+1 until S becomes empty, when more iterations benefit marginally. As confirmed by the experiments, our approach is much more efficient and robust than previous solutions and can always find the exact maximum clique.",
    "subtype": "research",
    "authors": [
      {
        "name": "Can Lu",
        "affiliation": "CUHK"
      },
      {
        "name": "Jeffrey Yu",
        "affiliation": "CUHK"
      },
      {
        "name": "Hao Wei",
        "affiliation": "CUHK"
      },
      {
        "name": "Yikai Zhang",
        "affiliation": "CUHK"
      }
    ],
    "type": "research",
    "id": "1381"
  },
  "research754": {
    "title": "Data Driven Approximation with Bounded Resources",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p973-cao.pdf",
    "abstract": "This paper proposes BEAS, a resource-bounded scheme for querying relations. It is parameterized with a resource ratio alpha in (0, 1], indicating that given a big dataset D, we can only afford to access an alpha-fraction of D with limited resources. For a query Q posed on D, BEAS computes exact answers Q(D) if it is doable with bounded resources; otherwise it returns approximate answers with an accuracy bound. In the entire process it accesses at most alpha*",
    "subtype": "research",
    "authors": [
      {
        "name": "Yang Cao",
        "affiliation": "University of Edinburgh"
      },
      {
        "name": "Wenfei Fan",
        "affiliation": "University of Edinburgh"
      }
    ],
    "type": "research",
    "id": "754"
  },
  "research738": {
    "title": "Leveraging Set Relations in Exact Set Similarity Join",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p925-wang.pdf",
    "abstract": "Exact set similarity join, which finds all the similar set pairs from two collections of sets, is a fundamental problem with a wide range of applications. The existing solutions for set similarity join follow a filtering-verification framework, which generates a list of candidate pairs through scanning indexes in the filtering phase, and reports those similar pairs in the verification phase. Though much research has been conducted on this problem, set relations, which we find out is quite effective on improving the algorithm efficiency through computational cost sharing, have never been studied. Therefore, in this paper, instead of considering each set individually, we explore the set relations in different levels to reduce the overall computational costs. First, it has been shown that most of the computational time is spent on the filtering phase, which can be quadratic to the number of sets in the worst case for the existing solutions. Thus we explore index-level set relations to reduce the filtering cost to be linear to the size of the input while keeping the same filtering power. We achieve this by grouping related sets into blocks in the index and skipping useless index probes in joins. Second, we explore answer-level set relations to further improve the algorithm based on the intuition that if two sets are similar, their answers may have a large overlap. We derive an algorithm which incrementally generates the answer of one set from an already computed answer of another similar set rather than compute the answer from scratch to reduce the computational cost. Finally, we conduct extensive performance studies using 21 real datasets with various data properties from a wide range of domains. The experimental results demonstrate that our algorithm outperforms all the existing algorithms across all datasets and can achieve more than an order of magnitude speedup against the state-of-the-art algorithms.",
    "subtype": "research",
    "authors": [
      {
        "name": "Xubo Wang",
        "affiliation": "CSE"
      },
      {
        "name": "Lu Qin",
        "affiliation": "QCIS"
      },
      {
        "name": "Xuemin Lin",
        "affiliation": "CSE"
      },
      {
        "name": "Ying Zhang",
        "affiliation": "QCIS"
      },
      {
        "name": "Lijun Chang",
        "affiliation": "CSE"
      }
    ],
    "type": "research",
    "id": "738"
  },
  "research1290": {
    "title": "Comparative Evaluation of Big-Data Systems on Scientific Image Analytics Workloads",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p1226-mehta.pdf",
    "abstract": "Scientific discoveries are increasingly driven by analyzing large volumes of image data. Many new libraries and specialized database management systems (DBMSs) have emerged to support such tasks. It is unclear, however, how well these systems support real-world image analysis use cases, and how performant are the image analytics tasks implemented on top of such systems. In this paper, we present the first comprehensive evaluation of large-scale image analysis systems using two real-world scientific image data processing use cases. We evaluate five representative systems (SciDB, Myria, Spark, Dask, and TensorFlow) and find that each of them has shortcomings that complicate implementation or hurt performance. Such shortcomings lead to new research opportunities in making large-scale image analysis both efficient and easy to use.",
    "subtype": "research",
    "authors": [
      {
        "name": "Parmita Mehta",
        "affiliation": "University of Washington"
      },
      {
        "name": "Sven Dorkenwald",
        "affiliation": "University of Washington"
      },
      {
        "name": "Dongfang Zhao",
        "affiliation": "University of Washington"
      },
      {
        "name": "Tomer Kaftan",
        "affiliation": "University of Washington"
      },
      {
        "name": "Alvin Cheung",
        "affiliation": "University of Washington"
      },
      {
        "name": "Magdalena Balazinska",
        "affiliation": "University of Washington"
      },
      {
        "name": "Ariel Rokem",
        "affiliation": "University of Washington"
      },
      {
        "name": "Andrew Connolly",
        "affiliation": "University of Washington"
      },
      {
        "name": "Jacob Vanderplas",
        "affiliation": "University of Washington"
      },
      {
        "name": "Yusra AlSayyad",
        "affiliation": "University of Washington"
      }
    ],
    "type": "research",
    "id": "1290"
  },
  "research130": {
    "title": "Cohort Query Processing",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p1-ooi.pdf",
    "abstract": "Non-uniform memory access (NUMA) architectures pose numerous performance challenges for main-memory column-stores in scaling up analytics on modern multi-socket multi-core servers. A NUMA-aware execution engine needs a strategy for data placement and task scheduling that prefers fast local memory accesses over remote memory accesses, and avoids an imbalance of resource utilization, both CPU and memory bandwidth, across sockets. State-of-the-art systems typically use a static strategy that always partitions data across sockets, and always allows inter-socket task stealing. In this paper, we show that adapting data placement and task stealing to the workload can improve throughput by up to a factor of 4 compared to a static approach. We focus on highly concurrent workloads dominated by operators working on a single table or table group (copartitioned tables). Our adaptive data placement algorithm tracks the resource utilization of tasks, partitions of tables and table groups, and sockets. When a utilization imbalance across sockets is detected, the algorithm corrects it by moving or repartitioning tables. Also, inter-socket task stealing is dynamically disabled for memory-intensive tasks that could otherwise hurt performance.",
    "subtype": "research",
    "authors": [
      {
        "name": "Dawei Jiang",
        "affiliation": "Zhejiang University"
      },
      {
        "name": "Qingchao Cai",
        "affiliation": "NUS"
      },
      {
        "name": "Gang Chen",
        "affiliation": "Zhejiang University"
      },
      {
        "name": "H. Jagadish",
        "affiliation": "University of Michigan"
      },
      {
        "name": "Beng Chin Ooi",
        "affiliation": "NUS"
      },
      {
        "name": "Kian-Lee Tan",
        "affiliation": "NUS"
      },
      {
        "name": "Anthony Tung",
        "affiliation": "NUS"
      }
    ],
    "type": "research",
    "id": "130"
  },
  "research314": {
    "title": "A Declarative Query Processing System for Nowcasting",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p145-antenucci.pdf",
    "abstract": "Nowcasting is the practice of using social media data to quantify ongoing real-world phenomena. It has been used by researchers to measure flu activity, unemployment behavior, and more. However, the typical nowcasting workflow requires either slow and tedious manual searching of relevant social media messages or automated statistical approaches that are prone to spurious and low-quality results. In this paper, we propose a method for declaratively specifying a nowcasting model; this method involves processing a user query over a very large social media database, which can take hours. Due to the human-in-the-loop nature of constructing nowcasting models, slow runtimes place an extreme burden on the user. Thus we also propose a novel set of query optimization techniques, which allow users to quickly construct nowcasting models over very large datasets. Further, we propose a novel query quality alarm that helps users estimate phenomena even when historical ground truth data is not available. These contributions allow us to build a declarative nowcasting data management system, RaccoonDB, which yields high-quality results in interactive time. We evaluate RaccoonDB using 40 billion tweets collected over five years. We show that our automated system saves work over traditional manual approaches while improving result quality---57% more accurate in our user study---and that its query optimizations yield a 424x speedup, allowing it to process queries 123x faster than a 300-core Spark cluster, using only 10% of the computational resources.",
    "subtype": "research",
    "authors": [
      {
        "name": "Dolan Antenucci",
        "affiliation": "University of Michigan"
      },
      {
        "name": "Michael Anderson",
        "affiliation": "University of Michigan"
      },
      {
        "name": "Michael Cafarella",
        "affiliation": "University of Michigan"
      }
    ],
    "type": "research",
    "id": "314"
  },
  "industrial875": {
    "title": "Samza: Stateful Scalable Stream Processing at LinkedIn",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p1634-noghabi.pdf",
    "abstract": "Distributed stream processing systems need to support stateful processing, recover quickly from failures to resume such processing, and reprocess an entire data stream quickly. We present Apache Samza, a distributed system for stateful and fault-tolerant stream processing. Samza utilizes a partitioned local state along with a low-overhead background changelog mechanism, allowing it to scale to massive state sizes (hundreds of TB) per application. Recovery from failures is sped up by re-scheduling based on Host Affinity. In addition to processing infinite streams of events, Samza supports processing a finite dataset as a stream, from either a streaming source (e.g., Kafka), a database snapshot (e.g., Databus), or a file system (e.g. HDFS), without having to change the application code (unlike the popular Lambda-based architectures which necessitate maintenance of separate code bases for batch and stream path processing). Samza is currently in use at LinkedIn by hundreds of production applications with more than 10,000 containers. Samza is an open-source Apache project adopted by many top-tier companies (e.g., LinkedIn, Uber, Netflix, TripAdvisor, etc.). Our experiments show that Samza: a) handles state efficiently, improving latency and throughput by more than 100x compared to using a remote storage; b) provides recovery time independent of state size; c) scales performance linearly with number of containers; and d) supports reprocessing of the data stream quickly and with minimal interference on real-time traffic.",
    "subtype": "industrial",
    "authors": [
      {
        "name": "Shadi A Noghabi",
        "affiliation": "University of Illinois at Urbana-Champaign"
      },
      {
        "name": "Kartik Paramasivam",
        "affiliation": "LinkedIn"
      },
      {
        "name": "Yi Pan",
        "affiliation": "LinkedIn"
      },
      {
        "name": "Navina Ramesh",
        "affiliation": "LinkedIn"
      },
      {
        "name": "Jon Bringhurst",
        "affiliation": "LinkedIn"
      },
      {
        "name": "Indranil Gupta",
        "affiliation": "UIUC"
      },
      {
        "name": "Roy Campbell",
        "affiliation": "University of Illinois at Urbana-Champaign"
      }
    ],
    "type": "industrial",
    "id": "875"
  },
  "research1321": {
    "title": "ASAP: Prioritizing Attention via Time Series Smoothing",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p1358-rong.pdf",
    "abstract": "Time series visualization of streaming telemetry (i.e., charting of key metrics such as server load over time) is increasingly prevalent in recent application deployments. Existing systems simply plot the raw data streams as they arrive, potentially obscuring large-scale deviations due to local variance and noise. We propose an alternative: to better prioritize attention in time series exploration and monitoring visualizations, smooth the time series as much as possible to remove noise and thus highlight long-term deviations. We develop a new technique for automatically smoothing streaming time series that adaptively optimizes this trade-off between noise reduction (i.e., variance) and outlier retention (i.e., kurtosis). We introduce metrics to quantitatively assess the quality of the choice of smoothing parameter and provide an efficient streaming analytics operator, ASAP, that optimizes these metrics by combining techniques from stream processing, user interface design, and signal processing via a novel autocorrelation-based pruning strategy and pixel-aware preaggregation. We demonstrate that ASAP is able to improve users' accuracy in identifying significant deviations in time series by up to 38.4% while reducing response times by up to 44.3%. Moreover, ASAP delivers these results several orders of magnitude faster than alternative optimization strategies.",
    "subtype": "research",
    "authors": [
      {
        "name": "Kexin Rong",
        "affiliation": "Stanford University"
      },
      {
        "name": "Peter Bailis",
        "affiliation": "Stanford University"
      }
    ],
    "type": "research",
    "id": "1321"
  },
  "research1283": {
    "title": "HoloClean: Holistic Data Repairs with Probabilistic Inference",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p1190-rekatsinas.pdf",
    "abstract": "We introduce HoloClean, a framework for holistic data repairing driven by probabilistic inference. HoloClean unifies existing qualitative data repairing approaches, which rely on integrity constraints or external data sources, with quantitative data repairing methods, which leverage statistical properties of the input data. Given an inconsistent dataset as input, HoloClean automatically generates a probabilistic program that performs data repairing. Inspired by recent theoretical advances in probabilistic inference, we introduce a series of optimizations which ensure that inference over HoloClean's probabilistic model scales data instances with millions of tuples. We show that HoloClean scales to instances with millions of tuples and find data repairs with an average precision of ~90% and an average recall of above ~76% across a diverse array of datasets exhibiting different types of errors. This yields an average F1 improvement of more than 2x against state-of-the-art methods.",
    "subtype": "research",
    "authors": [
      {
        "name": "Theodoros Rekatsinas",
        "affiliation": "Stanford University"
      },
      {
        "name": "Xu Chu",
        "affiliation": "University of Waterloo"
      },
      {
        "name": "Ihab Ilyas",
        "affiliation": "University of Waterloo"
      },
      {
        "name": "Chris Re",
        "affiliation": "Stanford University"
      }
    ],
    "type": "research",
    "id": "1283"
  },
  "research517": {
    "title": "ZooBP: Belief Propagation for Heterogeneous Networks",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p625-eswaran.pdf",
    "abstract": "Given a heterogeneous network, with nodes of different types -- e.g., products, users and sellers from an online recommendation site like Amazon -- and labels for a few nodes ('honest', 'suspicious', etc), can we find a closed formula for Belief Propagation (BP), exact or approximate? Can we say whether it will converge? BP, traditionally an inference algorithm for graphical models, exploits the so-called ``network effects'' to perform real-world graph classification tasks; and it has been successful in numerous settings like fraudulent entity detection in online retailers and classification in social networks when we are given the labels for a subset of the nodes. We propose ZooBP, a method to perform fast BP on undirected heterogeneous graphs with provable convergence guarantees. ZooBP has the following advantages: (1) Generality: It works on heterogeneous graphs with multiple node- and edge-types; (2) Theoretical Guarantees: unlike the traditional loopy BP, ZooBP gives a closed-form solution, as well as convergence guarantees; (3) Scalability: ZooBP is linear on the graph size and is up to 600 times faster than BP, running on graphs with 3.3 million edges in a few seconds. (4) Effectiveness: Applied on real data (a Flipkart e-commerce network with users, products and sellers), ZooBP identifies fraudulent users with a near-perfect precision of 92.3% over the top 300 results.",
    "subtype": "research",
    "authors": [
      {
        "name": "Dhivya Eswaran",
        "affiliation": "CMU"
      },
      {
        "name": "Stephan Guennemann",
        "affiliation": "TUM"
      },
      {
        "name": "Christos Faloutsos",
        "affiliation": "CMU"
      },
      {
        "name": "Disha Makhija",
        "affiliation": "Flipkart"
      },
      {
        "name": "Mohit Kumar",
        "affiliation": "Flipkart"
      }
    ],
    "type": "research",
    "id": "517"
  },
  "research235": {
    "title": "Multi-Query Optimization for Subgraph Isomorphism Search",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p121-ren.pdf",
    "abstract": "Existing work on subgraph isomorphism search mainly focuses on a-query-at-a-time approaches: optimizing and answering each query separately and sequentially. When multiple queries arrive in a batch, sequential processing is not always the most efficient. In this paper, we study multi-query optimization for subgraph isomorphism search. We first propose a novel method for efficiently detecting useful common subgraphs and a data structure to organize them. We propose a heuristic algorithm based on the data structure to compute a query execution order so that cached intermediate results can be effectively utilized. To balance memory usage and time for cached result retrieval, we present a novel structure for caching the intermediate results. We then provide strategies to revise existing single-query subgraph isomorphism algorithms to seamlessly utilize the cached results, which leads to significant performance improvement. Extensive experiments verified the effectiveness of our solution.",
    "subtype": "research",
    "authors": [
      {
        "name": "Xuguang Ren",
        "affiliation": "Griffith University"
      },
      {
        "name": "Junhu Wang",
        "affiliation": "Griffith University"
      }
    ],
    "type": "research",
    "id": "235"
  },
  "research852": {
    "title": "Don't Hold My Data Hostage - A Case For Client Protocol Redesign",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p1022-muehleisen.pdf",
    "abstract": "Transferring a large amount of data from a database to a client program is a surprisingly expensive operation. The time this requires can easily dominate the query execution time for large result sets. This represents a significant hurdle for external data analysis, for example when using statistical software. In this paper, we explore and analyse the result set serialization design space. We present experimental results from a large chunk of the database market and show the inefficiencies of current approaches. We then propose a columnar serialization method that improves transmission performance by an order of magnitude.",
    "subtype": "research",
    "authors": [
      {
        "name": "Mark Raasveldt",
        "affiliation": "CWI"
      },
      {
        "name": "Hannes Mühleisen",
        "affiliation": "CWI"
      }
    ],
    "type": "research",
    "id": "852"
  },
  "research747": {
    "title": "Bias-Aware Sketches",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p961-zhang.pdf",
    "abstract": "Linear sketching algorithms have been widely used for processing large-scale distributed and streaming datasets. Their popularity is largely due to the fact that linear sketches can be naturally composed in the distributed model and be efficiently updated in the streaming model. The errors of linear sketches are typically expressed in terms of the sum of coordinates of the input vector excluding those largest ones, or, the mass on the tail of the vector. Thus, the precondition for these algorithms to perform well is that the mass on the tail is small, which is, however, not always the case -- in many real-world datasets the coordinates of the input vector have a { bias}, which will generate a large mass on the tail. In this paper we propose linear sketches that are { bias-aware}. We rigorously prove that they achieve strictly better error guarantees than the corresponding existing sketches, and demonstrate their practicality and superiority via an extensive experimental evaluation on both real and synthetic datasets.",
    "subtype": "research",
    "authors": [
      {
        "name": "Jiecao Chen",
        "affiliation": "Indiana University"
      },
      {
        "name": "Qin Zhang",
        "affiliation": "Indiana University"
      }
    ],
    "type": "research",
    "id": "747"
  },
  "research510": {
    "title": "KBQA: Learning Question Answering over QA Corpora and Knowledge Bases",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p565-cui.pdf",
    "abstract": "Question answering (QA) has become a popular way for humans to access billion-scale knowledge bases. Unlike web search, QA over a knowledge base gives out accurate and concise results, provided that natural language questions can be understood and mapped precisely to structured queries over the knowledge base. The challenge, however, is that a human can ask one question in many different ways. Previous approaches have natural limits due to their representations: rule based approaches only understand a small set of ``canned'' questions, while keyword based or synonym based approaches cannot fully understand the questions. In this paper, we design a new kind of question representation: templates, over a billion scale knowledge base and a million scale QA corpora. For example, for questions about a city's population, we learn templates such as What's the population of $city?, How many people are there in $city?. We learned 27 million templates for 2782 relations. Based on these templates, our QA system KBQA effectively supports binary factoid questions, as well as complex questions which are composed of a series of binary factoid questions. Furthermore, we expand predicates in RDF knowledge base, which boosts the coverage of knowledge base by 57 times. Our QA system beats all other state-of-art works on both effectiveness and efficiency over QALD benchmarks.",
    "subtype": "research",
    "authors": [
      {
        "name": "Wanyun Cui",
        "affiliation": "Fudan University"
      },
      {
        "name": "Yanghua Xiao",
        "affiliation": "Fudan University"
      },
      {
        "name": "Haixun Wang",
        "affiliation": "Facebook"
      },
      {
        "name": "Yangqiu Song",
        "affiliation": "West Virginia University"
      },
      {
        "name": "Seung-won Hwang",
        "affiliation": "Yonsei University"
      },
      {
        "name": "Wei Wang",
        "affiliation": "Fudan University"
      }
    ],
    "type": "research",
    "id": "510"
  },
  "research144": {
    "title": "Effective Indexing for Approximate Constrained Shortest Path Queries on Large Road Networks",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p61-wang.pdf",
    "abstract": "In a constrained shortest path (CSP) query, each edge in the road network is associated with both a length and a cost. Given an origin s, a destination t, and a cost constraint theta, the goal is to find the shortest path from s to t whose total cost does not exceed theta. Because exact CSP is NP-hard, previous work mostly focuses on approximate solutions. Even so, existing methods are still prohibitively expensive for large road networks. Two main reasons are (i) that they fail to utilize the special properties of road networks and (ii) that most of them process queries without indices; the few existing indices consume large amounts of memory and yet have limited effectiveness in reducing query costs. Motivated by this, we propose COLA, the first practical solution for approximate CSP processing on large road networks. COLA exploits the facts that a road network can be effectively partitioned, and that there exists a relatively small set of landmark vertices that commonly appear in CSP results. Accordingly, COLA indexes the vertices lying on partition boundaries, and applies an on-the-fly algorithm called alpha-Dijk for path computation within a partition, which effectively prunes paths based on landmarks. Extensive experiments demonstrate that on continent-sized road networks, COLA answers an approximate CSP query in sub-second time, whereas existing methods take hours. Interestingly, even without an index, the alpha-Dijk algorithm in COLA still outperforms previous solutions by more than an order of magnitude.",
    "subtype": "research",
    "authors": [
      {
        "name": "Sibo Wang",
        "affiliation": "Nanyang Technological University"
      },
      {
        "name": "Xiaokui Xiao",
        "affiliation": "Nanyang Technological University"
      },
      {
        "name": "Yin Yang",
        "affiliation": "Hamad Bin Khalifa University"
      },
      {
        "name": "Wenqing Lin",
        "affiliation": "Qatar Computing Research Institute"
      }
    ],
    "type": "research",
    "id": "144"
  },
  "industrial915": {
    "title": "Quaestor: Query Web Caching for Database-as-a-Service Providers",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p1670-gessert.pdf",
    "abstract": "Today, web performance is primarily governed by round-trip latencies between end devices and cloud services. To improve performance, services need to minimize the delay of accessing data. In this paper, we propose a novel approach to low latency that relies on existing content delivery and web caching infrastructure. The main idea is to enable application-independent caching of query results and records with tunable consistency guarantees, in particular bounded staleness. Quaestor (Query Store) employs two key concepts to incorporate both expiration-based and invalidation-based web caches: (1) an Expiring Bloom Filter data structure to indicate potentially stale data, and (2) statistically derived cache expiration times to maximize cache hit rates. Through a distributed query invalidation pipeline, changes to cached query results are detected in real-time. The proposed caching algorithms offer a new means for data-centric cloud services to trade latency against staleness bounds, e.g. in a database-as-a-service. Quaestor is the core technology of the backend-as-a-service platform Baqend, a cloud service for low-latency websites. We provide empirical evidence for Quaestor's scalability and performance through both simulation and experiments. The results indicate that for read-heavy workloads, up to tenfold speed-ups can be achieved through Quaestor's caching.",
    "subtype": "industrial",
    "authors": [
      {
        "name": "Felix Gessert",
        "affiliation": "Baqend GmbH"
      },
      {
        "name": "Michael Schaarschmidt",
        "affiliation": "University of Cambridge"
      },
      {
        "name": "Wolfram Wingerath",
        "affiliation": "Universität Hamburg"
      },
      {
        "name": "Erik Wiit",
        "affiliation": "Baqend GmbH"
      },
      {
        "name": "Eiko Yoneki",
        "affiliation": "University of Cambridge"
      },
      {
        "name": "Norbert Ritter",
        "affiliation": "University of Hamburg"
      }
    ],
    "type": "industrial",
    "id": "915"
  },
  "research610": {
    "title": "Finding Diverse, High-Value Representatives on a Surface of Answers",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p793-wu.pdf",
    "abstract": "In many applications, the system needs to selectively present a small subset of answers to users. The set of all possible answers can be seen as an elevation surface over a domain, where the elevation measures the quality of each answer, and the dimensions of the domain correspond to attributes of the answers with which similarity between answers can be measured. This paper considers the problem of finding a diverse set of k high-quality representatives for such a surface. We show that existing methods for diversified top-k and weighted clustering problems are inadequate for this problem. We propose k-DHR as a better formulation for the problem. We show that k-DHR has a submodular and monotone objective function, and we develop efficient algorithms for solving k-DHR with provable guarantees. We conduct extensive experiments to demonstrate the usefulness of the results produced by k-DHR for applications in computational lead-finding and fact-checking, as well as the efficiency and effectiveness of our algorithms.",
    "subtype": "research",
    "authors": [
      {
        "name": "You Wu",
        "affiliation": "Google Research"
      },
      {
        "name": "Junyang Gao",
        "affiliation": "Duke University"
      },
      {
        "name": "Pankaj Agarwal",
        "affiliation": "Duke University"
      },
      {
        "name": "Jun Yang",
        "affiliation": "Duke University"
      }
    ],
    "type": "research",
    "id": "610"
  },
  "research1399": {
    "title": "Non-Invasive Progressive Optimization for In-Memory Databases",
    "acm_link": "http://www.vldb.org/pvldb/vol9/p1659-zeuch.pdf",
    "abstract": "Progressive optimization introduces robustness for database workloads against wrong estimates, skewed data, correlated attributes, or outdated statistics. Previous work focuses on cardinality estimates and rely on expensive counting methods as well as complex learning algorithms. In this paper, we utilize performance counters to drive progressive optimization during query execution. The main advantages are that performance counters introduce virtually no costs on modern CPUs and their usage enables a non-invasive monitoring. We present fine-grained cost models to detect divergences between estimates and actual costs to kick-start the reoptimization process. Based on these cost models, we implement an optimization approach that estimates the individual selectivities of a multi-selection query efficiently. Furthermore, we are able to learn properties like sortedness, skew, or correlation. In our evaluation we show, that the overhead of our approach is negligible but the performance improvements are convincing. Using progressive optimization, we improve runtime up to a factor of three compared to average runtime and up to a factor of 4,5 compared to worst case runtime. As a result, we avoid costly operator execution orders and thus make query execution highly robust.",
    "subtype": "research",
    "authors": [
      {
        "name": "Steffen Zeuch",
        "affiliation": "Humboldt-Universität zu Berlin"
      },
      {
        "name": "Holger Pirk",
        "affiliation": "MIT"
      },
      {
        "name": "Johann-Christoph Freytag",
        "affiliation": "Humboldt-Universität zu Berlin"
      }
    ],
    "type": "research",
    "id": "1399"
  },
  "research1331": {
    "title": "Reverse Engineering Aggregation Queries",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p1394-tan.pdf",
    "abstract": "Query reverse engineering seeks to re-generate the SQL query that produced a given query output table from a given database. In this paper, we propose our solution for this problem for OLAP queries with group-by and aggregation. We develop a novel three-phase algorithm named REGAL for this problem. First, based on a lattice graph structure, we identify a set of group-by candidates for the desired query. Second, we apply a set of aggregation constraints that are derived from the properties of aggregate operators at both the table level and the group level to discover candidate combinations of group-by columns and aggregations that are consistent with the given query output table. Finally, we find a multidimensional filter, i.e., a conjunction of selection predicates over the base table attributes, that is needed to generate the exact query output table. We conduct an extensive experimental study over the TPC-H dataset to examine the effectiveness and efficiency of our proposal.",
    "subtype": "research",
    "authors": [
      {
        "name": "Wei Chit Tan",
        "affiliation": "SUTD"
      },
      {
        "name": "Meihui Zhang",
        "affiliation": "SUTD"
      },
      {
        "name": "Hazem Elmeleegy",
        "affiliation": "Turn Inc."
      },
      {
        "name": "Divesh Srivastava",
        "affiliation": "AT&T"
      }
    ],
    "type": "research",
    "id": "1331"
  },
  "industrial1131": {
    "title": "FAD.js: Fast JSON Data Access Using JIT-based Speculative Optimizations",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p1778-bonetta.pdf",
    "abstract": "JSON is one of the most popular data encoding formats, with wide adoption in Databases and BigData frameworks as well as native support in popular programming languages such as JavaScript/Node.js, Python, and R. Nevertheless, JSON data processing can easily become a performance bottleneck in data-intensive applications because of parse and serialization overhead. In this paper, we introduce Fad.js, a runtime system for efficient processing of JSON objects in data-intensive applications. Fad.js is based on (1) speculative just-in-time (JIT) compilation and (2) selective access to data. Experiments show that applications using Fad.js achieve speedups up to 2.7x for encoding and 9.9x for decoding JSON data when compared to state-of-the art JSON processing libraries.",
    "subtype": "industrial",
    "authors": [
      {
        "name": "Daniele Bonetta",
        "affiliation": "Oracle Labs"
      },
      {
        "name": "Matthias Brantner",
        "affiliation": "Oracle Labs"
      }
    ],
    "type": "industrial",
    "id": "1131"
  },
  "research677": {
    "title": "MILC: Inverted List Compression in Memory",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p853-wang.pdf",
    "abstract": "Inverted list compression is a topic that has been studied for 50 years due to its fundamental importance in numerous applications including information retrieval, databases, and graph analytics. Typically, an inverted list compression algorithm is evaluated on its space overhead and query processing time. Earlier list compression designs mainly focused on minimizing the space overhead to reduce expensive disk I/O time in disk-oriented systems. But the recent trend is shifted towards reducing query processing time because the underlying systems tend to be memory-resident. Although there are many highly optimized compression approaches in main memory, there is still a considerable performance gap between query processing over compressed lists and uncompressed lists, which motivates this work. In this work, we set out to bridge this performance gap for the first time by proposing a new compression scheme, namely, MILC (memory inverted list compression). MILC relies on a series of techniques including offset-oriented fixed-bit encoding, dynamic partitioning, in-block compression, cache-aware optimization, and SIMD acceleration. We conduct experiments on three real-world datasets in information retrieval, databases, and graph analytics to demonstrate the high performance and low space overhead of MILC. We compare MILC with 12 recent compression algorithms and experimentally show that MILC improves the query performance by up to 13.2X and reduces the space overhead by up to 4.7X.",
    "subtype": "research",
    "authors": [
      {
        "name": "Jianguo Wang",
        "affiliation": "UCSD"
      },
      {
        "name": "Chunbin Lin",
        "affiliation": "UCSD"
      },
      {
        "name": "Ruining He",
        "affiliation": "UCSD"
      },
      {
        "name": "Moojin Chae",
        "affiliation": "UCSD"
      },
      {
        "name": "Yannis Papakonstantinou",
        "affiliation": "UCSD"
      },
      {
        "name": "Steven Swanson",
        "affiliation": "UCSD"
      }
    ],
    "type": "research",
    "id": "677"
  },
  "research323": {
    "title": "Knowledge Exploration using Tables on the Web",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p193-chirigati.pdf",
    "abstract": "The increasing popularity of mobile device usage has ushered in many features in modern search engines that help users with various information needs. One of those needs is Knowledge Exploration, where related documents are returned in response to a user query, either directly through right-hand side knowledge panels or indirectly through navigable sections underneath individual search results. Existing knowledge exploration features have relied on a combination of Knowledge Bases and query logs. In this paper, we propose Knowledge Carousels of two modalities, namely sideways and downwards, that facilitate exploration of IS-A and HAS-A relationships, respectively, with regard to an entity-seeking query, based on leveraging the large corpus of tables on the Web. This brings many technical challenges, including associating correct carousels with the search entity, selecting the best carousel from the candidates, and finding titles that best describe the carousel. We describe how we address these challenges and also experimentally demonstrate through user studies that our approach produces better result sets than baseline approaches.",
    "subtype": "research",
    "authors": [
      {
        "name": "Fernando Chirigati",
        "affiliation": "NYU"
      },
      {
        "name": "Jialu Liu",
        "affiliation": "University of Illinois at Urbana-Champaign"
      },
      {
        "name": "Flip Korn",
        "affiliation": "Google Research"
      },
      {
        "name": "You Wu",
        "affiliation": "Google Research"
      },
      {
        "name": "Cong Yu",
        "affiliation": "Google"
      },
      {
        "name": "Hao Zhang",
        "affiliation": "Google Research"
      }
    ],
    "type": "research",
    "id": "323"
  },
  "industrial1085": {
    "title": "Resumable Online Index Rebuild in SQL Server",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p1742-antonopoulos.pdf",
    "abstract": "Azure SQL Database and the upcoming release of SQL Server enhance Online Index Rebuild to provide fault-tolerance and allow index rebuild operations to resume after a system failure or a user-initiated pause. SQL Server is the first commercial DBMS to support pause and resume functionality for index rebuilds. This is achieved by splitting the operation into incremental units of work and persisting the required state so that it can be resumed later with minimal loss of progress. At the same time, the proposed technology minimizes the log space required for the operation to succeed, making it possible to rebuild large indexes using only a small, constant amount of log space. These capabilities are critical to guarantee the reliability of these operations in an environment where a) the database sizes are increasing at a much faster pace compared to the available hardware, b) system failures are frequent in Cloud architectures using commodity hardware, c) software upgrades and other maintenance tasks are automatically handled by the Cloud platforms, introducing further unexpected failures for the users and d) most modern applications need to be available 24/7 and have very tight maintenance windows. This paper describes the design of “Resumable Online Index Rebuild” and discusses how this technology can be extended to cover more schema management operations in the future.",
    "subtype": "industrial",
    "authors": [
      {
        "name": "Panagiotis Antonopoulos",
        "affiliation": "Microsoft"
      },
      {
        "name": "Hanuma Kodavalla",
        "affiliation": "Microsoft"
      },
      {
        "name": "Alex Tran",
        "affiliation": "Microsoft"
      },
      {
        "name": "Nitish Upreti",
        "affiliation": "Microsoft"
      },
      {
        "name": "Chaitali Shah",
        "affiliation": "Microsoft"
      },
      {
        "name": "Mirek Sztajno",
        "affiliation": "Microsoft"
      }
    ],
    "type": "industrial",
    "id": "1085"
  },
  "research613": {
    "title": "Understanding Workers, Developing Effective Tasks, and Enhancing Marketplace Dynamics: A Study of a Large Crowdsourcing Marketplace",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p829-dassarma.pdf",
    "abstract": "We conduct an experimental analysis of a dataset comprising over 25 million microtasks performed by over 70,000 workers issued to a large crowdsourcing marketplace between 2012-2016. Using this data---never before analyzed in an academic context---we shed light on three crucial aspects of crowdsourcing: (1) Task design --- helping requesters understand what constitutes an effective task, and how to go about designing one; (2) Marketplace dynamics --- helping marketplace administrators and designers understand the interaction between tasks and workers, and the corresponding marketplace load; and (3) Worker behavior --- understanding worker attention spans, lifetimes, and general behavior, for the improvement of the crowdsourcing ecosystem as a whole.",
    "subtype": "research",
    "authors": [
      {
        "name": "Ayush Jain",
        "affiliation": "University of Illinois"
      },
      {
        "name": "Akash Das Sarma",
        "affiliation": "Stanford University"
      },
      {
        "name": "Aditya Parameswaran",
        "affiliation": "UIUC"
      },
      {
        "name": "Jennifer Widom",
        "affiliation": "Stanford University"
      }
    ],
    "type": "research",
    "id": "613"
  },
  "industrial1097": {
    "title": "CarStream: An Industrial System of Big Data Processing for Internet-of-Vehicles",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p1766-zhang.pdf",
    "abstract": "As the Internet-of-Vehicles (IoV) technology becomes an increasingly important trend for future transportation, designing large-scale IoV systems has become a critical task that aims to process big data uploaded by fleet and to provide data-driven services. The IoV data, especially high-frequency vehicle statuses (e.g., location, engine parameters), are characterized as large volume with a low density of value and low data quality. Such characteristics pose challenges for developing real-time applications based on such data. In this paper, we address the challenges in designing a scalable IoV system by describing CarStream, an industrial system of big data processing for chauffeured car services. Connected with over 30,000 vehicles, CarStream collects and processes multiple types of driving data including vehicle status, driver activity, and passenger-trip information. Multiple services are provided based on the collected data. CarStream has been deployed and maintained for three years in industrial usage, collecting over 40 terabytes driving data. This paper shares our experiences about designing CarStream based on large-scale driving-data streams, and the lessons learned from the process of addressing the challenges in designing and maintaining CarStream.",
    "subtype": "industrial",
    "authors": [
      {
        "name": "Mingming Zhang",
        "affiliation": "Beihang University"
      },
      {
        "name": "Tianyu Wo",
        "affiliation": "Beihang University"
      },
      {
        "name": "Xuelian Lin",
        "affiliation": "Beihang University"
      },
      {
        "name": "Tao Xie",
        "affiliation": "University of Illinois"
      },
      {
        "name": "Yaxiao Liu",
        "affiliation": "CAR Inc."
      }
    ],
    "type": "industrial",
    "id": "1097"
  },
  "industrial1212": {
    "title": "Adaptive Statistics in Oracle 12c",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p1813-zait.pdf",
    "abstract": "Database Management Systems (DBMS) continue to be the foundation of mission critical applications, both OLTP and Analytics. They provide a safe, reliable and efficient platform to store and retrieve data. SQL is the lingua franca of the database world. A database developer writes a SQL statement to specify data sources and express the desired result and the DBMS will figure out the most efficient way to implement it. The query optimizer is the component in a DBMS responsible for finding the best execution plan for a given SQL statement based on statistics, access structures, location, and format. At the center of a query optimizer is a cost model that consumes the above information and helps the optimizer make decisions related to query transformations, join order, join methods, access paths, and data movement. The final execution plan produced by the query optimizer depends on the quality of information used by the cost model, as well as the sophistication of the cost model. In addition to statistics about the data, the cost model also relies on statistics generated internally for intermediate results, e.g. size of the output of a join operation. This paper presents the problems caused by incorrect statistics of intermediate results, survey the existing solutions and present our solution introduced in Oracle 12c. The solution includes validating the generated statistics using table data and via the automatic creation of auxiliary statistics structures. We limit the overhead of the additional work by confining their use to cases where it matters the most, caching the computed statistics, and using table samples. The statistics management is automated. We demonstrate the benefits of our approach based on experiments using two SQL workloads, a benchmark that uses data from the Internal Movie Data Base (IMDB) and a real customer workload.",
    "subtype": "industrial",
    "authors": [
      {
        "name": "Mohamed Zait",
        "affiliation": "Oracle"
      },
      {
        "name": "Sunil Chakkappen",
        "affiliation": "Oracle"
      },
      {
        "name": "Suratna Budalakoti",
        "affiliation": "Oracle Labs"
      },
      {
        "name": "Satyanarayana Valluri",
        "affiliation": "Oracle"
      },
      {
        "name": "Ramarajan Krishnamachari",
        "affiliation": "Oracle"
      },
      {
        "name": "Alan Wood",
        "affiliation": "Oracle Labs"
      }
    ],
    "type": "industrial",
    "id": "1212"
  },
  "research1279": {
    "title": "Trajectory Similarity Join in Spatial Networks",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p1178-shang.pdf",
    "abstract": "The matching of similar pairs of objects, called similarity join, is fundamental functionality in data management. We consider the case of trajectory similarity join (TS-Join), where the objects are trajectories of vehicles moving in road networks. Thus, given two sets of trajectories and a threshold $theta$, the TS-Join returns all pairs of trajectories from the two sets with similarity above $theta$. This join targets applications such as trajectory duplicate detection, data cleaning, ridesharing/carpooling recommendation, and traffic congestion prediction. With these applications in mind, we provide a purposeful definition of similarity. To enable efficient TS-Join processing on large sets of trajectories, we develop search space pruning techniques and take into account the parallel processing capabilities of modern processors. Specifically, we present a two-phase divide-and-conquer algorithm. For each trajectory, the algorithm first finds similar trajectories. Then it merges the results to achieve a final result. The algorithm exploits an upper bound on the spatiotemporal similarity and a heuristic scheduling strategy for search space pruning. The algorithm's per-trajectory searches are independent of each other and can be performed in parallel, and the merging has constant cost. An empirical study with real data offers insight in the performance of the algorithm and demonstrates that is capable of outperforming a well-designed baseline algorithm by an order of magnitude.",
    "subtype": "research",
    "authors": [
      {
        "name": "Shuo Shang",
        "affiliation": "KAUST"
      },
      {
        "name": "Lisi Chen",
        "affiliation": "Hong Kong Baptist University"
      },
      {
        "name": "Zhewei Wei",
        "affiliation": "RUC"
      },
      {
        "name": "Christian Jensen",
        "affiliation": "Aalborg University"
      },
      {
        "name": "Kai Zheng",
        "affiliation": "Soochow University"
      },
      {
        "name": "Panos Kalnis",
        "affiliation": "KAUST"
      }
    ],
    "type": "research",
    "id": "1279"
  },
  "research507": {
    "title": "Clue-based Spatio-textual Query",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p529-deng.pdf",
    "abstract": "Along with the proliferation of online digital map and location-based service, very large POI (point of interest) databases have been constructed where a record corresponds to a POI with information including name, category, address, geographical location and other features. A basic spatial query in POI database is POI retrieval. In many scenarios, a user cannot provide enough information to pinpoint the POI except some clue. For example, a user wants to identify a cafe in a city visited many years ago. He cannot remember the name and address but he still recalls that “the cafe is about 200 meters away from a restaurant; and turning left at the restaurant there is a bakery 500 meters away, etc.”. Intuitively, the clue, even partial and approximate, describes the spatio-textual context around the targeted POI. Motivated by this observation, this work investigates clue-based spatio-textual query which allows user providing clue, i.e., some nearby POIs and the spatial relationships between them, in POI retrieval. The objective is to retrieve k POIs from a POI database with the highest spatio-textual context similarities against the clue. This work has deliberately designed data-quality-tolerant spatio-textual context similarity metric to cope with various data quality problems in both the clue and the POI database. Through crossing valuation, the query accuracy is further enhanced by ensemble method. Also, this work has developed an index called roll-out-star R-tree (RSR-tree) to dramatically improve the query processing efficiency. The extensive tests on data sets from the real world have verified the superiority of our methods in all aspects.",
    "subtype": "research",
    "authors": [
      {
        "name": "Junling Liu",
        "affiliation": "Northeastern University China"
      },
      {
        "name": "Ke Deng",
        "affiliation": "RMIT University"
      },
      {
        "name": "Huanliang Sun",
        "affiliation": "Shenyang Jianzhu University"
      },
      {
        "name": "Yu Ge",
        "affiliation": "Northeastern University China"
      },
      {
        "name": "Xiaofang Zhou",
        "affiliation": "University of Queensland"
      },
      {
        "name": "Christian Jensen",
        "affiliation": "Aalborg University"
      }
    ],
    "type": "research",
    "id": "507"
  },
  "research318": {
    "title": "NG-DBSCAN: Scalable Density-Based Clustering for Arbitrary Data",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p157-lulli.pdf",
    "abstract": "We present NG-DBSCAN, an approximate density-based clustering algorithm that operates on arbitrary data and any symmetric distance measure. The distributed design of our algorithm makes it scalable to very large datasets; its approximate nature makes it fast, yet capable of producing high quality clustering results. We provide a detailed overview of the steps of NG-DBSCAN, together with their analysis. Our results, obtained through an extensive experimental campaign with real and synthetic data, substantiate our claims about NG-DBSCAN’s performance and scalability.",
    "subtype": "research",
    "authors": [
      {
        "name": "Alessandro Lulli",
        "affiliation": "University of Pisa"
      },
      {
        "name": "Matteo Dell'Amico",
        "affiliation": "Symantec Research Labs"
      },
      {
        "name": "Pietro Michiardi",
        "affiliation": "EURECOM"
      },
      {
        "name": "Laura Ricci",
        "affiliation": "University of Pisa"
      }
    ],
    "type": "research",
    "id": "318"
  },
  "research1370": {
    "title": "Reconciling Skyline and Ranking Queries",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p1454-martinenghi.pdf",
    "abstract": "Traditionally, skyline and ranking queries have been treated separately as alternative ways of discovering interesting data in potentially large datasets. While ranking queries adopt a specific scoring function to rank tuples, skyline queries return the set of non-dominated tuples and are independent of attribute scales and scoring functions. Ranking queries are thus less general but usually cheaper to compute and widely used in data management systems. We propose a framework to seamlessly integrate these two approaches by introducing the notion of restricted skyline queries (R-skylines). We propose R-skyline operators that generalize both skyline and ranking queries by applying the notion of dominance to a set of scoring functions of interest. Such sets can be characterized, e.g., by imposing constraints on the function’s parameters, such as the weights in a linear scoring function. We discuss the formal properties of these new operators, show how to implement them efficiently, and evaluate them on both synthetic and real datasets.",
    "subtype": "research",
    "authors": [
      {
        "name": "Paolo Ciaccia",
        "affiliation": "Università di Bologna"
      },
      {
        "name": "Davide Martinenghi",
        "affiliation": "Politecnico di Milano"
      }
    ],
    "type": "research",
    "id": "1370"
  },
  "research1304": {
    "title": "A holistic view of stream partitioning costs",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p1286-katsipoulakis.pdf",
    "abstract": "Stream processing has become the dominant processing model for monitoring and real–time analytics. Modern Parallel Stream Processing Engines (pSPEs) have made it feasible to increase the performance in both monitoring and analytical queries by parallelizing a query’s execution and distributing the load on multiple workers. A determining factor for the performance of a pSPE is the partitioning algorithm used to disseminate tuples to the workers. Until now, partitioning methods in pSPEs have been similar to the ones used in parallel databases and only recently load-aware algorithms have been employed to improve the effectiveness of parallel execution. We identify and demonstrate the need to incorporate aggregation costs in the partitioning model when executing stateful operations in parallel, in order to minimize the overall latency and/or throughput. Towards this, we propose new stream partitioning algorithms,that consider both tuple imbalance and aggregation cost. We evaluate our proposed algorithms and show that they can achieve up to an order of magnitude better performance, compared to the current state of the art.",
    "subtype": "research",
    "authors": [
      {
        "name": "Nikos R. Katsipoulakis",
        "affiliation": "University of Pittsburgh"
      },
      {
        "name": "Alexandros  Labrinidis",
        "affiliation": "University of Pittsburgh"
      },
      {
        "name": "Panos Chrysanthis",
        "affiliation": "University of Pittsburgh"
      }
    ],
    "type": "research",
    "id": "1304"
  },
  "research1133": {
    "title": "Revisiting Reuse for Approximate Query Processing",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p1142-galakatos.pdf",
    "abstract": "Visual data exploration tools allow users to rapidly explore datasets in order to quickly understand trends and gather insights. As dataset sizes continue to increase, new techniques will be necessary to maintain the interactivity guarantees that these tools require. Approximate query processing (AQP) attempts to tackle this problem and allows systems to return query results at ``human speed.'' However, existing techniques start to break down when applied to queries over rare subpopulations, since they fail to leverage the unique properties of interactive data exploration. We therefore present a novel AQP formulation that can provide low-error approximate results at interactive speeds, even for queries over rare subpopulations. In particular, our formulation treats query results as random variables in order to better take advantage of the ample opportunities for result reuse inherent in interactive data exploration. As part of our approach, we apply a variety of optimization techniques that are based on probability theory, including new query rewrite rules and index structures. We implemented these techniques in a prototype system and show that they can achieve interactive latencies where alternative approaches cannot.",
    "subtype": "research",
    "authors": [
      {
        "name": "Alex Galakatos",
        "affiliation": "Brown University"
      },
      {
        "name": "Andrew Crotty",
        "affiliation": "Brown University"
      },
      {
        "name": "Emanuel Zgraggen",
        "affiliation": "Brown University"
      },
      {
        "name": "Carsten Binnig",
        "affiliation": "Brown University"
      },
      {
        "name": "Tim Kraska",
        "affiliation": "Brown University"
      }
    ],
    "type": "research",
    "id": "1133"
  },
  "research1292": {
    "title": "SquirrelJoin: Network-Aware Distributed Join Processing with Lazy Partitioning",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p1250-rupprecht.pdf",
    "abstract": "To execute distributed joins in parallel on compute clusters, systems partition and exchange data records between workers. With large datasets, workers spend a considerable amount of time transferring data over the network. When compute clusters are shared among multiple applications, workers must compete for network bandwidth with other applications. These variances in the available network bandwidth lead to network skew, which causes straggling workers to prolong the join completion time. We describe SquirrelJoin, a distributed join processing technique that uses lazy partitioning to adapt to transient network skew in clusters. Workers maintain in-memory lazy partitions to withhold a subset of records, i.e. not sending them immediately to other workers for processing. Lazy partitions are then assigned dynamically to other workers based on network conditions: each worker takes periodic throughput measurements to estimate its completion time, and lazy partitions are allocated as to minimise the join completion time. We implement SquirrelJoin as part of the Apache Flink distributed dataflow framework and show that, under transient network contention in a shared compute cluster, SquirrelJoin speeds up join completion times by up to 2.3x with only a small, fixed overhead.",
    "subtype": "research",
    "authors": [
      {
        "name": "Lukas Rupprecht",
        "affiliation": "Imperial College London"
      },
      {
        "name": "William Culhane",
        "affiliation": "Imperial College London"
      },
      {
        "name": "Peter Pietzuch",
        "affiliation": "Imperial College London"
      }
    ],
    "type": "research",
    "id": "1292"
  },
  "research860": {
    "title": "SilkMoth: An Efficient Method for Finding Related Sets with Maximum Matching Constraints",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p1082-deng.pdf",
    "abstract": "Determining if two sets are related – that is, if they have similar values or if one set contains the other – is an important problem with many applications in data cleaning, data integration, and information retrieval. For example, set relatedness can be a useful tool to discover whether columns from two different databases are join- able; if enough of the values in the columns match, it may make sense to join them. A common metric is to measure the related- ness of two sets by treating the elements as vertices of a bipartite graph and calculating the score of the maximum matching pairing between elements. Compared to other metrics which require ex- act matchings between elements, this metric uses a similarity function to compare elements between the two sets, making it robust to small dissimilarities in elements and more useful for real-world, dirty data. Unfortunately, the metric suffers from expensive computational cost, taking O(n3) time, where n is the number of elements in the sets, for each set-to-set comparison. Thus for applications that try to search for all pairings of related sets in a brute-force manner, the runtime becomes unacceptably large. To address this challenge, we developed SilkMoth, a system capable of rapidly discovering related set pairs in collections of sets. Internally, SilkMoth creates a signature for each set, with the property that any other set which is related must match the sig- nature. SilkMoth then uses these signatures to prune the search space, so only sets that match the signatures are left as candidates. Finally, SilkMoth applies the maximum matching metric on remaining candidates to verify which of these candidates are truly related sets. An important property of SilkMoth is that it is guaranteed to output exactly the same related set pairings as the brute- force method, unlike approximate techniques. Thus, a contribution of this paper is the characterization of the space of signatures which enable this property. We show that selecting the optimal signature in this space is NP-complete, and based on insights from the characterization of the space, we propose two novel filters which help to prune the candidates further before verification. In addition, we introduce a simple optimization to the calculation of the maximum matching metric itself based on the triangle inequality. Compared to related approaches, SilkMoth is much more general, handling a larger space of similarity functions and relatedness metrics, and is an order of magnitude more efficient on real datasets.",
    "subtype": "research",
    "authors": [
      {
        "name": "Dong Deng",
        "affiliation": "MIT"
      },
      {
        "name": "Albert Kim",
        "affiliation": "MIT"
      },
      {
        "name": "Samuel Madden",
        "affiliation": "MIT"
      },
      {
        "name": "Michael Stonebraker",
        "affiliation": "MIT"
      }
    ],
    "type": "research",
    "id": "860"
  },
  "research511": {
    "title": "Provenance for Natural Language Queries",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p577-deutch.pdf",
    "abstract": "Multiple lines of research have developed Natural Language (NL) interfaces for formulating database queries. We build upon this work, but focus on presenting a highly detailed form of the answers in NL. The answers that we present are importantly based on the provenance of tuples in the query result, detailing not only the results but also their explanations. We develop a novel method for transforming provenance information to NL, by leveraging the original NL query structure. Furthermore, since provenance information is typically large and complex, we present two solutions for its effective presentation as NL text: one that is based on provenance factorization, with novel desiderata relevant to the NL case, and one that is based on summarization. We have implemented our solution in an end-to-end system supporting questions, answers and provenance, all expressed in NL. Our experiments, including a user study, indicate the quality of our solution and its scalability.",
    "subtype": "research",
    "authors": [
      {
        "name": "Daniel Deutch",
        "affiliation": "Tel Aviv University"
      },
      {
        "name": "Nave Frost",
        "affiliation": "Tel Aviv University"
      },
      {
        "name": "Amir Gilad",
        "affiliation": "Tel Aviv University"
      }
    ],
    "type": "research",
    "id": "511"
  },
  "research1374": {
    "title": "Distributed Trajectory Similarity Search",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p1478-xie.pdf",
    "abstract": "Mobile and sensing devices have already become ubiquitous. They have made tracking moving objects an easy task. As a result, mobile applications like Uber and many IoT projects have generated massive amounts of trajectory data that can no longer be processed by a single machine efficiently. Among the typical query operations over trajectories, similarity search is a common yet expensive operator in querying trajectory data. It is useful for applications in different domains such as traffic and transportation optimizations, weather forecast and modeling, and sports analytics. It is also a fundamental operator for many important mining operations such as clustering and classification of trajectories. In this paper, we propose a distributed query framework to process trajectory similarity search over a large set of trajectories. We have implemented the proposed framework in Spark, a popular distributed data processing engine, by carefully considering different design choices. Our query framework supports both the Hausdorff distance the Fréchet distance. Extensive experiments have demonstrated the excellent scalability and query efficiency achieved by our design, compared to other methods and design alternatives.",
    "subtype": "research",
    "authors": [
      {
        "name": "Dong Xie",
        "affiliation": "University of Utah"
      },
      {
        "name": "Feifei Li",
        "affiliation": "University of Utah"
      },
      {
        "name": "Jeff Phillips",
        "affiliation": "University of Utah"
      }
    ],
    "type": "research",
    "id": "1374"
  },
  "research333": {
    "title": "A General Framework for Estimating Graphlet Statistics via Random Walk",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p253-chen.pdf",
    "abstract": "Graphlets are induced subgraph patterns and have been frequently applied to characterize the local topology structures of graphs across various domains, e.g., online social networks (OSNs) and biological networks. Discovering and computing graphlet statistics are highly challenging. First, the massive size of real-world graphs makes the exact computation of graphlets extremely expensive. Secondly, the graph topology may not be readily available so one has to resort to web crawling using the available application programming interfaces (APIs). In this work, we propose a general and novel framework to estimate graphlet statistics of ``any size''. Our framework is based on collecting samples through consecutive steps of random walks. We derive an analytic bound on the needed sample size (via the Chernoff-Hoeffding technique) to guarantee the convergence of our unbiased estimator. To further improve the accuracy, we introduce two novel optimization techniques to reduce the lower bound on the sample size. Experimental evaluations demonstrate that our methods outperform the state-of-the-art method up to an order of magnitude both in terms of accuracy and time cost.",
    "subtype": "research",
    "authors": [
      {
        "name": "Xiaowei Chen",
        "affiliation": "CUHK"
      },
      {
        "name": "Yongkun Li",
        "affiliation": "University of Science and Technology of China"
      },
      {
        "name": "Pinghui Wang",
        "affiliation": "Xi’an Jiaotong University"
      },
      {
        "name": "John C.S. Lui",
        "affiliation": "The CUHK"
      }
    ],
    "type": "research",
    "id": "333"
  },
  "research1371": {
    "title": "CleanM: An Optimizable Query Language for Unified Scale-Out Data Cleaning",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p1466-giannakopoulou.pdf",
    "abstract": "Data cleaning has become an indispensable part of data analysis due to the increasing amount of dirty data. Data scientists spend most of their time preparing dirty data before it can be used for data analysis. At the same time, the existing tools that attempt to automate the data cleaning procedure typically focus on a specific use case and operation. Still, even such specialized tools exhibit long running times or fail to process large datasets. Therefore, from a user’s perspective, one is forced to use a different, potentially inefficient tool for each category of errors. This paper addresses the coverage and efficiency problems of data cleaning. It introduces CleanM (pronounced clean’em), a language which can express multiple types of cleaning operations. CleanM goes through a three-level translation process for optimization purposes; a different family of optimizations is applied in each abstraction level. Thus, CleanM can express complex data cleaning tasks, optimize them in a unified way, and deploy them in a scale-out fashion. We validate the applicability of CleanM by using it on top of CleanDB, a newly designed and implemented framework which can query heterogeneous data. When compared to existing data cleaning solutions, CleanDB a) covers more data corruption cases, b) is up to ∼ 2× faster on average, scales better, and can handle cases for which its competitors are unable to terminate, and c) uses a single interface for querying and for data cleaning.",
    "subtype": "research",
    "authors": [
      {
        "name": "Stella Giannakopoulou",
        "affiliation": "EPFL"
      },
      {
        "name": "Manos Karpathiotakis",
        "affiliation": "EPFL"
      },
      {
        "name": "Benjamin Gaidioz",
        "affiliation": "EPFL"
      },
      {
        "name": "Anastasia Ailamaki",
        "affiliation": "EPFL"
      }
    ],
    "type": "research",
    "id": "1371"
  },
  "research1243": {
    "title": "Probabilistic Database Summarization for Interactive Data Exploration",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p1154-orr.pdf",
    "abstract": "We present a probabilistic approach to generate a small, query-able summary of a dataset for interactive data exploration. Departing from traditional summarization techniques, we use the Principle of Maximum Entropy to generate a probabilistic representation of the data that can be used to give approximate query answers. We develop the theoretical framework and formulation of our probabilistic representation and show how to use it to answer queries. We then present solving techniques and give three critical optimizations to improve preprocessing time and query accuracy. Lastly, we experimentally evaluate our work using a 5 GB dataset of flights within the United States and a 210 GB dataset from an astronomy particle simulation. While our current work only supports linear queries, we show that our technique can successfully answer queries faster than sampling while introducing, on average, no more error than sampling and can better distinguish between rare and nonexistent values.",
    "subtype": "research",
    "authors": [
      {
        "name": "Laurel Orr",
        "affiliation": "University of Washington"
      },
      {
        "name": "Dan Suciu",
        "affiliation": "University of Washington"
      },
      {
        "name": "Magdalena Balazinska",
        "affiliation": "University of Washington"
      }
    ],
    "type": "research",
    "id": "1243"
  },
  "research1386": {
    "title": "Optimizing Deep CNN-Based Queries over Video Streams at Scale",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p1586-kang.pdf",
    "abstract": "Video is one of the fastest-growing sources of data and is rich with interesting semantic information. Furthermore, recent advances in computer vision, in the form of deep convolutional neural networks (CNNs), have made it possible to query this semantic information with near-human accuracy (in the form of image tagging). However, performing inference with state-of-the-art CNNs is computationally expensive: analyzing videos in real time (at 30 frames/sec) requires a $1200 GPU per video stream, posing a serious computational barrier to CNN adoption in large-scale video data management systems. In response, we present sn, a system that uses cost-based optimization to assemble a specialized video processing pipeline for each input video stream, greatly accelerating subsequent CNN-based queries on the video. As sn observes a video, it trains two types of pipeline components (which we call filters) to exploit the locality in the video stream: {difference detectors} that exploit temporal locality between frames, and {specialized models} that are tailored to a specific scene and query (i.e., exploit environmental and query-specific locality). We show that the optimal set of filters and their parameters depends significantly on the video stream and query in question, so sn introduces an efficient cost-based optimizer for this problem to select them. With this approach, our sn prototype achieves up to 120-3,200$times$ speed-ups (318-8,500$times$ real-time) on binary classification tasks over real-world webcam and surveillance video while maintaining accuracy within 1-5% of a state-of-the-art CNN.",
    "subtype": "research",
    "authors": [
      {
        "name": "Daniel Kang",
        "affiliation": "Stanford University"
      },
      {
        "name": "John Emmons",
        "affiliation": "Stanford University"
      },
      {
        "name": "Firas Abuzaid",
        "affiliation": "Stanford University"
      },
      {
        "name": "Peter Bailis",
        "affiliation": "Stanford University"
      },
      {
        "name": "Matei Zaharia",
        "affiliation": "Stanford University"
      }
    ],
    "type": "research",
    "id": "1386"
  },
  "research857": {
    "title": "Heterogeneous Recommendations: What You Might Like To Read After Watching Interstellar",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p1070-patra.pdf",
    "abstract": "Recommenders, as widely implemented nowadays by major e-commerce players like Netflix or Amazon, use collaborative filtering to suggest the most relevant items to their users. Clearly, the effectiveness of recommenders depends on the data they can exploit, i.e., the feedback of users conveying their preferences, typically based on their past ratings. As of today, most recommenders are homogeneous in the sense that they utilize one specific application at a time. In short, Alice will only get recommended a movie if she has been rating movies. But what if she has been only rating books and would like to get recommendations for a movie? Clearly, the multiplicity of web applications is calling for heterogeneous recommenders that could utilize ratings in one application to provide recommendations in another one. This paper presents X-MAP, a heterogeneous recommender. X-MAP leverages meta-paths between heterogeneous items over several application domains, based on users who rated across these domains. These meta-paths are then used in X-MAP to generate, for every user, a profile (AlterEgo) in a domain where the user might not have rated any item yet. Not surprisingly, leveraging meta-paths poses non-trivial issues of (a) meta-path-based inter-item similarity, in order to enable accurate predictions, (b) scalability, given the amount of computation required, as well as (c) privacy, given the need to aggregate information across multiple applications. We show in this paper how X-MAP addresses the above-mentioned issues to achieve accuracy, scalability and differential privacy. In short, X-MAP weights the meta-paths based on several factors to compute inter-item similarities, and ensures scalability through a layer-based pruning technique. X-MAP guarantees differential privacy using an exponential scheme that leverages the meta-path-based similarities while determining the probability of item selection to construct the AlterEgos. We present an exhaustive experimental evaluation of X-MAP using real traces from Amazon. We show that, in terms of accuracy, X-MAP outperforms alternative heterogeneous recommenders and, in terms of throughput, X-MAP achieves a linear speedup with an increasing number of machines.",
    "subtype": "research",
    "authors": [
      {
        "name": "Rachid Guerraoui",
        "affiliation": "EPFL"
      },
      {
        "name": "Anne-Marie Kermarrec",
        "affiliation": "Inria"
      },
      {
        "name": "Tao Lin",
        "affiliation": "EPFL"
      },
      {
        "name": "Rhicheek Patra",
        "affiliation": "EPFL"
      }
    ],
    "type": "research",
    "id": "857"
  },
  "research851": {
    "title": "An Experimental Evaluation of Point-of-interest Recommendation in Location-based Social Networks",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p1010-liu.pdf",
    "abstract": "Point-of-interest (POI) recommendation is an important service to Location-Based Social Networks (LBSNs) that can benefit both users and businesses. In recent years, a number of POI recommender systems have been proposed, but there is still a lack of systematical comparison thereof. In this paper, we provide an all-around evaluation of 11 state-of-the-art POI recommendation models. From the evaluation, we obtain several important findings, based on which we can better understand and utilize POI recommendation models in various scenarios. We anticipate this work to provide readers with an overall picture of the cutting-edge research on POI recommendation.",
    "subtype": "research",
    "authors": [
      {
        "name": "Yiding Liu",
        "affiliation": "Nanyang Technological University"
      },
      {
        "name": "Tuan-Anh Pham",
        "affiliation": "Nanyang Technological University"
      },
      {
        "name": "Gao Cong",
        "affiliation": "Nanyang Technological University"
      },
      {
        "name": "Quan Yuan",
        "affiliation": "UIUC"
      }
    ],
    "type": "research",
    "id": "851"
  },
  "research508": {
    "title": "Truth Inference in Crowdsourcing: Is the Problem Solved?",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p541-zheng.pdf",
    "abstract": "Crowdsourcing has emerged as a novel problem-solving paradigm, which facilitates addressing problems that are hard for computers, e.g., entity resolution and sentiment analysis. However, due to the openness of crowdsourcing, workers may yield low-quality answers, and a redundancy-based method is widely employed, which first assigns each task to multiple workers and then infers the correct answer (called truth) for the task based on the answers of the assigned workers. A fundamental problem in this method is Truth Inference, which decides how to effectively infer the truth. Recently, the database community and data mining community independently study this problem and propose various algorithms. However, these algorithms are not compared extensively under the same framework and it is hard for practitioners to select appropriate algorithms. To alleviate this problem, we provide a detailed survey on 17 existing algorithms and perform a comprehensive evaluation using 5 real datasets. We make all codes and datasets public for future research. Through experiments we find that existing algorithms are not stable across different datasets and there is no algorithm that outperforms others consistently. We believe that the truth inference problem is not fully solved, and identify the limitations of existing algorithms and point out promising research directions.",
    "subtype": "research",
    "authors": [
      {
        "name": "Yudian Zheng",
        "affiliation": "Hong Kong University"
      },
      {
        "name": "Guoliang Li",
        "affiliation": "Tsinghua University"
      },
      {
        "name": "Yuanbing Li",
        "affiliation": "Tsinghua University"
      },
      {
        "name": "Caihua Shan",
        "affiliation": "Hong Kong University"
      },
      {
        "name": "Reynold Cheng",
        "affiliation": "Hong Kong University"
      }
    ],
    "type": "research",
    "id": "508"
  },
  "research513": {
    "title": "An Experimental Evaluation of SimRank-based Similarity Search Algorithms",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p601-zhang.pdf",
    "abstract": "Given a graph, SimRank is one of the most popular measures of the similarity between two vertices. We focus on efficiently calculating SimRank, which has been studied intensively over the last decade. This has led to many algorithms that efficiently calculate or approximate SimRank being proposed by researchers. Despite these abundant research efforts, there is no systematic comparison of these algorithms. In this paper, we conduct a study to compare these algorithms to understand their pros and cons. We first introduce a taxonomy for different algorithms that calculate SimRank and classify each algorithm into one of the following three classes, namely, { iterative-}, { non-iterative-}, and { random walk-based} method. We implement ten algorithms published from 2002 to 2015, and compare them using both synthetic and real-world graphs. To ensure the fairness of our study, our implementations use the same data structure and execution framework, and we try our best to optimize each of these algorithms. Our study reveals that none of these algorithms dominates the others: algorithms based on iterative method often have higher accuracy while algorithms based on random walk can be more scalable. One non-iterative algorithm has good effectiveness and efficiency on graphs with medium size. Thus, depending on the requirements of different applications, the optimal choice of algorithms differs. This paper provides an empirical guideline for making such choices.",
    "subtype": "research",
    "authors": [
      {
        "name": "Zhipeng Zhang",
        "affiliation": "Peking University"
      },
      {
        "name": "Yingxia Shao",
        "affiliation": "PKU"
      },
      {
        "name": "Bin Cui",
        "affiliation": "Peking University"
      },
      {
        "name": "Ce Zhang",
        "affiliation": "ETH"
      }
    ],
    "type": "research",
    "id": "513"
  },
  "research570": {
    "title": "Data Tweening: Incremental Visualization of Data Transforms",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p661-khan.pdf",
    "abstract": "In the context of interactive query sessions, it is common to issue a succession of queries, transforming a dataset to the desired result. It is often difficult to comprehend a suc- cession of transformations, especially for complex queries. Thus, to facilitate understanding of each data transforma- tion and to provide continuous feedback, we introduce the concept of “data tweening”, i.e., interpolating between re- sultsets, presenting to the user a series of incremental visual representations of a resultset transformation. We present tweening methods that consider not just the changes in the result, but also the changes in the query. Through user stud- ies, we show that data tweening allows users to efficiently comprehend data transforms, and also enables them to gain a better understanding of the underlying query operations.",
    "subtype": "research",
    "authors": [
      {
        "name": "Meraj Ahmed Khan",
        "affiliation": "The Ohio State University"
      },
      {
        "name": "Larry Xu",
        "affiliation": "UC Berkeley"
      },
      {
        "name": "Arnab Nandi",
        "affiliation": "Ohio State University"
      },
      {
        "name": "Joseph Hellerstein",
        "affiliation": "UC Berkeley"
      }
    ],
    "type": "research",
    "id": "570"
  },
  "research680": {
    "title": "Looking Ahead Makes Query Plans Robust",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p889-zhu.pdf",
    "abstract": "Query optimizers and query execution engines cooperate to deliver high performance on complex analytic queries. Typically, the optimizer searches through the plan space and sends a selected plan to the execution engine. However, optimizers may at times miss the optimal plan, with sometimes disastrous impact on performance. In this paper, we develop the notion of robustness of a query evaluation strategy with respect to a space of query plans. We also propose a novel query execution strategy called Lookahead Information Passing (LIP) that is robust with respect to the space of (fully pipeline-able) left-deep query plan trees for in-memory star schema data warehouses. LIP ensures that execution times for the best and the worst case plans are far closer than without LIP. In fact, under certain assumptions of independent and uniform distributions, any plan in that space is theoretically guaranteed to execute in near-optimal time. LIP ensures that the execution time for every plan in the space is nearly-optimal. In this paper, we also evaluate these claims using workloads that include skew and correlation. With LIP we make an initial foray into a novel way of thinking about robustness from the perspective of query evaluation, where we develop strategies (like LIP) that collapse plan sub-spaces in the overall global plan space.",
    "subtype": "research",
    "authors": [
      {
        "name": "Jianqiao Zhu",
        "affiliation": "UW-Madison"
      },
      {
        "name": "Navneet Potti",
        "affiliation": "UW-Madison"
      },
      {
        "name": "Saket Saurabh",
        "affiliation": "UW-Madison"
      },
      {
        "name": "Jignesh Patel",
        "affiliation": "UW-Madison"
      }
    ],
    "type": "research",
    "id": "680"
  },
  "research1365": {
    "title": "On Sampling from Massive Graph Streams",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p1430-ahmed.pdf",
    "abstract": "We propose Graph Priority Sampling (GPS), a new paradigm for order-based reservoir sampling from massive streams of graph edges. GPS provides a general way to weight edge sampling according to auxiliary and/or size variables so as to accomplish various estimation goals of graph properties. In the context of subgraph counting, we show how edge sampling weights can be chosen so as to minimize the estimation variance of counts of specified sets of subgraphs. In distinction with many prior graph sampling schemes, GPS separates the functions of edge sampling and subgraph estimation. We propose two estimation frameworks: (1) Post-Stream estimation, to allow GPS to construct a reference sample of edges to support retrospective graph queries, and (2) In-Stream estimation, to allow GPS to obtain lower variance estimates by incrementally updating the subgraph count estimates during stream processing. Unbiasedness of subgraph estimators is established through a new Martingale formulation of graph stream order sampling, in which subgraph estimators, written as a product of constituent edge estimators, are unbiased, even when computed at different points in the stream. The separation of estimation and sampling enables significant resource savings relative to previous work. We illustrate our framework with applications to triangle and wedge counting. We perform a large-scale experimental study on real-world graphs from various domains and types. GPS achieves high accuracy with less than 1% error for triangle and wedge counting, while storing a small fraction of the graph with average update times of a few microseconds per edge. Notably, for billion-scale graphs, GPS accurately estimates triangle and wedge counts with less than 1% error, while storing a small fraction of less than 0.01% of the total edges in the graph.",
    "subtype": "research",
    "authors": [
      {
        "name": "Nesreen Ahmed",
        "affiliation": "Intel Labs"
      },
      {
        "name": "Nick Duffield",
        "affiliation": "Texas A&M University"
      },
      {
        "name": "Theodore Willke",
        "affiliation": "Intel Labs"
      },
      {
        "name": "Ryan Rossi",
        "affiliation": "PARC"
      }
    ],
    "type": "research",
    "id": "1365"
  },
  "research1053": {
    "title": "Mison: A Fast JSON Parser for Data Analytics",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p1118-li.pdf",
    "abstract": "The growing popularity of the JSON format has fueled increased interest in loading and processing JSON data within analytical data processing systems. However, in many applications, JSON parsing dominates performance and cost. In this paper, we present a new JSON parser called Mison that is particularly tailored to this class of applications, by pushing down both projection and filter operators of analytical queries into the parser. To achieve these features, we propose to deviate from the traditional approach of building parsers using finite state machines (FSMs). Instead, we follow a two-level approach that enables the parser to jump directly to the correct position of a queried field without having to perform expensive tokenizing steps to find the field. At the upper level, Mison speculatively predicts the logical locations of queried fields based on previously seen patterns in a dataset. At the lower level, Mison builds structural indices on JSON data to map logical locations to physical locations. Unlike all existing FSM-based parsers, building structural indices converts control flow into data flow, thereby largely eliminating inherently unpredictable branches in the program and exploiting the parallelism available in modern processors. We experimentally evaluate Mison using representative real-world JSON datasets and the TPC-H benchmark, and show that Mison produces significant performance benefits over the best existing JSON parsers; in some cases, the performance improvement is over one order of magnitude.",
    "subtype": "research",
    "authors": [
      {
        "name": "Yinan Li",
        "affiliation": "Microsoft Research"
      },
      {
        "name": "Nikos R. Katsipoulakis",
        "affiliation": "University of Pittsburgh"
      },
      {
        "name": "Badrish Chandramouli",
        "affiliation": "Microsoft Research"
      },
      {
        "name": "Jonathan Goldstein",
        "affiliation": "Microsoft Research"
      },
      {
        "name": "Donald Kossmann",
        "affiliation": "ETH"
      }
    ],
    "type": "research",
    "id": "1053"
  },
  "research223": {
    "title": "Sapprox: Enabling Efficient and Accurate Approximations on Sub-datasets with Distribution-aware Online Sampling",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p109-zhang.pdf",
    "abstract": "In this paper, we aim to enable both efficient and accurate approximations on arbitrary sub-dataset of a large dataset. Due to the prohibitive storage overhead of caching offline samples for each sub-dataset, existing offline sample based systems provide high accuracy results for only a limited number of sub-datasets, such as the popular ones. On the other hand, current online sample based approximation systems, which generate samples at runtime, do not take into account the uneven storage distribution of a sub-dataset. They work well for uniform distribution of a sub-dataset while suffer low sampling efficiency and poor estimation accuracy on unevenly distributed sub-datasets. To address the problem, we develop a distribution aware method called Sapprox. Our idea is to collect the occurrences of a sub-dataset at each logical partition of a dataset (storage distribution) in the distributed system, and make good use of such information to facilitate online sampling. There are three thrusts in Sapprox. First, we develop a probabilistic map to reduce the exponential number of recorded sub-datasets to a linear one. Second, we apply the cluster sampling with unequal probability theory to implement a distribution-aware sampling method for efficient online sub-dataset sampling. Third, we quantitatively derive the optimal sampling unit size in a distributed file system by associating it with approximation costs and accuracy. %Finally, we leverage the sampling theory to compute error bounds for approximations in MapReduce-like systems. We have implemented Sapprox into Hadoop ecosystem as an example system and open sourced it on GitHub. Our comprehensive experimental results show that Sapprox can achieve a speedup by up to 20x over the precise execution.",
    "subtype": "research",
    "authors": [
      {
        "name": "Xuhong Zhang",
        "affiliation": "University of Central Florida"
      },
      {
        "name": "Jun Wang",
        "affiliation": "University of Central Florida"
      },
      {
        "name": "Jiangling Yin",
        "affiliation": "University of Central Florida"
      },
      {
        "name": "Shouling Ji",
        "affiliation": "Georgia Institute of Technology"
      }
    ],
    "type": "research",
    "id": "223"
  },
  "industrial843": {
    "title": "Dimensions Based Data Clustering and Zone Maps",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p1622-ziauddin.pdf",
    "abstract": "In recent years, the data warehouse industry has witnessed decreased use of indexing but increased use of compression and clustering of data facilitating efficient data access and data pruning in the query processing area. A classic example of data pruning is the partition pruning, which is used when table data is range or list partitioned. But lately, techniques have been developed to prune data at a lower granularity than a table partition or sub-partition. A good example is the use of data pruning structure called zone map. A zone map prunes zones of data from a table on which it is defined. Data pruning via zone map is very effective when the table data is clustered by the filtering columns. The database industry has offered support to cluster data in tables by its local columns, and to define zone maps on clustering columns of such tables. This has helped improve the performance of queries that contain filter predicates on local columns. However, queries in data warehouses are typically based on star/snowflake schema with filter predicates usually on columns of the dimension tables joined to a fact table. Given this, the performance of data warehouse queries can be significantly improved if the fact table data is clustered by columns of dimension tables together with zone maps that maintain min/max value ranges of these clustering columns over zones of fact table data. In recognition of this opportunity of significantly improving the performance of data warehouse queries, Oracle 12c release 1 has introduced the support for dimension based clustering of fact tables together with data pruning of the fact tables via dimension based zone maps.",
    "subtype": "industrial",
    "authors": [
      {
        "name": "Mohamed Ziauddin",
        "affiliation": "Oracle"
      },
      {
        "name": "Andrew Witkowski",
        "affiliation": "Oracle"
      },
      {
        "name": "You Jung Kim",
        "affiliation": "Oracle"
      },
      {
        "name": "Janaki Lahorani",
        "affiliation": "Oracle"
      },
      {
        "name": "Dmitry Potapov",
        "affiliation": "Oracle"
      },
      {
        "name": "Murali Krishna",
        "affiliation": "Oracle"
      }
    ],
    "type": "industrial",
    "id": "843"
  },
  "research306": {
    "title": "Efficient Computation of Feedback Arc Set at Web-Scale",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p133-simpson.pdf",
    "abstract": "The minimum feedback arc set problem is an NP-hard problem on graphs that seeks a minimum set of arcs which, when removed from the graph, leave it acyclic. In this work, we investigate several approximations for computing a minimum feedback arc set with the goal of comparing the quality of the solutions and the running times. Our investigation is motivated by applications in Social Network Analysis such as misinformation removal and label propagation. We present careful algorithmic engineering for multiple algorithms to improve the scalability of each approach. In particular, two approaches we optimize (one greedy and one randomized) provide a nice balance between feedback arc set size and running time complexity. We experimentally compare the performance of a wide range of algorithms on a broad selection of large online networks including Twitter, LiveJournal, and the Clueweb12 dataset. The experiments reveal that our greedy and randomized implementations outperform the other approaches by simultaneously computing a feedback arc set of competitive size and scaling to web-scale graphs with billions of vertices and tens of billions of arcs. Finally, we extend the algorithms considered to the probabilistic case in which arcs are realized with some fixed probability and provide detailed experimental comparisons.",
    "subtype": "research",
    "authors": [
      {
        "name": "Michael Simpson",
        "affiliation": "University of Victoria"
      },
      {
        "name": "Venkatesh Srinivasan",
        "affiliation": "University of Victoria"
      },
      {
        "name": "Alex Thomo",
        "affiliation": "University of Victoria"
      }
    ],
    "type": "research",
    "id": "306"
  },
  "research1396": {
    "title": "Quill: Efficient, Transferable, and Rich Analytics at Scale",
    "acm_link": "http://www.vldb.org/pvldb/vol9/p1623-chandramouli.pdf",
    "abstract": "This paper introduces Quill (stands for a quadrillion tuples per day), a library and distributed platform for relational and temporal analytics over large datasets in the cloud. Quill exposes a new abstraction for parallel datasets and computation, called ShardedStreamable. This abstraction provides the ability to express efficient distributed physical query plans that are transferable, i.e., movable from offline to real-time and vice versa. ShardedStreamable decouples incremental query logic specification, a small but rich set of data movement operations, and keying; this allows Quill to express a broad space of plans with complex querying functionality, while leveraging existing temporal libraries such as Trill. Quill's layered architecture provides a careful separation of responsibilities with independently useful components, while retaining high performance. We built Quill for the cloud, with a master-less design where a language-integrated client library directly communicates and coordinates with cloud workers using off-the-shelf distributed cloud components such as queues. Experiments on up to 400 cloud machines, and on datasets up to 1TB, find Quill to incur low overheads and outperform SparkSQL by up to orders-of-magnitude for temporal and 6X for relational queries, while supporting a rich space of transferable, programmable, and expressive distributed physical query plans.",
    "subtype": "research",
    "authors": [
      {
        "name": "Badrish Chandramouli",
        "affiliation": "Microsoft Research"
      },
      {
        "name": "Raul Castro Fernandez",
        "affiliation": "MIT"
      },
      {
        "name": "Jonathan Goldstein",
        "affiliation": "Microsoft Research"
      },
      {
        "name": "Ahmed Eldawy",
        "affiliation": "University of Minnesota"
      },
      {
        "name": "Abdul Quamar",
        "affiliation": "University of Maryland "
      }
    ],
    "type": "research",
    "id": "1396"
  },
  "research409": {
    "title": "BlueCache: A Scalable Distributed Flash-based Key-value Store",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p301-xu.pdf",
    "abstract": "A key-value store (KVS), such as memcached and Redis, is widely used as a caching layer to augment the slower persistent backend storage in data centers. DRAM-based KVS provides fast key-value access, but its scalability is limited by the cost, power and space needed by the machine cluster to support a large amount of DRAM. This paper offers a 10X to 100X cheaper solution based on flash storage and hardware accelerators. In BlueCache key-value pairs are stored in flash storage and all KVS operations, including the flash controller are directly implemented in hardware. Furthermore, BlueCache includes a fast interconnect between flash controllers to provide a scalable solution. We show that BlueCache has 4.18X higher throughput and consumes 25X less power than a flash-backed KVS software implementation on x86 servers. We further show that BlueCache can outperform DRAM-based KVS when the latter has more than 7.4% misses for a read-intensive application. BlueCache is an attractive solution for both rack-level appliances and data-center-scale key-value cache.",
    "subtype": "research",
    "authors": [
      {
        "name": "Shuotao Xu",
        "affiliation": "MIT"
      },
      {
        "name": "Sungjin Lee",
        "affiliation": "Inha University"
      },
      {
        "name": "Sang-Woo Jun",
        "affiliation": "Massachusetts Institute of Technology"
      },
      {
        "name": "Ming Liu",
        "affiliation": "Massachusetts Institute of Technology"
      },
      {
        "name": "Jamey Hicks",
        "affiliation": "Accelerated Tech"
      },
      {
        "name": "Arvind",
        "affiliation": "MIT"
      }
    ],
    "type": "research",
    "id": "409"
  },
  "research1401": {
    "title": "Fast and Adaptive Indexing of Multi-Dimensional Observational Data",
    "acm_link": "http://www.vldb.org/pvldb/vol9/p1683-wang.pdf",
    "abstract": "Tremendous amounts of data are being generated by sensing devices each day, which include large quantities of multi-dimensional measurements. These data are expected to be immediately available for real-time analytics as they are streamed into storage. Such scenarios pose challenges to state-of-the-art indexing methods, as they must not only support efficient queries but also frequent updates. In this paper, we propose a novel indexing method that ingests multi-dimensional observational data in real time. This method primarily guarantees extremely high throughput for data ingestion, while it can be continuously refined in the background to improve query efficiency. Instead of representing collections of points using Minimal Bounding Boxes as in conventional indexes, we model sets of successive points as line segments in hyperspaces, by exploiting the intrinsic value continuity in observational data. Such a representation reduces the number of index entries and drastically reduces “over-coverage” by entries. Our experimental results show that our proposed approach handles real-world workloads gracefully, providing both low-overhead indexing and excellent query efficiency.",
    "subtype": "research",
    "authors": [
      {
        "name": "Sheng Wang",
        "affiliation": "NUS"
      },
      {
        "name": "David Maier",
        "affiliation": "Portland State University"
      },
      {
        "name": "Beng Chin Ooi",
        "affiliation": "NUS"
      }
    ],
    "type": "research",
    "id": "1401"
  },
  "research863": {
    "title": "A Data Quality Metric (DQM): How to Estimate the Number of Undetected Errors in Data Sets",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p1094-chung.pdf",
    "abstract": "Data cleaning, whether manual or algorithmic, is rarely perfect leaving a dataset with an unknown number of false positives and false negatives after cleaning. In many scenarios, quantifying the number of remaining errors is challenging because our data integrity rules themselves may be incomplete, or the available gold-standard datasets may be too small to extrapolate. As the use of inherently fallible crowds becomes more prevalent in data cleaning problems, it is important to have estimators to quantify the extent of such errors. We propose novel species estimators to estimate the number of distinct remaining errors in a dataset after it has been cleaned by a set of crowd workers -- essentially, quantifying the utility of hiring additional workers to clean the dataset. This problem requires new estimators that are robust to false positives and false negatives, and we empirically show on three real-world datasets that existing species estimators are unstable for this problem, while our proposed techniques quickly converge.",
    "subtype": "research",
    "authors": [
      {
        "name": "Yeounoh Chung",
        "affiliation": "Brown University"
      },
      {
        "name": "Sanjay Krishnan",
        "affiliation": "UC Berkeley"
      },
      {
        "name": "Tim Kraska",
        "affiliation": "Brown University"
      }
    ],
    "type": "research",
    "id": "863"
  },
  "research1297": {
    "title": "Minimal OnRoad Time Route Scheduling on Time-Dependent Graph",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p1274-zhou.pdf",
    "abstract": "On time-dependent graph, fastest path query is an important problem and has been well studied. It focuses on minimizing the total travel time(waiting time + on-road time) but does not allow waiting on any intermediate vertex if FIFO property is applied. However, in practice, waiting on a vertex has the ability to reduce the time spent on road (for example, resuming traveling after a traffic jam). In this paper, we study how to find a path with the minimal on-road time on time-dependent graph by allowing waiting on some predefined parking vertices. The existing works are based on the following fact: the arrival time of a vertex v is determined by the arrival time of its in-neighbor u, which does not hold in our scenario since we also consider the waiting time on u if u allowing waiting. Thus, determining the waiting time on each parking vertex to achieve the minimal on-road time becomes a big challenge, which further breaks FIFO property. To cope with this challenging problem, we propose two efficient algorithms using minimum on-road travel cost function to answer the query. The evaluations on real-world road networks show that the proposed algorithms are more accurate and efficient than the extensions of existing algorithms. In addition to that, the results further indicate, if the parking facilities are enabled in the route scheduling algorithms, the on-road time will reduce significantly compared to the fastest path algorithm.",
    "subtype": "research",
    "authors": [
      {
        "name": "Lei Li",
        "affiliation": "University of Queensland"
      },
      {
        "name": "Wen Hua",
        "affiliation": "University of Queensland"
      },
      {
        "name": "Xingzhong Du",
        "affiliation": "University of Queensland"
      },
      {
        "name": "Xiaofang Zhou",
        "affiliation": "University of Queensland"
      }
    ],
    "type": "research",
    "id": "1297"
  },
  "research1363": {
    "title": "LDA*: A Robust and Large-scale Topic Modeling System",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p1406-yu.pdf",
    "abstract": "We present LDA*, a system that has been deployed in one of the largest Internet companies to fulfil their requirements of “topic modeling as an internal service”—relying on thousands of machines,engineers in different sectors submit their data, some are as large as 1.8TB, to LDA* and get results back in hours. LDA* is motivated by the observation that none of the existing topic modeling systems is robust enough—Each of these existing systems is designed for a specific point in the trade-off space that can be suboptimal, sometimes by up to 10×, across workloads. Our first contribution is a systematic study of all recently proposed samplers: AliasLDA, F+LDA, LightLDA, and WarpLDA. We discovered a novel system trade-off among these samplers. Each sampler has different sampling complexity and performs differently, sometimes by 5×, on documents with different lengths. Based on this trade-off, we further developed a hybrid sampler that uses different samplers for different types of documents. This hybrid approach works across a wide range of workloads and outperforms the existing samplers by up to 2×. We then focused on distributed environments in which thousands of workers, each with different performance (due to virtualization and resource sharing), coordinate to train a topic model. Our second contribution is an asymmetric parameter server architecture that pushes some computation to the parameter server side. This architecture is motivated by the skew of the word frequency distribution and a novel trade-off we discovered between communication and computation. With this architecture, we outperform the traditional, symmetric architecture by up to 2×. With these two contributions, together with a carefully engineered implementation, our system is able to outperform existing systems by up to 10× and has already been running to provide topic modeling services for more than six months.",
    "subtype": "research",
    "authors": [
      {
        "name": "Lele Yu",
        "affiliation": "Peking University"
      },
      {
        "name": "Bin Cui",
        "affiliation": "Peking University"
      },
      {
        "name": "Ce Zhang",
        "affiliation": "ETH"
      },
      {
        "name": "Yingxia Shao",
        "affiliation": "PKU"
      }
    ],
    "type": "research",
    "id": "1363"
  },
  "research384": {
    "title": "Finding Persistent Items in Data Streams",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p289-dai.pdf",
    "abstract": "Frequent item mining, which deals with finding items that occur frequently in a given data stream over a period of time, is one of the heavily studied problems in data stream mining. A generalized version of frequent item mining is the persistent item mining, where a persistent item, unlike a frequent item, does not necessarily occur more frequently compared to other items over a short period of time, rather persists and occurs more frequently over a long period of time. To the best of our knowledge, there is no prior work on mining persistent items in a data stream. In this paper, we address the fundamental problem of finding persistent items in a given data stream during a given period of time at any given observation point. We propose a novel scheme, PIE, that can accurately identify each persistent item with a probability greater than any desired false negative rate (FNR) while using a very small amount of memory. The key idea of PIE is that it uses Raptor codes to encode the ID of each item that appears at the observation point during a measurement period and stores only a few bits of the encoded ID in the memory of that observation point during that measurement period. The item that is persistent occurs in enough measurement period- s that enough encoded bits for the ID can be retrieved from the observation point to decode them correctly and get the ID of the persistent item. We implemented and extensively evaluated PIE using three real network traffic traces and compared its performance with two prior adapted schemes. Our results show that not only PIE achieves the desired FNR in every scenario, its FNR, on average, is 19.5 times smaller than the FNR of the best adapted prior art.",
    "subtype": "research",
    "authors": [
      {
        "name": "Haipeng Dai",
        "affiliation": "Nanjing University"
      },
      {
        "name": "Muhammad Shahzad",
        "affiliation": "North Carolina State University"
      },
      {
        "name": "Alex X. Liu",
        "affiliation": "Nanjing University"
      },
      {
        "name": "Yuankun Zhong",
        "affiliation": "Nanjing University"
      }
    ],
    "type": "research",
    "id": "384"
  },
  "research489": {
    "title": "MapReduce and Streaming Algorithms for Diversity Maximization in Metric Spaces of Bounded Doubling Dimension",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p469-ceccarello.pdf",
    "abstract": "Given a dataset of points in a metric space and an integer $k$, a diversity maximization problem requires determining a subset of $k$ points maximizing some diversity objective measure, e.g., the minimum or the average distance between a pair of points in the subset. Diversity maximization is computationally hard, hence only approximate solutions can be hoped for. Although its applications are mainly in massive data analysis, most of the past research on diversity maximization has concentrated on the standard sequential setting. In this work we present space and pass/round-efficient diversity maximization algorithms for the Streaming and MapReduce models and analyze their approximation guarantees for the relevant class of metric spaces of bounded doubling dimension. Similarly to other approaches in the literature, our algorithms revolve upon the determination of high-quality core-sets, that is, (much) smaller subsets of the input which contain good approximations to the optimal solution for the whole input. For a variety of diversity objective functions, our algorithms attain an $(alpha+epsilon)$-approximation ratio, for any constant $epsilon > 0$, where $alpha$ is the best approximation ratio achieved by a polynomial-time, linear-space sequential algorithm for the same diversity objective. This is a substantial improvement compared to the approximation ratios attainable in the Streaming and MapReduce models by state-of-the-art algorithms for the case of general metric spaces. We also provide extensive experimental evidence of the practical relevance of our algorithms.",
    "subtype": "research",
    "authors": [
      {
        "name": "Matteo Ceccarello",
        "affiliation": "University of Padova"
      },
      {
        "name": "Andrea Pietracaprina",
        "affiliation": "University of Padova"
      },
      {
        "name": "Geppino Pucci",
        "affiliation": "University of Padova"
      },
      {
        "name": "Eli Upfal",
        "affiliation": "Brown University"
      }
    ],
    "type": "research",
    "id": "489"
  },
  "research1364": {
    "title": "Social Hash Partitioner: A Scalable Distributed Hypergraph Partitioner",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p1418-pupyrev.pdf",
    "abstract": "We design and implement a distributed algorithm for balanced k-way hypergraph partitioning that minimizes fanout, a fundamental hypergraph quantity also known as the communication volume and (k-1)-cut metric, by optimizing a novel objective we call probabilistic fanout. This choice allows our simple local heuristic algorithm to achieve comparable solution quality to the best existing hypergraph partitioners. Our algorithm is arbitrarily scalable due to a careful design that controls computational complexity, space complexity, and communication. In practice, we commonly process hypergraphs with billions of vertices and hyperedges in a few hours. We explain how the algorithm's scalability, both in terms of hypergraph size and bucket count, is limited only by the number of machines available. We perform an extensive comparison to existing distributed hypergraph partitioners and find that our approach is able to optimize hypergraphs roughly 100 times bigger on the same set of machines. We call the resulting tool Social Hash Partitioner (SHP), and accompanying this paper, we open-source the most scalable version based on recursive bisection.",
    "subtype": "research",
    "authors": [
      {
        "name": "Igor Kabiljo",
        "affiliation": "Facebook"
      },
      {
        "name": "Brian Karrer",
        "affiliation": "Facebook"
      },
      {
        "name": "Mayank Pundir",
        "affiliation": "Facebook"
      },
      {
        "name": "Sergey Pupyrev",
        "affiliation": "Facebook"
      },
      {
        "name": "Alon Shalita",
        "affiliation": "Facebook"
      },
      {
        "name": "Yaroslav Akhremtsev",
        "affiliation": "Karlsruhe Institute of Technology"
      },
      {
        "name": "Alessandro Presta",
        "affiliation": "Google"
      }
    ],
    "type": "research",
    "id": "1364"
  },
  "research853": {
    "title": "Auto-Join: Joining Tables by Leveraging Transformations",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p1034-he.pdf",
    "abstract": "Traditional equi-join relies solely on string comparisons to perform joins. However, in scenarios such as ad-hoc data analysis in spreadsheets, users increasingly need to join tables whose join-columns use different textual representa- tions, for which transformations are needed before equi-join can be executed. We develop an Auto-Join system that can automatically search over a rich space of operators to compose a program, whose execution makes input tables equi-join-able. Our evaluation using real test cases collected from both public web tables and proprietary enterprise tables shows that the proposed system can perform the desired syntactic joins with high quality.",
    "subtype": "research",
    "authors": [
      {
        "name": "Erkang Zhu",
        "affiliation": "University of Toronto"
      },
      {
        "name": "Yeye He",
        "affiliation": "Microsoft Research"
      },
      {
        "name": "Surajit Chaudhuri",
        "affiliation": "Microsoft Research"
      }
    ],
    "type": "research",
    "id": "853"
  },
  "research411": {
    "title": "A General and Parallel Platform for Mining Co-Movement Patterns over Large-scale Trajectories",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p313-fan.pdf",
    "abstract": "Discovering co-movement patterns from large-scale trajectory databases is an important mining task and has a wide spectrum of applications. Previous studies have identified several types of interesting co-movement patterns and showcased their usefulness. In this paper, we make two key contributions to this research field. First, we propose a more general co-movement pattern to unify those defined in the past literature. Second, we propose two types of parallel and scalable frameworks and deploy them on Apache Spark. To the best of our knowledge, this is the first work to mine co-movement patterns in real life trajectory databases with hundreds of millions of points. Experiments on three real life large-scale trajectory datasets have verified the efficiency and scalability of our proposed solutions.",
    "subtype": "research",
    "authors": [
      {
        "name": "Qi Fan",
        "affiliation": "NUS"
      },
      {
        "name": "Dongxiang Zhang",
        "affiliation": "NUS"
      },
      {
        "name": "Huayu Wu",
        "affiliation": "NUS"
      },
      {
        "name": "Kian-Lee Tan",
        "affiliation": "NUS"
      }
    ],
    "type": "research",
    "id": "411"
  },
  "research331": {
    "title": "Fast Algorithm for the Lasso based L1-Graph Construction",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p229-fujiwara.pdf",
    "abstract": "The lasso-based L_1-graph is used in many applications since it can effectively model a set of data points as a graph. The lasso is a popular regression approach and the L_1-graph represents data points as nodes by using the regression result. More specifically, by solving the L_1-optimization problem of the lasso, the sparse regression coefficients are used to obtain the weights of the edges in the graph. Conventional graph structures such as k-NN graph use two steps, adjacency searching and weight selection, for constructing the graph whereas the lasso-based L_1-graph derives the adjacency structure as well as the edge weights simultaneously by using a coordinate descent. However, the construction cost of the lasso-based L_1-graph is impractical for large data sets since the coordinate descent iteratively updates the weights of all edges until convergence. Our proposal, Castnet, can efficiently construct the lasso-based L_1-graph. In order to avoid updating the weights of all edges, we prune edges that cannot have nonzero weights before entering the iterations. In addition, we update edge weights only if they are nonzero in the iterations. Experiments show that Castnet is significantly faster than existing approaches.",
    "subtype": "research",
    "authors": [
      {
        "name": "Yasuhiro Fujiwara",
        "affiliation": "NTT"
      },
      {
        "name": "Yasutoshi Ida",
        "affiliation": "NTT"
      },
      {
        "name": "Junya Arai",
        "affiliation": "NTT"
      },
      {
        "name": "Mai Nishimura",
        "affiliation": "NTT"
      },
      {
        "name": "Sotetsu Iwamura",
        "affiliation": "NTT"
      }
    ],
    "type": "research",
    "id": "331"
  },
  "research678": {
    "title": "Cümülön-D: Data Analytics in a Dynamic Spot Market",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p865-yang.pdf",
    "abstract": "We present a system called Cümülön-D for matrix-based data analysis in a spot market of a public cloud. Prices in such markets fluctuate over time: while users can acquire machines usually at a very low bid price, the cloud can terminate these machines as soon as the market price exceeds their bid price. The distinguishing features of Cümülön-D include its continuous, proactive adaptation to the changing market, and its ability to quantify and control the monetary risk involved in paying for a workflow execution. We solve the dynamic optimization problem in a principled manner with a Markov decision process, and account for practical details that are often ignored previously but nonetheless important to performance. We evaluate Cümülön-D's effectiveness and advantages over previous approaches with experiments on Amazon EC2.",
    "subtype": "research",
    "authors": [
      {
        "name": "Botong Huang",
        "affiliation": "Duke University"
      },
      {
        "name": "Jun Yang",
        "affiliation": "Duke University"
      }
    ],
    "type": "research",
    "id": "678"
  },
  "research321": {
    "title": "Computing Longest Increasing Subsequences over Sequential Data Streams",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p181-zou.pdf",
    "abstract": "In this paper, we propose a data structure, a quadruple neighbor list (QN-list, for short), to support real time queries of all {l}ongest {i}ncreasing {s}ubsequence (LIS) and LIS with constraints over sequential data streams. The data structure built by our algorithm requires $O(w)$ space, where $w$ is the time window size. The running time for building the initial data structure takes $O(wlog w)$ time. Applying the data structure, insertion of the new item takes $O(log w)$ time and deletion of the first item takes $O(w)$ time. To the best of our knowledge, this is the first work to support both LIS enumeration and LIS with constraints computation by using a single uniform data structure for real time sequential data streams. Our method outperforms the state-of-the-art methods in both time and space cost, not only theoretically, but also empirically.",
    "subtype": "research",
    "authors": [
      {
        "name": "Youhuan Li",
        "affiliation": "Peking University"
      },
      {
        "name": "Lei Zou",
        "affiliation": "Peking University"
      },
      {
        "name": "Huaming Zhang",
        "affiliation": "University of Alabama in Huntsville"
      },
      {
        "name": "Dongyan Zhao",
        "affiliation": "Peking University"
      }
    ],
    "type": "research",
    "id": "321"
  },
  "research1397": {
    "title": "Perturbation Analysis of Database Queries",
    "acm_link": "http://www.vldb.org/pvldb/vol9/p1635-walenz.pdf",
    "abstract": "We present a system, Perada, for parallel perturbation analysis of database queries. Perturbation analysis considers the results of a query evaluated with (a typically large number of) different parameter settings, to help discover leads and evaluate claims from data. Perada simplifies the development of general, ad hoc perturbation analysis by providing a flexible API to support a variety of optimizations such as grouping, memoization, and pruning; by automatically optimizing performance through run-time observation, learning, and adaptation; and by hiding the complexity of concurrency and failures from its developers. We demonstrate Perada's efficacy and efficiency with real workloads applying perturbation analysis to computational journalism.",
    "subtype": "research",
    "authors": [
      {
        "name": "Brett Walenz",
        "affiliation": "Duke University"
      },
      {
        "name": "Jun Yang",
        "affiliation": "Duke University"
      }
    ],
    "type": "research",
    "id": "1397"
  },
  "research1317": {
    "title": "In Search of an Entity Resolution OASIS: Optimal Asymptotic Sequential Importance Sampling",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p1322-rubinstein.pdf",
    "abstract": "Entity resolution (ER) presents unique challenges for evaluation methodology. While crowdsourcing platforms acquire ground truth, sound approaches to sampling must drive labelling efforts. In ER, extreme class imbalance between matching and non-matching records can lead to enormous labelling requirements when seeking statistically consistent estimates for rigorous evaluation. This paper addresses this important challenge with the OASIS algorithm: a sampler and F-measure estimator for ER evaluation. OASIS draws samples from a (biased) instrumental distribution, chosen to ensure estimators with optimal asymptotic variance. As new labels are collected OASIS updates this instrumental distribution via a Bayesian latent variable model of the annotator oracle, to quickly focus on unlabelled items providing more information. We prove that resulting estimates of F-measure, precision, recall converge to the true population values. Thorough comparisons of sampling methods on a variety of ER datasets demonstrate significant labelling reductions of up to 83% without loss to estimate accuracy.",
    "subtype": "research",
    "authors": [
      {
        "name": "Neil Marchant",
        "affiliation": "University of Melbourne"
      },
      {
        "name": "Benjamin Rubinstein",
        "affiliation": "University of Melbourne"
      }
    ],
    "type": "research",
    "id": "1317"
  },
  "research1376": {
    "title": "Stitching Web Tables for Improving Matching Quality",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p1502-lehmberg.pdf",
    "abstract": "Tables on web pages (“web tables”) cover a diversity of topics and can be a source of information for different tasks such as knowledge base augmentation or the ad-hoc extension of datasets. However, to use this information, the tables must first be integrated, either with each other or into existing data sources. The challenges that matching methods for this purpose have to overcome are the high heterogeneity and the small size of the tables. Though it is known that the majority of web tables in currently available corpora are very small, most of the existing methods do not recognise this fact as a problem for the matching process. In this experimental paper, we evaluate T2K Match, an existing web table to knowledge base matching method, and COMA, a standard schema matching tool, on a sample of web tables that is more realistic than the gold standards that are often used to compare matching systems, drawn from a corpus of 5 million web tables. We find that the methods fail to produce a correct result for many of the very small tables in this sample. As a remedy, we propose to stitch the many small web tables into larger ones. For this stitching process, we experimentally evaluate several standard schema matching methods in combination with holistic schema matching. Limiting this procedure to web tables from the same web site decreases the heterogeneity and we can perform this stitching at very high precision. Our experiments show that we improve the results for the original task by 0.38 in F1-Measure for T2K Match and by 0.14 for COMA when applying the stitching method. Stitching the tables further allows us to reduce the amount of tables in the corpus from 5 million original web tables to as few as 100,000 stitched tables.",
    "subtype": "research",
    "authors": [
      {
        "name": "Oliver Lehmberg",
        "affiliation": "University of Mannheim"
      },
      {
        "name": "Christian Bizer",
        "affiliation": "University of Mannheim"
      }
    ],
    "type": "research",
    "id": "1376"
  },
  "research600": {
    "title": "Effective and Complete Discovery of Order Dependencies via Set-based Axiomatization",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p721-szlichta.pdf",
    "abstract": "Integrity constraints (ICs) are useful for query optimization and for expressing and enforcing application semantics. However, formulating constraints manually requires domain expertise, is prone to human errors, and may be excessively time consuming, especially on large datasets. Hence, proposals for automatic discovery have been made for some classes of ICs, such as functional dependencies (FDs), and recently, order dependencies (ODs). ODs properly subsume FDs, as they can additionally express business rules involving order; e.g., an employee never has a higher salary while paying lower taxes than another employee. We present a new OD discovery algorithm enabled by a novel polynomial mapping to a canonical form of ODs, and a sound and complete set of axioms (inference rules) for canonical ODs. Our algorithm has exponential worst-case time complexity, O(2^{",
    "subtype": "research",
    "authors": [
      {
        "name": "Jaroslaw Szlichta",
        "affiliation": "UOIT"
      },
      {
        "name": "Parke Godfrey",
        "affiliation": "York University"
      },
      {
        "name": "Lukasz Golab",
        "affiliation": "University of Waterloo"
      },
      {
        "name": "Mehdi Kargar",
        "affiliation": "University of Windsor"
      },
      {
        "name": "Divesh Srivastava",
        "affiliation": "AT&T"
      }
    ],
    "type": "research",
    "id": "600"
  },
  "research332": {
    "title": "Resisting Tag Spam by Leveraging Implicit User Behaviors",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p241-zhai.pdf",
    "abstract": "Tagging systems are vulnerable to tag spam attacks. However, defending against tag spam has been challenging in practice, since adversaries can easily launch spam attacks in various ways and scales. To deeply understand users' tagging behaviors and explore more effective defense, this paper first conducts measurement experiments on public datasets of two representative tagging systems: Del.icio.us and CiteULike. Our key finding is that a significant fraction of correct tag-resource annotations are contributed by a small number of implicit similarity cliques, where users annotate common resources with similar tags. Guided by the above finding, we propose a new service, called Spam-Resistance-as-a-Service (or SRaaS), to effectively defend against heterogeneous tag spam attacks even at very large scales. At the heart of SRaaS is a novel reputation assessment protocol, whose design leverages the implicit similarity cliques coupled with the social networks inherent to typical tagging systems. With such a design, SRaaS manages to offer provable guarantees on diminishing the influence of tag spam attacks. We build an SRaaS prototype and evaluate it using a large-scale spam-oriented research dataset (which is much more polluted by tag spam than Del.icio.us and CiteULike datasets). Our evaluational results demonstrate that SRaaS outperforms existing tag spam defenses deployed in real-world systems, while introducing low overhead.",
    "subtype": "research",
    "authors": [
      {
        "name": "Ennan Zhai",
        "affiliation": "Yale University"
      },
      {
        "name": "Zhenhua Li",
        "affiliation": "Tsinghua University"
      },
      {
        "name": "Zhenyu Li",
        "affiliation": "Chinese Academy of Sciences"
      },
      {
        "name": "Fan Wu",
        "affiliation": "Shanghai Jiaotong University"
      },
      {
        "name": "Guihai Chen",
        "affiliation": "Shanghai Jiaotong University"
      }
    ],
    "type": "research",
    "id": "332"
  },
  "research1330": {
    "title": "SkyGraph: Retrieving Regions of Interest using Skyline Subgraph Queries",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p1382-ranu.pdf",
    "abstract": "Several services today are annotated with {points of interest (PoIs)}. In this paper, we study the scenario where a user wants to identify the best {region of interest (RoI)} in a city. An RoI is a neighborhood that contains PoIs relevant to the user. The user expresses relevance through a set of keywords such as ``coffee shop'', ``park'', etc. Ideally, the RoI should be small enough in size so that the user can conveniently explore the PoIs. How does one balance the importance of size versus relevance? To a user exploring the RoI on foot, size is critical for convenience. On the other hand, to a user equipped with a vehicle, relevance is a more important factor. In this paper, we solve this dilemma through {skyline subgraph queries} on keyword-embedded road networks. Skyline subgraphs subsume the choice of optimization function for an RoI since the optimal RoI for any rational user is necessarily a part of the skyline set. Our analysis reveals that the problem of computing the skyline set is NP-hard. We overcome this computational bottleneck by proposing a polynomial-time approximation algorithm called {SkyGraph}. To further expedite the running time, we develop an index structure called {Partner Index} that drastically prunes the search space and provides up to $3$ orders of magnitude speed-up on real road networks.",
    "subtype": "research",
    "authors": [
      {
        "name": "Shiladitya Pande",
        "affiliation": "IIT Madras"
      },
      {
        "name": "Sayan Ranu",
        "affiliation": "IIT Delhi"
      },
      {
        "name": "Arnab Bhattacharya",
        "affiliation": "IIT Kanpur"
      }
    ],
    "type": "research",
    "id": "1330"
  },
  "research741": {
    "title": "READS: A Random Walk Approach for Efficient and Accurate Dynamic SimRank",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p937-jiang.pdf",
    "abstract": "Similarity among entities in graphs plays a key role in data analysis and mining. SimRank is a widely used and popular measurement to evaluate the similarity among the vertices. In real-life applications, graphs do not only grow in size, requiring fast and precise SimRank computation for large graphs, but also change and evolve continuously over time, demanding an efficient maintenance process to handle dynamic updates. In this paper, we propose a random walk based indexing scheme to compute SimRank efficiently and accurately over large dynamic graphs. We show that our algorithm outperforms the state-of-the-art static and dynamic SimRank algorithms.",
    "subtype": "research",
    "authors": [
      {
        "name": "minhao jiang",
        "affiliation": "HKUST"
      },
      {
        "name": "Ada Wai Chee Fu",
        "affiliation": "The CUHK"
      },
      {
        "name": "Raymond Chi-Wing Wong",
        "affiliation": "HKUST"
      },
      {
        "name": "Ke Wang",
        "affiliation": "Simon Fraser University"
      }
    ],
    "type": "research",
    "id": "741"
  },
  "research558": {
    "title": "Understanding the Sparse Vector Technique for Differential Privacy",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p637-lyu.pdf",
    "abstract": "The Sparse Vector Technique (SVT) is a fundamental technique for satisfying differential privacy and has the unique quality that one can output some query answers without apparently paying any privacy cost. SVT has been used in both the interactive setting, where one tries to answer a sequence of queries that are not known ahead of the time, and in the non-interactive setting, where all queries are known. Because of the potential savings on privacy budget, many variants for SVT have been proposed and employed in privacy-preserving data mining and publishing. However, most variants of SVT are actually not private. In this paper, we analyze these errors and identify the misunderstandings that likely contribute to them. We also propose a new version of SVT that provides better utility, and introduce an effective technique to improve the performance of SVT. These enhancements can be applied to improve utility in the interactive setting. Through both analytical and experimental comparisons, we show that, in the non-interactive setting (but not the interactive setting), the SVT technique is unnecessary, as it can be replaced by the Exponential Mechanism (EM) with better accuracy.",
    "subtype": "research",
    "authors": [
      {
        "name": "Min Lyu",
        "affiliation": "University of Science and Technology of China"
      },
      {
        "name": "Dong Su",
        "affiliation": "Purdue University"
      },
      {
        "name": "Ninghui Li",
        "affiliation": "Purdue University"
      }
    ],
    "type": "research",
    "id": "558"
  },
  "industrial927": {
    "title": "Fiber-based architecture for NFV cloud databases",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p1682-gasiunas.pdf",
    "abstract": "The Telecom industry is gradually shifting from using monolithic software packages deployed on custom hardware, to using modular virtualized software functions deployed on cloudified datacenters of commodity hardware. This transformation is referred to as Network Function Virtualization (NFV). The scalability of the databases (DBs) underlying the virtual network functions is the cornerstone for ripping the benefits from the NFV transformation. This paper presents an industrial experience of applying shared-nothing techniques in order to achieve scalability for a DB in an NFV setup. The special combination of requirements in NFV DBs are not easily met with conventional execution models. Therefore, we designed a special shared-nothing architecture that is based on cooperative multi-tasking on top of user-level threads (fibers). We further show that the fiber-based approach outperforms the approach built using conventional multi-threading and answers the variable deployment needs of the NFV transformation. Furthermore, the fibers yield a simpler-to-maintain software and enable controlling a trade-off between long-duration computations and real-time requests.",
    "subtype": "industrial",
    "authors": [
      {
        "name": "Vaidas Gasiunas",
        "affiliation": "Huawei"
      },
      {
        "name": "David Dominguez-Sal",
        "affiliation": "Huawei"
      },
      {
        "name": "Ralph Acker",
        "affiliation": "Huawei"
      },
      {
        "name": "Aharon Avitzur",
        "affiliation": "Huawei"
      },
      {
        "name": "Ilan Bronshtein",
        "affiliation": "Huawei"
      },
      {
        "name": "Rushan Chen",
        "affiliation": ""
      },
      {
        "name": "Eli Ginot",
        "affiliation": "Huawei"
      },
      {
        "name": "Norbert Martinez",
        "affiliation": "Huawei"
      },
      {
        "name": "Michael Müller",
        "affiliation": "Huawei"
      },
      {
        "name": "Alexander Nozdrin",
        "affiliation": "Huawei"
      },
      {
        "name": "Weijie Ou",
        "affiliation": "Huawei"
      },
      {
        "name": "Nir Pachter",
        "affiliation": "Huawei"
      },
      {
        "name": "Dima Sivov",
        "affiliation": "Huawei"
      },
      {
        "name": "Eliezer Levy",
        "affiliation": "Huawei"
      }
    ],
    "type": "industrial",
    "id": "927"
  },
  "research1307": {
    "title": "Truss-based Community Search: a Truss-equivalence Based Indexing Approach",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p1298-zhao.pdf",
    "abstract": "We consider the community search problem defined upon a large graph G: given a query vertex q in G, to find as output all the densely connected subgraphs from G, each of which contains the query vertex v. As an online, query-dependent variant of the well-known community detection problem, community search enables personalized community discovery from graphs, and has found a wide range of real-world applications. In this paper, we study the community search problem in the truss-based model aimed at discovering all dense and cohesive k-truss communities to which the query vertex q belongs. We introduce a novel equivalence relation, k-truss equivalence, to model the intrinsic density and cohesiveness of edges in k-truss communities. Consequently, all the edges of G can be partitioned to a series of k-truss equivalence classes that constitute a space-efficient, truss-preserving index structure, EquiTruss. Community search can thus be addressed upon EquiTruss without repeated, time-demanding accesses to the original graph G, which proves to be theoretically optimal. In addition, EquiTruss can be efficiently maintained in a dynamic fashion when G evolves with edge insertion/deletion. Experimental studies validate both the efficiency and effectiveness of EquiTruss in real-world, large-scale graphs, which achieves at least an order of magnitude speedup in community search over the state-of-the-art method, TCP-Index.",
    "subtype": "research",
    "authors": [
      {
        "name": "Esra Akbas",
        "affiliation": "Florida State University"
      },
      {
        "name": "Peixiang Zhao",
        "affiliation": "Florida State University"
      }
    ],
    "type": "research",
    "id": "1307"
  },
  "industrial1028": {
    "title": "ExtraV: Boosting Graph Processing Near Storage with a Coherent Accelerator",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p1706-lee.pdf",
    "abstract": "In this paper, we propose ExtraV, a framework for near-storage graph processing. It is based on the novel concept of graph virtualization, which efficiently utilizes a cache-coherent hardware accelerator at the storage side to achieve performance and flexibility at the same time. ExtraV consists of four main components: 1) host processor, 2) main memory, 3) AFU (Accelerator Function Unit) and 4) storage. The AFU, a hardware accelerator, sits between the host processor and storage. Using a coherent interface that allows main memory accesses, it performs graph traversal functions that are common to various algorithms while the program running on the host processor (called the host program) manages the overall execution along with more application-specific tasks. Graph virtualization is a high-level programming model of graph processing that allows designers to focus on algorithm-specific functions. Realized by the accelerator, graph virtualization gives the host programs an illusion that the graph data reside on the main memory in a layout that fits with the memory access behavior of host programs even though the graph data are actually stored in a multi-level, compressed form in storage. We prototyped ExtraV on a Power8 machine with a CAPI-enabled FPGA. Our experiments on a real system prototype offer significant speedup compared to state-of-the-art software only implementations.",
    "subtype": "industrial",
    "authors": [
      {
        "name": "Jinho Lee",
        "affiliation": "IBM Research"
      },
      {
        "name": "Heesu Kim",
        "affiliation": "Seoul National University"
      },
      {
        "name": "Sungjoo Yoo",
        "affiliation": "Seoul National University"
      },
      {
        "name": "Kiyoung Choi",
        "affiliation": "Seoul National University"
      },
      {
        "name": "Peter Hofstee",
        "affiliation": "IBM Research"
      },
      {
        "name": "GiJoon Nam",
        "affiliation": "IBM Research"
      },
      {
        "name": "Mark Nutter",
        "affiliation": "IBM Research"
      },
      {
        "name": "Damir Jamsek",
        "affiliation": "IBM Research"
      }
    ],
    "type": "industrial",
    "id": "1028"
  },
  "research1366": {
    "title": "Pyramid Sketch: a Sketch Framework for Frequency Estimation of Data Streams",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p1442-yang.pdf",
    "abstract": "Sketch is a probabilistic data structure, and is used to store and query the frequency of any item in a given multiset. Due to its high memory efficiency, it has been applied to various fields in computer science, such as stream database, network traffic measurement, etc. The key metrics of sketches for data streams are accuracy, speed, and memory usage. There are various sketches in the literature, but they cannot achieve both high accuracy and high speed using limited memory, especially for skewed datasets. To address this issue, we propose a sketch framework, the Pyramid sketch, which can significantly improve accuracy as well as update and query speed. To verify the effectiveness and efficiency of our framework, we applied our framework to four typical sketches. Extensive experimental results show that the accuracy is improved up to 3.50 times, while the speed is improved up to 2.10 times. We have released our source code at Github.",
    "subtype": "research",
    "authors": [
      {
        "name": "Tong Yang",
        "affiliation": "Peking University"
      },
      {
        "name": "Yang Zhou",
        "affiliation": "Peking University"
      },
      {
        "name": "Hao Jin",
        "affiliation": "Peking University"
      },
      {
        "name": "Shigang Chen",
        "affiliation": "University of Florida"
      },
      {
        "name": "Xiaoming Li",
        "affiliation": "Peking University"
      }
    ],
    "type": "research",
    "id": "1366"
  },
  "research737": {
    "title": "Revisiting the Stop-and-Stare Algorithms for Influence Maximization",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p913-Huang.pdf",
    "abstract": "Influence maximization is a combinatorial optimization problem that finds important applications in viral marketing, feed recommendation, etc. Recent research has led to a number of scalable approximation algorithms for influence maximization, such as TIM^+ and IMM, and more recently, SSA and D-SSA. The goal of this paper is to conduct a rigorous theoretical and experimental analysis of SSA and D-SSA and compare them against the preceding algorithms. In doing so, we uncover inaccuracies in previously reported technical results on the accuracy and efficiency of SSA and D-SSA, which we set right. We also attempt to reproduce the original experiments on SSA and D-SSA, based on which we provide interesting empirical insights. Our evaluation confirms some results reported from the original experiments, but it also reveals anomalies in some other results and sheds light on the behavior of SSA and D-SSA in some important settings not considered previously. We also report on the performance of SSA-Fix, our modification to SSA in order to restore the approximation guarantee that was claimed for but not enjoyed by SSA. Overall, our study suggests that there exist opportunities for further scaling up influence maximization with approximation guarantees.",
    "subtype": "research",
    "authors": [
      {
        "name": "Keke Huang",
        "affiliation": "Nanyang Technological University"
      },
      {
        "name": "Sibo Wang",
        "affiliation": "Nanyang Technological University"
      },
      {
        "name": "Glenn Bevilacqua",
        "affiliation": "University of British Columbia"
      },
      {
        "name": "Xiaokui Xiao",
        "affiliation": "Nanyang Technological University"
      },
      {
        "name": "Laks Lakshmanan",
        "affiliation": "UBC"
      }
    ],
    "type": "research",
    "id": "737"
  },
  "research427": {
    "title": "Lifting the Haze off the Cloud: A Consumer-Centric Market for Database Computation in the Cloud",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p373-wang.pdf",
    "abstract": "The availability of public computing resources in the cloud has revolutionized data analysis, but requesting cloud resources often involves complex decisions for consumers. Estimating the completion time and cost of a computation and requesting the appropriate cloud resources are challenging tasks even for an expert user. We propose a new market-based framework for pricing computational tasks in the cloud. Our framework introduces an agent between consumers and cloud providers. The agent takes data and computational tasks from users, estimates time and cost for evaluating the tasks, and returns to consumers contracts that specify the price and completion time. Our framework can be applied directly to existing cloud markets without altering the way cloud providers offer and price services. In addition, it simplifies cloud use for consumers by allowing them to compare contracts, rather than choose resources directly. We present design, analytical, and algorithmic contributions focusing on pricing computation contracts, analyzing their properties, and optimizing them in complex workflows. We conduct an experimental evaluation of our market framework over a real-world cloud service and demonstrate empirically that our market ensures three key properties: (a) that consumers benefit from using the market due to competitiveness among agents, (b) that agents have incentive to price contracts fairly, and (c) that inaccuracies in estimates do not pose a significant risk to agents’ profits. Finally, we present a fine-grained pricing mechanism for complex workflows and show that it can increase agent profits by more than an order of magnitude in some cases.",
    "subtype": "research",
    "authors": [
      {
        "name": "Yue Wang",
        "affiliation": "University of Massachusetts Amherst"
      },
      {
        "name": "Alexandra Meliou",
        "affiliation": "University of Massachusetts Amherst"
      },
      {
        "name": "Gerome Miklau",
        "affiliation": "University of Massachusetts Amherst"
      }
    ],
    "type": "research",
    "id": "427"
  },
  "research573": {
    "title": "NED: An Inter-Graph Node Metric Based On Edit Distance",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p697-zhu.pdf",
    "abstract": "Node similarity is fundamental in graph analytics. However, node similarity between nodes in different graphs (inter-graph nodes) has not received enough attention yet. The inter-graph node similarity is important in learning a new graph based on the knowledge extracted from an existing graph (transfer learning on graphs) and has applications in biological, communication, and social networks. In this paper, we propose a novel distance function for measuring inter-graph node similarity with edit distance, called NED. In NED, two nodes are compared according to their local neighborhood topologies which are represented as unordered k-adjacent trees, without relying on any extra information. Due to the hardness of computing tree edit distance on unordered trees which is NP-Complete, we propose a modified tree edit distance, called TED, for comparing unordered and unlabeled k-adjacent trees. TED* is a metric distance, as the original tree edit distance, but more importantly, TED* is polynomially computable. As a metric distance, NED admits efficient indexing, provides interpretable results, and shows to perform better than existing approaches on a number of data analysis tasks, including graph de-anonymization. Finally, the efficiency and effectiveness of NED are empirically demonstrated using real-world graphs.",
    "subtype": "research",
    "authors": [
      {
        "name": "Haohan Zhu",
        "affiliation": "Boston University"
      },
      {
        "name": "Xianrui Meng",
        "affiliation": "Boston University"
      },
      {
        "name": "George Kollios",
        "affiliation": "Boston University"
      }
    ],
    "type": "research",
    "id": "573"
  },
  "research1319": {
    "title": "Flexible Online Task Assignment in Real-Time Spatial Data",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p1334-tong.pdf",
    "abstract": "The popularity of Online To Offline (O2O) service platforms has spurred the need for effective online task assignment in real-time spatial data, where streams of spatially distributed tasks and workers are matched in real time such that the total number of assigned pairs is maximized. Existing online task assignment models assume that each worker is either assigned a task immediately or waits for a subsequent task at a fixed location once she/he appears on the platform. However, in practice, a worker may actively move around rather than passively wait at the same location if no task is assigned. If the workers guided effectively, the platform can increase the total number of assigned worker-task pairs, which is overlooked in previous works. In this paper, we define a new problem Flexible Two-sided Online task Assignment (FTOA). To address the FTOA problem, we face two challenges: (i) How to leverage the prediction of the spatiotemporal distribution of tasks and workers to effectively generate guidance for idle workers? (ii) How to leverage the guide of worker’s movement to optimize the online task assignment? To this end, we propose a novel two-step framework, which integrates offline prediction and online task assignment. Specifically, the offline prediction component estimates the distributions of tasks and workers per time slot and per unit area, based on which we develop an online task assignment algorithm, Prediction-oriented Online task Assignment in Real-time spatial data (POLAR). POLAR-OP not only yields a 0.5-competitive ratio, which is nearly twice better than that of the state-of-the-art, but also makes the time complexity of processing each newly-arrived task/worker to O(1). We verify the effectiveness and efficiency of our methods through extensive experiments on both synthetic datasets and a large-scale real-time taxi-calling platform.",
    "subtype": "research",
    "authors": [
      {
        "name": "Yongxin Tong",
        "affiliation": "Beihang University"
      },
      {
        "name": "Libin Wang",
        "affiliation": "Beihang University"
      },
      {
        "name": "Zimu Zhou",
        "affiliation": "ETH"
      },
      {
        "name": "Bolin Ding",
        "affiliation": "Microsoft Research"
      },
      {
        "name": "Lei Chen",
        "affiliation": "HKUST"
      },
      {
        "name": "Jieping Ye",
        "affiliation": "Didi Research"
      },
      {
        "name": "Ke Xu",
        "affiliation": "Beihang University"
      }
    ],
    "type": "research",
    "id": "1319"
  },
  "industrial825": {
    "title": "Developing a Low Dimensional Patient Class Profile in Accordance to Their Respiration-Induced Tumor Motion",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p1610-shamsuddin.pdf",
    "abstract": "Tumor location displacement caused by respiration-induced motion reduces the efficacy of radiation therapy. Three medically relevant patterns are often observed in the respiration- induced motion signal: baseline shift, ES-Range shift, and D-Range shift. In this paper, for patients with lower body cancer, we develop class profiles (a low dimensional pattern frequency structure) that characterize them in terms of these three medically relevant patterns. We propose an adaptive segmentation technique that turns each respiration-induced motion signal into a multi-set of segments based on persistent variations within the signal. These multi-sets of segments is then probed for base behaviors. These base behaviors are then used to develop the group/class profiles using a modified version of the clustering technique described in [1]. Finally, via quantitative analysis, we provide a medical characterization for the class profiles, which can be used to explore breathing intervention technique. We show that, with i) carefully designed feature sets, ii) the proposed adaptive segmentation technique, iii) the reasonable modifications to an existing clustering algorithm for multi-sets, and iv) the proposed medical characterization methodology, it is possible to reduce the time series respiration-induced motion signals into a compact class profile. One of our co-authors is a medical physician and we used his expert opinion to verify the results.",
    "subtype": "industrial",
    "authors": [
      {
        "name": "Rittika Shamsuddin",
        "affiliation": "University of Texas at Dallas"
      },
      {
        "name": "Balakrishnan Prabhakaran",
        "affiliation": "University of Texas at Dallas"
      },
      {
        "name": "Amit Sawant",
        "affiliation": "University of Maryland"
      }
    ],
    "type": "industrial",
    "id": "825"
  },
  "industrial1203": {
    "title": "Matrix Profile IV: Using Weakly Labeled Time Series to Predict Outcomes",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p1802-yeh.pdf",
    "abstract": "In academic settings over the last decade, there has been significant progress in time series classification. However, much of this work makes assumptions that are simply unrealistic for deployed industrial applications. Examples of these unrealistic assumptions include the following: assuming that data subsequences have a single fixed-length, are precisely extracted from the data, and are correctly labeled according to their membership in a set of equalsize classes. In real-world industrial settings, these patterns can be of different lengths, the class annotations may only belong to a general region of the data, may contain errors, and finally, the class distribution is typically highly skewed. Can we learn from such weakly labeled data? In this work, we introduce SDTS, a scalable algorithm that can learn in such challenging settings. We demonstrate the utility of our ideas by learning from diverse datasets with millions of datapoints. As we shall demonstrate, our domain-agnostic parameter-free algorithm can be competitive with domain-specific algorithms used in neuroscience and entomology, even when those algorithms have been tuned by domain experts to incorporate domain knowledge.",
    "subtype": "industrial",
    "authors": [
      {
        "name": "Chin-Chia Michael Yeh",
        "affiliation": "UC Riverside"
      },
      {
        "name": "Nickolas Kavantzas",
        "affiliation": "Oracle"
      },
      {
        "name": "Eamonn Keogh",
        "affiliation": "UC Riverside"
      }
    ],
    "type": "industrial",
    "id": "1203"
  },
  "research845": {
    "title": "Scalable Asynchronous Gradient Descent Optimization for Out-of-Core Models",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p986-rusu.pdf",
    "abstract": "Existing data analytics systems have approached predictive model training exclusively from a data-parallel perspective. Data examples are partitioned to multiple workers and training is executed concurrently over different partitions, under various synchronization policies that emphasize speedup or convergence. Since models with millions and even billions of features become increasingly common nowadays, model management becomes an equally important task for effective training. In this paper, we present a general framework for parallelizing stochastic optimization algorithms over massive models that cannot fit in memory. We extend the lock-free HOGWILD!-family of algorithms to disk-resident models by vertically partitioning the model offline and asynchronously updating the resulting partitions online. Unlike HOGWILD!, concurrent requests to the common model are minimized by a preemptive push-based sharing mechanism that reduces the number of disk accesses. Experimental results on real and synthetic datasets show that the proposed framework achieves improved convergence over HOGWILD! and is the only solution scalable to massive models.",
    "subtype": "research",
    "authors": [
      {
        "name": "Chengjie Qin",
        "affiliation": "UC Merced"
      },
      {
        "name": "Martin Torres",
        "affiliation": "UC Merced"
      },
      {
        "name": "Florin Rusu",
        "affiliation": "University of California"
      }
    ],
    "type": "research",
    "id": "845"
  },
  "research1400": {
    "title": "Dscaler: Synthetically Scaling A Given Relational Database",
    "acm_link": "http://www.vldb.org/pvldb/vol9/p1671-zhang.pdf",
    "abstract": "The Dataset Scaling Problem (DSP) defined in previous work states: Given an empirical set of relational tables D and a scale factor s, generate a database state D' that is similar to D but s times its size. A DSP solution is useful for application development (s < 1), scalability testing (s > 1) and anonymization (s = 1). Current solutions assume all table sizes scale by the same ratio s. However, a real database tends to have tables that grow at different rates. This paper therefore considers non-uniform scaling (nuDSP), a DSP generalization where, instead of a single scale factor s, tables can scale by different factors. Dscaler is the first solution for nuDSP. It follows previous work in achieving similarity by reproducing correlation among the primary and foreign keys. However, it introduces the concept of a correlation database that captures fine-grained, per-tuple correlation. Experiments with well-known real and synthetic datasets D show that Dscaler produces D' with greater similarity to D than state-of-the-art techniques. Here, similarity is measured by number of tuples, frequency distribution of foreign key references, and multi-join aggregate queries.",
    "subtype": "research",
    "authors": [
      {
        "name": "Jiangwei Zhang",
        "affiliation": "NUS"
      },
      {
        "name": "Y.C. Tay",
        "affiliation": "NUS"
      }
    ],
    "type": "research",
    "id": "1400"
  },
  "research607": {
    "title": "Dimensional Testing for Reverse k-Nearest Neighbor Search",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p769-houle.pdf",
    "abstract": "Given a query object q, reverse k-nearest neighbor (RkNN) search aims to locate those objects of the database that have q among their k-nearest neighbors. In this paper, we propose an approximation method for solving RkNN queries, where the pruning operations and termination tests are guided by a characterization of the intrinsic dimensionality of the data. The method can accommodate any index structure supporting incremental (forward) nearest-neighbor search for the generation and verification of candidates, while avoiding impractically-high preprocessing costs. We also provide experimental evidence that our method significantly outperforms its competitors in terms of the tradeoff between execution time and the quality of the approximation. Our approach thus addresses many of the scalability issues surrounding the use of previous methods in data mining.",
    "subtype": "research",
    "authors": [
      {
        "name": "Guillaume Casanova",
        "affiliation": "ONERA-DCSD"
      },
      {
        "name": "Elias Englmeier",
        "affiliation": "LMU"
      },
      {
        "name": "Michael Houle",
        "affiliation": "NII"
      },
      {
        "name": "Peer Kroeger",
        "affiliation": "LMU"
      },
      {
        "name": "Michael Nett",
        "affiliation": "Google"
      },
      {
        "name": "Erich Schubert",
        "affiliation": "LMU"
      },
      {
        "name": "Arthur Zimek",
        "affiliation": "SDU"
      }
    ],
    "type": "research",
    "id": "607"
  },
  "research1327": {
    "title": "Knowledge Verification for LongTail Verticals",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p1370-li.pdf",
    "abstract": "Collecting structured knowledge for real-world entities has become a critical task for many applications. A big gap between the knowledge in existing knowledge repositories and the knowledge in the real world is the knowledge on tail verticals (i.e., less popular domains). Such knowledge, though not necessarily globally popular, can be personal hobbies to many people and thus collectively impactful. This paper studies the problem of knowledge verification for tail verticals; that is, deciding the correctness of a given triple. Through comprehensive experimental study we answer the following questions. 1) Can we find evidence for tail knowledge from an extensive set of sources, including knowledge bases, the web, and query logs? 2) Can we judge correctness of the triples based on the collected evidence? 3) How can we further improve knowledge verification on tail verticals? Our empirical study suggests a new knowledge-verification framework, which we call FACTY, that applies various kinds of evidence collection techniques followed by knowledge fusion. FACTY can verify 50% of the (correct) tail knowledge with a precision of 84%, and it significantly outperforms state-of-the-art methods. Detailed error analysis on the obtained results suggests future research directions.",
    "subtype": "research",
    "authors": [
      {
        "name": "Furong Li",
        "affiliation": "NUS"
      },
      {
        "name": "Xin Luna Dong",
        "affiliation": "Amazon"
      },
      {
        "name": "Anno Langen",
        "affiliation": "Google"
      },
      {
        "name": "Yang Li",
        "affiliation": "Google"
      }
    ],
    "type": "research",
    "id": "1327"
  },
  "research505": {
    "title": "Shrink - Prescribing Resiliency Solutions for Streaming",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p505-goldstein.pdf",
    "abstract": "Streaming query deployments make up a vital part of cloud oriented applications. They vary widely in their data, logic, and statefulness, and are typically executed in multi-tenant distributed environments with varying uptime SLAs. In order to achieve these SLAs, one of a number of proposed resiliency strategies is employed to protect against failure. This paper has introduced the first, comprehensive, cloud friendly comparison between different resiliency techniques for streaming queries. In this paper, we introduce models which capture the costs associated with different resiliency strategies, and through a series of experiments which implement and validate these models, show that (1) there is no single resiliency strategy which efficiently handles most streaming scenarios; (2) the optimization space is too complex for a person to employ a “rules of thumb” approach; and (3) there exists a clear generalization of periodic checkpointing that is worth considering in many cases. Finally, the models presented in this paper can be adapted to fit a wide variety of resiliency strategies, and likely have important consequences for cloud services beyond those that are obviously streaming.",
    "subtype": "research",
    "authors": [
      {
        "name": "Badrish Chandramouli",
        "affiliation": "Microsoft Research"
      },
      {
        "name": "Jonathan Goldstein",
        "affiliation": "Microsoft Research"
      }
    ],
    "type": "research",
    "id": "505"
  },
  "research432": {
    "title": "Estimating Quantiles from the Union of Historical and Streaming Data",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p433-tirthapura.pdf",
    "abstract": "Modern enterprises generate huge amounts of streaming data, for example, micro-blog feeds, financial data, network monitoring and industrial application monitoring. While Data Stream Management Systems have proven successful in providing support for real-time alerting, many applications, such as network monitoring for intrusion detection and real-time bidding, require complex analytics over historical and real-time data over the data streams. We present a new method to process one of the most fundamental analytical primitives, quantile queries, on the union of historical and streaming data. Our method combines an index on historical data with a memory-efficient sketch on streaming data to answer quantile queries with accuracy-resource tradeoffs that are significantly better than current solutions that are based solely on disk-resident indexes or solely on streaming algorithms.",
    "subtype": "research",
    "authors": [
      {
        "name": "Sneha Singh",
        "affiliation": "Iowa State University"
      },
      {
        "name": "Divesh Srivastava",
        "affiliation": "AT&T"
      },
      {
        "name": "Srikanta Tirthapura",
        "affiliation": "Iowa State University"
      }
    ],
    "type": "research",
    "id": "432"
  },
  "research611": {
    "title": "Real-Time Influence Maximization on Dynamic Social Streams",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p805-wang.pdf",
    "abstract": "Influence maximization (IM), which selects a set of $k$ users (called seeds) to maximize the influence spread over a social network, is a fundamental problem in a wide range of applications such as viral marketing and network monitoring. Existing IM solutions fail to consider the highly dynamic nature of social influence, which results in either poor seed qualities or long processing time when the network evolves. To address this problem, we define a novel IM query named Stream Influence Maximization (SIM) on social streams. Technically, SIM adopts the sliding window model and maintains a set of $k$ seeds with the largest influence value over the most recent social actions. Next, we propose the Influential Checkpoints (IC) framework to facilitate continuous SIM query processing. The IC framework creates a checkpoint for each window shift and ensures an $varepsilon$-approximate solution. To improve its efficiency, we further devise a Sparse Influential Checkpoints (SIC) framework which selectively keeps $O(frac{log{N}}{beta})$ checkpoints for a sliding window of size $N$ and maintains an $frac{varepsilon(1-beta)}{2}$ approximate solution. Experimental results on both real-world and synthetic datasets confirm the effectiveness and efficiency of our proposed frameworks against the state-of-the-art IM approaches.",
    "subtype": "research",
    "authors": [
      {
        "name": "Yanhao Wang",
        "affiliation": "NUS"
      },
      {
        "name": "Qi Fan",
        "affiliation": "NUS"
      },
      {
        "name": "Yuchen Li",
        "affiliation": "NUS"
      },
      {
        "name": "Kian-Lee Tan",
        "affiliation": "NUS"
      }
    ],
    "type": "research",
    "id": "611"
  },
  "research330": {
    "title": "Scalable Distributed Subgraph Enumeration",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p217-lai.pdf",
    "abstract": "Subgraph enumeration aims to find all the subgraphs of a large data graph that are isomorphic to a given pattern graph. As the subgraph isomorphism operation is computationally intensive, researchers have recently focused on solving this problem in distributed environments, such as MapReduce and Pregel. Among them, the state-of-the-art algorithm, TwinTwigJoin, is proven to be instance optimal based on a left-deep-join framework. However, it is still not scalable to large graphs because of the constraints in the left-deep-join framework and that each decomposed component (join unit) must be a star. In this paper, we propose SEED - a scalable subgraph enumeration approach in the distributed environment. Compared to TwinTwigJoin, SEED returns optimal solution in a generalized join framework without the constraints in TwinTwigJoin. We use both star and clique as the join units, and design an effective distributed graph storage mechanism to support such an extension. We develop a comprehensive cost model, that evaluates the number of matches of any given pattern graph by considering power-law degree distribution in the data graph. We then generalize the left-deep-join framework and develop a dynamic-programming algorithm to compute an optimal bushy join plan. We also consider overlaps among the join units. Finally, we propose clique compression to further improve the algorithm by reducing the number of the intermediate results. Extensive performance studies are conducted on several real graphs, one containing billions of edges. The results demonstrate that our algorithm outperforms all other state-of-the-art algorithms by more than one order of magnitude.",
    "subtype": "research",
    "authors": [
      {
        "name": "Longbin Lai",
        "affiliation": "CSE"
      },
      {
        "name": "Lu Qin",
        "affiliation": "QCIS"
      },
      {
        "name": "Xuemin Lin",
        "affiliation": "CSE"
      },
      {
        "name": "Ying Zhang",
        "affiliation": "QCIS"
      },
      {
        "name": "Lijun Chang",
        "affiliation": "CSE"
      }
    ],
    "type": "research",
    "id": "330"
  },
  "research850": {
    "title": "When Engagement Meets Similarity: Efficient (k,r)-Core Computation on Social Networks",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p998-zhang.pdf",
    "abstract": "In this paper, we investigate the problem of (k,r)-core which intends to find cohesive subgraphs on social networks considering both user engagement and similarity perspectives. In particular, we adopt the popular concept of k-core to guarantee the engagement of the users (vertices) in a group (subgraph) where each vertex in a (k,r)-core connects to at least k other vertices. Meanwhile, we consider the pairwise similarity among users based on their attributes. Efficient algorithms are proposed to enumerate all maximal (k,r)-cores and find the maximum (k,r)-core, where both problems are shown to be NP-hard. Effective pruning techniques substantially reduce the search space of two algorithms. A novel (k,k')-core based (k,r)-core size upper bound enhances performance of the maximum (k,r)-core computation. We also devise effective search orders for two mining algorithms where search priorities for vertices are different. Comprehensive experiments on real-life data demonstrate that the maximal/maximum (k,r)-cores enable us to find interesting cohesive subgraphs, and performance of two mining algorithms is effectively improved by proposed techniques.",
    "subtype": "research",
    "authors": [
      {
        "name": "Fan Zhang",
        "affiliation": "UTS"
      },
      {
        "name": "Ying Zhang",
        "affiliation": "QCIS"
      },
      {
        "name": "Lu Qin",
        "affiliation": "QCIS"
      },
      {
        "name": "Wenjie Zhang",
        "affiliation": "CSE"
      },
      {
        "name": "Xuemin Lin",
        "affiliation": "CSE"
      }
    ],
    "type": "research",
    "id": "850"
  },
  "industrial887": {
    "title": "Query-able Kafka: An agile data analytics pipeline for mobile wireless networks",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p1646-falk.pdf",
    "abstract": "Due to their promise of delivering real-time network insights, today’s streaming analytics platforms are increasingly be- ing used in the communications networks where the impact of the insights go beyond sentiment and trend analysis to include real-time detection of security attacks and predic- tion of network state (i.e., is the network transitioning to- wards an outage). Current streaming analytics platforms operate under the assumption that arriving traffic is to the order of kilobytes produced at very high frequencies. How- ever, communications networks, especially the telecommu- nication networks, challenge this assumption because some of the arriving traffic in these networks is to the order of gigabytes, but produced at medium to low velocities. Fur- thermore, these large datasets may need to be ingested in their entirety to render network insights in real-time. Our interest is to subject today’s streaming analytics platforms — constructed from state-of-the art software components (Kafka, Spark, HDFS, ElasticSearch) — to traffic densities observed in such communications networks. We find that filtering on such large datasets is best done in a common upstream point instead of being pushed to, and repeated, in downstream components. To demonstrate the advantages of such an approach, we modify Apache Kafka to perform limited native data transformation and filtering, relieving the downstream Spark application from doing this. Our approach outperforms four prevalent analytics pipeline ar- chitectures with negligible overhead compared to standard Kafka. (Our modifications to Apache Kafka are publicly available at https://github.com/Esquive/queryable-kafka.git)",
    "subtype": "industrial",
    "authors": [
      {
        "name": "Eric Falk",
        "affiliation": "University of Luxembourg"
      },
      {
        "name": "Vijay Gurbani",
        "affiliation": "Bell Labs"
      },
      {
        "name": "Radu State",
        "affiliation": "University of Luxembourg"
      }
    ],
    "type": "industrial",
    "id": "887"
  },
  "research1291": {
    "title": "Revenue Maximization in Incentivized Social Advertising",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p1238-aslay.pdf",
    "abstract": "Incentivized social advertising, an emerging marketing model, provides monetization opportunities not only to the owners of the social networking platforms but also to their influential users by offering a ``cut'' on the advertising revenue. More in details, we consider a social network (the host) that sells ad-engagements to advertisers by inserting their ads, in the form of promoted posts, into the feeds of carefully selected ``initial endorsers or seed users: these users receive monetary incentives in exchange for their endorsements. The endorsements help propagate the ads to the feeds of their followers. Whenever any user of the platform engages with an ad, the host is paid some fixed amount by the advertiser, and the ad further propagates to the feed of her followers, potentially recursively. In this context, the problem for the host is is to allocate ads to influential users, taking into account the propensity of ads for viral propagation, and carefully apportioning the monetary budget of each of the advertisers between incentives to influential users and ad-engagement costs, with the rational goal of maximizing its own revenue. In particular, in this paper we consider a monetary incentive for the influential users, which is proportional to their influence potential. We show that, taking all important factors into account, the problem of revenue maximization in incentivized social advertising corresponds to the problem of monotone submodular function maximization, subject to a partition matroid constraint on the ads-to-seeds allocation, and submodular knapsack constraints on the advertisers' budgets. We show that this problem is NP-hard and devise two greedy algorithms with provable approximation guarantees, which differ in their sensitivity to seed user incentive costs. Our approximation algorithms require repeatedly estimating the expected marginal gain in revenue as well as in advertiser payment. By exploiting a connection to the recent advances made in scalable estimation of expected influence spread, we devise efficient and scalable versions of our two greedy algorithms. An extensive experimental assessment confirms the high quality of our proposal.",
    "subtype": "research",
    "authors": [
      {
        "name": "Cigdem Aslay",
        "affiliation": "ISI Foundation"
      },
      {
        "name": "Francesco Bonchi",
        "affiliation": "ISI Foundation"
      },
      {
        "name": "Laks Lakshmanan",
        "affiliation": "UBC"
      },
      {
        "name": "Wei Lu",
        "affiliation": "LinkedIn"
      }
    ],
    "type": "research",
    "id": "1291"
  },
  "research218": {
    "title": "Path Cost Distribution Estimation Using Trajectory Data",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p85-dai.pdf",
    "abstract": "With the growing volumes of vehicle trajectory data, it becomes increasingly possible to capture time-varying and uncertain travel costs in a road network, including travel time and fuel consumption. The current paradigm represents a road network as a weighted graph, blasts trajectories into small fragments that fit the underlying edges to assign weights to edges, and then applies a routing algorithm to the resulting graph. We propose a new paradigm, the hybrid graph, that targets more accurate and more efficient cost distribution estimation. The new paradigm avoids blasting trajectories into small fragments and instead assigns weights to paths rather than simple to the edges. We show how to compute path weights using trajectory data while taking into account the travel cost dependencies among the edges in the paths. Next, given a departure time and a path, we show how to select an optimal set of weights with associated paths that cover the query path and such that the weights enable the most accurate joint cost distribution estimation for the query path. The cost distribution of the path is then computed accurately using the joint distribution. Finally, we show how the resulting method for computing cost distributions of paths can be integrated into existing routing algorithms. Empirical studies with substantial trajectory data from two different cities offer insight into the design properties of the proposed method and confirm that the method is effective in real-world settings.",
    "subtype": "research",
    "authors": [
      {
        "name": "Jian Dai",
        "affiliation": "NUS"
      },
      {
        "name": "Bin Yang",
        "affiliation": "AAU"
      },
      {
        "name": "Chenjuan Guo",
        "affiliation": "AAU"
      },
      {
        "name": "Christian Jensen",
        "affiliation": "Aalborg University"
      },
      {
        "name": "Jilin Hu",
        "affiliation": "AAU"
      }
    ],
    "type": "research",
    "id": "218"
  },
  "research616": {
    "title": "One-Pass Error Bounded Trajectory Simplification",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p841-ma.pdf",
    "abstract": "Nowadays, various sensors are collecting, storing and transmitting tremendous trajectory data, and it is known that raw trajectory data seriously wastes the storage, network band and computing resource. Line simplification (LS) algorithms are an effective approach to attacking this issue by compressing data points in a trajectory to a set of continuous line segments, and are commonly used in practice. However, existing LS algorithms are not sufficient for the needs of sensors in mobile devices. In this study, we first develop a one-pass error bounded trajectory simplification algorithm (OPERB), which scans each data point in a trajectory once and only once. We then propose an aggressive one-pass error bounded trajectory simplification algorithm (OPERB-A), which allows interpolating new data points into a trajectory under certain conditions. Finally, we experimentally verify that our approaches (OPERB and OPERB-A) are both efficient and effective, using four real-life trajectory datasets.",
    "subtype": "research",
    "authors": [
      {
        "name": "Xuelian Lin",
        "affiliation": "Beihang University"
      },
      {
        "name": "Shuai Ma",
        "affiliation": "Beihang University"
      },
      {
        "name": "Han Zhang",
        "affiliation": "Beihang University"
      },
      {
        "name": "Tianyu Wo",
        "affiliation": "Beihang University"
      },
      {
        "name": "Jinpeng Huai",
        "affiliation": "Beihang University"
      }
    ],
    "type": "research",
    "id": "616"
  },
  "research1384": {
    "title": "Truth Discovery for SpatioTemporal Events from Crowdsourced Data",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p1562-garciaulloa.pdf",
    "abstract": "One of the greatest challenges in spatial crowdsourcing is determining the veracity of reports from multiple users about a particular event or phenomenon. In this paper, we address the difficulties of truth discovery in spatio-temporal tasks and present a new method based on recursive Bayesian estimation (BE) from multiple reports of users. Our method incorporates a reliability model for users, which improves as more reports arrive while increasing the accuracy of the model in labeling the state of the event. The model is fur- ther improved by Kalman estimation (BE+KE) that models the spatio-temporal correlations of the events and predicts the next state of an event and is corrected when new reports arrive. The methods are tested in a simulated environment, as well as using real-world data. Experimental results show that our methods are adaptable to the available data, can incorporate previous beliefs, and outperform existing truth discovery methods of spatio-temporal events.",
    "subtype": "research",
    "authors": [
      {
        "name": "Daniel Garcia Ulloa",
        "affiliation": "Emory University"
      },
      {
        "name": "Li Xiong",
        "affiliation": "Emory University"
      },
      {
        "name": "Vaidy Sunderam",
        "affiliation": "Emory University"
      }
    ],
    "type": "research",
    "id": "1384"
  },
  "research1385": {
    "title": "Data Vocalization: Optimizing Voice Output of Relational Data",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p1574-trummer.pdf",
    "abstract": "Research on data visualization aims at finding the best way to present data via visual interfaces. We introduce the complementary problem of data vocalization. Our goal is to present relational data in the most efficient way via voice output. This problem setting is motivated by emerging tools and devices (e.g., Google Home, Amazon Echo, Apple's Siri, or voice-based SQL interfaces) that communicate data primarily via audio output to their users. We treat voice output generation as an optimization problem. The goal is to minimize speaking time while transmitting an approximation of a relational table to the user. We consider constraints on the precision of the transmitted data as well as on the cognitive load placed on the listener. We formalize voice output optimization and show that it is NP-hard. We present three approaches to solve that problem. First, we show how the problem can be translated into an integer linear program which enables us to apply corresponding solvers. Second, we present a two-phase approach that forms groups of similar rows in a pre-processing step, using a variant of the apriori algorithm. Then, we select an optimal combination of groups to generate a speech. Finally, we present a greedy algorithm that runs in polynomial time. Under simplifying assumptions, we prove that it generates near-optimal output by leveraging the sub-modularity property of our cost function. We compare our algorithms experimentally and analyze their complexity. ",
    "subtype": "research",
    "authors": [
      {
        "name": "Immanuel Trummer",
        "affiliation": "Cornell University"
      },
      {
        "name": "Jiancheng Zhu",
        "affiliation": "Cornell University"
      },
      {
        "name": "Mark Bryan",
        "affiliation": "Cornell University"
      }
    ],
    "type": "research",
    "id": "1385"
  },
  "research856": {
    "title": "Pivot-based Metric Indexing",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p1058-gao.pdf",
    "abstract": "The general notion of a metric space encompasses a diverse range of data types and accompanying similarity measures. Hence, met-ric search plays an important role in a wide range of settings, in-cluding multimedia retrieval, data mining, and data integration. With the aim of accelerating metric search, a collection of pivot-based indexing techniques for metric data has been proposed, which reduces the number of potentially expensive similarity comparisons by exploiting the triangle inequality for pruning and validation. However, no comprehensive empirical study of those techniques exists. Existing studies each offers only a narrower coverage, and they use different pivot selection strategies that af-fect performance substantially and thus render cross-study com-parisons difficult or impossible. We offer a survey of existing piv-ot-based indexing techniques, and report a comprehensive empiri-cal comparison of their construction costs, update efficiency, stor-age sizes, and similarity search performance. As part of the study, we provide modifications for two existing indexing techniques to make them more competitive. The findings and insights obtained from the study reveal different strengths and weaknesses of dif-ferent indexing techniques, and offer guidance on selecting an ap-propriate indexing technique for a given setting.",
    "subtype": "research",
    "authors": [
      {
        "name": "Lu Chen",
        "affiliation": "Zhejiang University"
      },
      {
        "name": "Yunjun Gao",
        "affiliation": "Zhejiang University"
      },
      {
        "name": "Baihua Zheng",
        "affiliation": "Singapore Management University"
      },
      {
        "name": "Christian Jensen",
        "affiliation": "Aalborg University"
      },
      {
        "name": "Hanyu Yang",
        "affiliation": "Zhejiang University"
      },
      {
        "name": "Keyu Yang",
        "affiliation": "Zhejiang University"
      }
    ],
    "type": "research",
    "id": "856"
  },
  "research319": {
    "title": "Interactive Time Series Exploration Powered by the Marriage of Similarity Distances",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p169-neamtu.pdf",
    "abstract": "Finding similar trends among time series data is critical for applications ranging from financial planning to policy making. The detection of these multifaceted relationships, especially time warped matching of time series of different lengths and alignments is prohibitively expensive to compute. To achieve real time responsiveness on large time series datasets, we propose a novel paradigm called Online Exploration of Time Series (ONEX) employing a powerful one-time preprocessing step that encodes critical similarity relationships to support subsequent rapid data exploration. Since the encoding of a huge number of pairwise similarity relationships for all variable lengths time series segments is not feasible, our work rests on the important insight that clustering with inexpensive point-to-point distances such as the Euclidean Distance can support subsequent time warped matching. Our ONEX framework overcomes the prohibitive computational costs associated with a more robust elastic distance namely the Dynamic Time Warping distance by applying it over the surprisingly compact knowledge base instead of the raw data. Our comparative study reveals that ONEX is up to 19% more accurate and several times faster than the state-of-the-art. Beyond being a highly accurate and fast domain independent solution, ONEX offers a truly interactive exploration experience supporting novel time series operations.",
    "subtype": "research",
    "authors": [
      {
        "name": "Rodica Neamtu",
        "affiliation": "Worcester Polytechnic Institute"
      },
      {
        "name": "Ramoza Ahsan",
        "affiliation": "Worcester Polytechnic Institute"
      },
      {
        "name": "Elke Rundensteiner",
        "affiliation": "Worcester Polytechnic Institute"
      },
      {
        "name": "Gabor Sarkozy",
        "affiliation": "Worcester Polytechnic Institute"
      }
    ],
    "type": "research",
    "id": "319"
  },
  "research606": {
    "title": "Local Search Methods for k-Means with Outliers",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p757-Lu.pdf",
    "abstract": "We study the problem of $k$-means clustering in the presence of outliers. The goal is to cluster a given set of data points to minimize the variance of the points assigned to the same cluster, with the freedom of ignoring a small set of data points that can be labeled as outliers. Clustering with outliers has received a lot of attention in the data processing community, but practical, efficient, and provably good algorithms remain unknown for the most popular $k$-means objective. Our work proposes a simple local search-based algorithm for $k$-means clustering with outliers. We prove that this algorithm achieves constant-factor approximate solutions and can be combined with known sketching techniques to scale to large data sets. Using empirical evaluation on both synthetic and real-world data, we demonstrate that the algorithm dominates recently proposed heuristic approaches for the problem.",
    "subtype": "research",
    "authors": [
      {
        "name": "Shalmoli Gupta",
        "affiliation": "UIUC"
      },
      {
        "name": "Ravi Kumar",
        "affiliation": "Google"
      },
      {
        "name": "Kefu Lu",
        "affiliation": "Washington University"
      },
      {
        "name": "Benjamin Moseley",
        "affiliation": "Washington University St. Louis"
      },
      {
        "name": "Sergei Vassilvitskii",
        "affiliation": "Google"
      }
    ],
    "type": "research",
    "id": "606"
  },
  "research1382": {
    "title": "Privacy-preserving Network Provenance",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p1550-zhang.pdf",
    "abstract": "Network accountability, forensic analysis, and failure diagnosis are becoming increasingly important for network management and security. { Network provenance} significantly aids network administrators in these tasks by explaining system behavior and revealing the dependencies between system states. Although resourceful, network provenance can sometimes be too rich, revealing potentially sensitive information that was involved in system execution. In this paper, we propose a cryptographic approach to preserve the confidentiality of provenance (sub)graphs while allowing users to query and access the parts of the graph for which they are authorized. Our proposed solution is a novel application of searchable symmetric encryption (SSE), first introduced by Curtmola et al. (CCS 2006). Our SSE-enabled provenance system allows a node to enforce access control policies over its provenance data even after the data has been shipped to remote nodes (e.g., for optimization purposes). We present a prototype of our design and demonstrate its practicality, scalability, and efficiency for both provenance maintenance and querying.",
    "subtype": "research",
    "authors": [
      {
        "name": "Yuankai Zhang",
        "affiliation": "Georgetown University"
      },
      {
        "name": "Adam O'Neill",
        "affiliation": "Georgetown University"
      },
      {
        "name": "Micah Sherr",
        "affiliation": "Georgetown University"
      },
      {
        "name": "Wenchao Zhou",
        "affiliation": "Georgetown University"
      }
    ],
    "type": "research",
    "id": "1382"
  },
  "research571": {
    "title": "SMCQL: Secure Query Processing for Private Data Networks",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p673-rogers.pdf",
    "abstract": "People and machines are collecting data at an unprecedented rate. Despite this newfound abundance of data, progress has been slow in sharing it for open science, business, and other data-intensive endeavors. Many such efforts are stymied by privacy concerns and regulatory compliance issues. For example, many hospitals are interested in pooling their medical records for research, but none may disclose arbitrary patient records to researchers or other healthcare providers. In this context we propose the Private Data Network (PDN), a federated database for querying over the collective data of mutually distrustful parties. In a PDN, each member database does not reveal its tuples to its peers nor to the query writer. Instead, the user submits a query to an honest broker that plans and coordinates its execution over multiple private databases using secure multiparty computation (SMC). Here, each database’s query execution is oblivious, and its program counters and memory traces are agnostic to the inputs of others. We introduce a framework for executing PDN queries named SMCQL. This system translates SQL statements into SMC primitives to compute query results over the union of its source databases without revealing sensitive information about individual tuples to peer data providers or the honest broker. Only the honest broker and the querier receive the results of a PDN query. For fast secure query evaluation, we explore a heuristics-driven optimizer that minimizes the PDN’s use of secure computation and partitions its query evaluation into scalable slices.",
    "subtype": "research",
    "authors": [
      {
        "name": "Johes Bater",
        "affiliation": "Northwestern University"
      },
      {
        "name": "Greg Elliott",
        "affiliation": "Northwestern University"
      },
      {
        "name": "Craig Eggen",
        "affiliation": "Northwestern University"
      },
      {
        "name": "Satyender Goel",
        "affiliation": "Northwestern University"
      },
      {
        "name": "Abel Kho",
        "affiliation": "Northwestern University"
      },
      {
        "name": "Jennie Rogers",
        "affiliation": "Northwestern University"
      }
    ],
    "type": "research",
    "id": "571"
  },
  "research138": {
    "title": "IL-Miner: Instance-Level Discovery of Complex Event Patterns",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p25-weidlich.pdf",
    "abstract": "Modern Internet applications often produce a large volume of user activity records. Data analysts are interested in cohort analysis, or finding unusual user behavioral trends, in these large tables of activity records. In a traditional database system, cohort analysis queries are both painful to specify and expensive to evaluate. We propose to extend database systems to support cohort analysis. We do so by extending SQL with three new operators. We devise three different evaluation schemes for cohort query processing. Two of them adopt a non-intrusive approach. The third approach employs a columnar based evaluation scheme with optimizations specifically designed for cohort query processing. Our experimental results confirm the performance benefits of our proposed columnar database system, compared against the two non-intrusive approaches that implement cohort queries on top of regular relational databases.",
    "subtype": "research",
    "authors": [
      {
        "name": "Lars George",
        "affiliation": "Humboldt-Universität zu Berlin"
      },
      {
        "name": "Bruno Cadonna",
        "affiliation": "Humboldt-Universität zu Berlin"
      },
      {
        "name": "Matthias Weidlich",
        "affiliation": "Humboldt-Universität zu Berlin"
      }
    ],
    "type": "research",
    "id": "138"
  },
  "research413": {
    "title": "VIP-Tree: An Effective Index for Indoor Spatial Queries",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p325-shao.pdf",
    "abstract": "Due to the growing popularity of indoor location-based services, indoor data management has received significant research attention in the past few years. However, we observe that the existing indexing and query processing techniques for the indoor space do not fully exploit the properties of the indoor space. Consequently, they provide below par performance which makes them unsuitable for large indoor venues with high query workloads. In this paper, we propose two novel indexes called Indoor Partitioning Tree (IP-Tree) and Vivid IP-Tree (VIP-Tree) that are carefully designed by utilizing the properties of indoor venues. The proposed indexes are lightweight, have small pre-processing cost and provide near- optimal performance for shortest distance and shortest path queries. We also present efficient algorithms for other spatial queries such as k nearest neighbors queries and range queries. Our extensive experimental study on real and synthetic data sets demonstrates that our proposed indexes outperform the existing algorithms by several orders of magnitude.",
    "subtype": "research",
    "authors": [
      {
        "name": "Zhou Shao",
        "affiliation": "Monash University"
      },
      {
        "name": "Muhammad Cheema",
        "affiliation": "Monash University"
      },
      {
        "name": "David Taniar",
        "affiliation": "Monash University"
      },
      {
        "name": "Hua Lu",
        "affiliation": "Aalborg University"
      }
    ],
    "type": "research",
    "id": "413"
  },
  "research499": {
    "title": "Plausible Deniability for Privacy-Preserving Data Synthesis",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p481-bindschaedler.pdf",
    "abstract": "Releasing full data records is one of the most challenging problems in data privacy. On the one hand, many of the popular techniques such as data de-identification are problematic because of their dependence on the background knowledge of adversaries. On the other hand, rigorous methods such as the exponential mechanism for differential privacy are often computationally impractical to use for releasing high dimensional data or cannot preserve high utility of original data due to their extensive data perturbation. This paper presents a criterion called plausible deniability that provides a formal privacy guarantee, notably for releasing sensitive datasets: an output record can be released only if a certain amount of input records are indistinguishable, up to a privacy parameter. This notion does not depend on the background knowledge of an adversary. Also, it can efficiently be checked by privacy tests. We present mechanisms to generate synthetic datasets with similar statistical properties to the input data and the same format. We study this technique both theoretically and experimentally. A key theoretical result shows that, with proper randomization, the plausible deniability mechanism generates differentially private synthetic data. We demonstrate the efficiency of this generative technique on a large dataset; it is shown to preserve the utility of original data with respect to various statistical analysis and machine learning measures.",
    "subtype": "research",
    "authors": [
      {
        "name": "Vincent Bindschaedler",
        "affiliation": "UIUC"
      },
      {
        "name": "Reza Shokri",
        "affiliation": "CornellTech"
      },
      {
        "name": "Carl Gunter",
        "affiliation": "UIUC"
      }
    ],
    "type": "research",
    "id": "499"
  },
  "research603": {
    "title": "LFTF: A Framework for Efficient Tensor Analytics at Scale",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p745-yang.pdf",
    "abstract": "Tensors are higher order generalizations of matrices to model multi-aspect data, e.g., a set of purchase records with the schema (user_id, product_id, timestamp, feedback). Tensor factorization is a powerful technique for generating a model from a tensor, just like matrix factorization generates a model from a matrix, but with higher accuracy and richer information as more attributes are available in a higher-order tensor than a matrix. The data model obtained by tensor factorization can be used for classification, recommendation, anomaly detection, and so on. Though having a broad range of applications, tensor factorization has not been popularly applied compared with matrix factorization that has been widely used in recommender systems, mainly due to the high computational cost and poor scalability of existing tensor factorization methods. Efficient and scalable tensor factorization is particularly challenging because real world tensor data are mostly sparse and massive. In this paper, we propose a novel distributed algorithm, called Lock-Free Tensor Factorization (LFTF), which significantly improves the efficiency and scalability of distributed tensor factorization by exploiting asynchronous execution in a re-formulated problem. Our experiments show that LFTF achieves much higher CPU and network throughput than existing methods, converges at least 17 times faster and scales to much larger datasets.",
    "subtype": "research",
    "authors": [
      {
        "name": "Fan Yang",
        "affiliation": "CUHK"
      },
      {
        "name": "Fanhua Shang",
        "affiliation": "CUHK"
      },
      {
        "name": "Yuzhen Huang",
        "affiliation": "CUHK"
      },
      {
        "name": "James Cheng",
        "affiliation": "CUHK"
      },
      {
        "name": "Jinfeng Li",
        "affiliation": "CUHK"
      },
      {
        "name": "Yunjian Zhao",
        "affiliation": "CUHK"
      },
      {
        "name": "Ruihao Zhao",
        "affiliation": "CUHK"
      }
    ],
    "type": "research",
    "id": "603"
  },
  "research213": {
    "title": "Toward High-Performance Distributed Stream Processing via Approximate Fault Tolerance",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p73-lee.pdf",
    "abstract": "Fault tolerance is critical for distributed stream processing systems, yet achieving error-free fault tolerance often incurs substantial performance overhead. We present AF-Stream, a distributed stream processing system that addresses the trade-off between performance and accuracy in fault tolerance. AF-Stream builds on a notion called approximate fault tolerance, whose idea is to mitigate backup overhead by adaptively issuing backups, while ensuring that the errors upon failures are bounded with theoretical guarantees. Our AF-Stream design provides an extensible programming model for incorporating general streaming algorithms, and also exports only few threshold parameters for configuring approximation fault tolerance. Experiments on Amazon EC2 show that AF-Stream maintains high performance (compared to no fault tolerance) and high accuracy after multiple failures (compared to no failures) under various streaming algorithms.",
    "subtype": "research",
    "authors": [
      {
        "name": "Qun Huang",
        "affiliation": "The Chinese Univ of Hong Kong"
      },
      {
        "name": "Patrick P. C. Lee",
        "affiliation": "The Chinese Univ of Hong Kong"
      }
    ],
    "type": "research",
    "id": "213"
  },
  "research612": {
    "title": "From Community Detection to Community Profiling",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p817-zheng.pdf",
    "abstract": "Most existing community-related studies focus on detection, which aim to find the community membership for each user from user friendship links. However, membership alone, without a complete profile of what a community is and how it interacts with other communities, has limited applications. This motivates us to consider systematically profiling the communities and thereby developing useful community-level applications. In this paper, we for the first time formalize the concept of community profiling. With rich user information on the network, such as user published content and user diffusion links, we characterize a community in terms of both its internal content profile and external diffusion profile. The difficulty of community profiling is often underestimated. We novelly identify three unique challenges and propose a joint Community Profiling and Detection (CPD) model to address them accordingly. We also contribute a scalable inference algorithm, which scales linearly with the data size and it is easily parallelizable. We evaluate CPD on large-scale real-world data sets, and show that it is significantly better than the state-of-the-art baselines in various tasks.",
    "subtype": "research",
    "authors": [
      {
        "name": "Hongyun Cai",
        "affiliation": "ADSC"
      },
      {
        "name": "Vincent Zheng",
        "affiliation": "Advanced Digital Sciences Cent"
      },
      {
        "name": "Fanwei Zhu",
        "affiliation": "Zhejiang University City College"
      },
      {
        "name": "Kevin Chen-Chuan Chang",
        "affiliation": "UIUC"
      },
      {
        "name": "Zi Huang",
        "affiliation": "The University of Queensland"
      }
    ],
    "type": "research",
    "id": "612"
  },
  "research565": {
    "title": "OLAK: An Efficient Algorithm to Prevent Unraveling in Social Networks",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p649-zhang.pdf",
    "abstract": "In this paper, we study the problem of anchored k-core. Given a graph G, an integer k and a budget b, we aim to identify b vertices in G so that we can determine the largest induced subgraph J in which every vertex, except the b vertices, has at least k neighbors in J. This problem was introduced by Bhawalkar and Kleinberg et al. in the context of user engagement in social networks, where a user may leave a community if he/she has less than k friends engaged. The problem has been shown to be NP-hard and inapproximable. A polynomial-time algorithm for graphs with bounded tree-width has been proposed. However, this assumption usually does not hold in real-life graphs, and their techniques cannot be extended to handle general graphs. Motivated by this, we propose an efficient algorithm, namely onion-layer based anchored k-core (OLAK), for the anchored k-core problem on large scale graphs. To facilitate computation of the anchored k-core, we design an onion layer structure, which is generated by a simple onion-peeling-like algorithm against a small set of vertices in the graph. We show that the computation of the best anchor can simply be conducted upon the vertices on the onion layers, which significantly reduces the search space. Based on the well-organized layer structure, we develop efficient candidates exploration, early termination and pruning techniques to further speed up computation. Comprehensive experiments on 10 real-life graphs demonstrate the effectiveness and efficiency of our proposed methods.",
    "subtype": "research",
    "authors": [
      {
        "name": "Fan Zhang",
        "affiliation": "UTS"
      },
      {
        "name": "Wenjie Zhang",
        "affiliation": "CSE"
      },
      {
        "name": "Ying Zhang",
        "affiliation": "QCIS"
      },
      {
        "name": "Lu Qin",
        "affiliation": "QCIS"
      },
      {
        "name": "Xuemin Lin",
        "affiliation": "CSE"
      }
    ],
    "type": "research",
    "id": "565"
  },
  "research1402": {
    "title": "Price-Optimal Querying with Data APIs",
    "acm_link": "http://www.vldb.org/pvldb/vol9/p1695-upadhyaya.pdf",
    "abstract": "Data is increasingly being purchased online in data markets and REST APIs have emerged as a favored method to acquire such data. Typically, sellers charge buyers based on how much data they purchase. In many scenarios, buyers need to make repeated calls to the seller’s API. The challenge is then for buyers to keep track of the data they purchase and avoid purchasing the same data twice. In this paper, we propose lightweight modifications to data APIs to achieve optimal history-aware pricing so that buyers are only charged once for data that they have purchased and that has not been updated. The key idea behind our approach is the notion of refunds: buyers buy data as needed but have the ability to ask for refunds of data that they had already purchased before. We show that our techniques can provide significant data cost savings while reducing overheads by two orders of magnitude as compared to the state-of-the-art competing approaches.",
    "subtype": "research",
    "authors": [
      {
        "name": "Prasang Upadhyaya",
        "affiliation": "University of Washington"
      },
      {
        "name": "Magdalena Balazinska",
        "affiliation": "University of Washington"
      },
      {
        "name": "Dan Suciu",
        "affiliation": "University of Washington"
      }
    ],
    "type": "research",
    "id": "1402"
  },
  "research337": {
    "title": "Stochastic Data Acquisition for Answering Queries as Time Goes by",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p277-ge.pdf",
    "abstract": "Data and actions are tightly coupled. On one hand, data analysis results trigger decision making and actions. On the other hand, the action of acquiring data is the very first step in the whole data processing pipeline. Data acquisition almost always has some costs, which could be either monetary costs or computing resource costs such as sensor battery power, network transfers, or I/O costs. Using outdated data to answer queries can avoid the data acquisition costs, but there is a penalty of potentially inaccurate results. Given a sequence of incoming queries over time, we study the problem of sequential decision making on when to acquire data and when to use existing versions to answer each query. We propose two approaches to solve this problem using reinforcement learning and tailored locality-sensitive hashing. A systematic empirical study using two real-world datasets shows that our approaches are effective and efficient.",
    "subtype": "research",
    "authors": [
      {
        "name": "Zheng Li",
        "affiliation": "University of Massachusetts Lowell"
      },
      {
        "name": "Tingjian Ge",
        "affiliation": "University of Massachusetts Lowell"
      }
    ],
    "type": "research",
    "id": "337"
  },
  "industrial975": {
    "title": "Probabilistic Demand Forecasting at Scale",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p1694-schelter.pdf",
    "abstract": "We present a platform built on large-scale, data-centric machine learning (ML) approaches, whose particular focus is demand forecasting in retail. At its core, this platform enables the training and application of probabilistic demand forecasting models, and provides convenient abstractions and support functionality for forecasting problems. The platform comprises of a complex end-to-end machine learning system built on Apache Spark, which includes data preprocessing, feature engineering, distributed learning, as well as evaluation, experimentation and ensembling. Furthermore, it meets the demands of a production system and scales to large catalogues containing millions of items. We describe the challenges of building such a platform and discuss our design decisions. We detail aspects on several levels of the system, such as a set of general distributed learning schemes, our machinery for ensembling predictions, and a high-level dataflow abstraction for modeling complex ML pipelines. To the best of our knowledge, we are not aware of prior work on real-world demand forecasting systems which rivals our approach in terms of scalability.",
    "subtype": "industrial",
    "authors": [
      {
        "name": "Joos-Hendrik Boese",
        "affiliation": "Amazon"
      },
      {
        "name": "Valentin Flunkert",
        "affiliation": "Amazon"
      },
      {
        "name": "Jan Gasthaus",
        "affiliation": "Amazon"
      },
      {
        "name": "Tim Januschowski",
        "affiliation": "Amazon"
      },
      {
        "name": "Dustin Lange",
        "affiliation": "Amazon"
      },
      {
        "name": "David Salinas",
        "affiliation": "Amazon"
      },
      {
        "name": "Sebastian Schelter",
        "affiliation": "Amazon"
      },
      {
        "name": "Matthias Seeger",
        "affiliation": "Amazon"
      },
      {
        "name": "Bernie Wang",
        "affiliation": "Amazon"
      }
    ],
    "type": "industrial",
    "id": "975"
  },
  "research574": {
    "title": "Effective Community Search over Large Spatial Graphs",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p709-fang.pdf",
    "abstract": "Communities are prevalent in social networks, knowledge graphs, and biological networks. Recently, the topic of community search (CS) has received plenty of attention. Given a query vertex, CS looks for a dense subgraph that contains it. Existing CS solutions do not consider the spatial extent of a community. They can yield communities whose locations of vertices span large areas. In applications that facilitate the creation of social events (e.g., finding conference attendees to join a dinner), it is important to find groups of peoples who are physically close to each other. In this situation, it is desirable to have a spatial-aware community (or SAC), whose vertices are close structurally and spatially. Given a graph G and a query vertex q, we develop exact solutions for finding an SAC that contains q. Since these solutions cannot scale to large datasets, we have further designed three approximation algorithms to compute an SAC. We have performed an experimental evaluation for these solutions on several large real datasets. Experimental results show that SAC is better than the communities returned by existing solutions. Moreover, our approximation solutions can find SACs accurately and efficiently.",
    "subtype": "research",
    "authors": [
      {
        "name": "Yixiang Fang",
        "affiliation": "Hong Kong University"
      },
      {
        "name": "Reynold Cheng",
        "affiliation": "Hong Kong University"
      },
      {
        "name": "Xiaodong Li",
        "affiliation": "Hong Kong University"
      },
      {
        "name": "Siqiang Luo",
        "affiliation": "Hong Kong University"
      },
      {
        "name": "Jiafeng Hu",
        "affiliation": "Hong Kong University"
      }
    ],
    "type": "research",
    "id": "574"
  },
  "industrial1190": {
    "title": "Colt: Concept Lineage Tool for Data Flow Metadata Capture and Analysis",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p1790-aggour.pdf",
    "abstract": "Most organizations are becoming increasingly data-driven, often processing data from many different sources to enable critical business operations. Beyond the well-addressed challenge of storing and processing large volumes of data, financial institutions in particular are increasingly subject to federal regulations requiring high levels of accountability for the accuracy and lineage of this data. For companies like GE Capital, which maintain data across a globally interconnected network of thousands of systems, it is becoming increasingly challenging to capture an accurate understanding of the data flowing between those systems. To address this problem, we designed and developed a concept lineage tool allowing organizational data flows to be modeled, visualized and interactively explored. This tool has novel features that allow a data flow network to be contextualized in terms of business-specific metadata such as the concept, business, and product for which it applies. Key analysis features have been implemented, including the ability to trace the origination of particular datasets, and to discover all systems where data is found that meets some user-defined criteria. This tool has been readily adopted by users at GE Capital and in a short time has already become a business-critical application, with over 2,200 data systems and over 1,000 data flows captured.",
    "subtype": "industrial",
    "authors": [
      {
        "name": "Kareem Aggour",
        "affiliation": "GE Global Research"
      },
      {
        "name": "Jenny Weisenberg Williams",
        "affiliation": "GE Global Research"
      },
      {
        "name": "Justin McHugh",
        "affiliation": "GE Global Research"
      },
      {
        "name": "Vijay Kumar",
        "affiliation": "GE Global Research"
      }
    ],
    "type": "industrial",
    "id": "1190"
  },
  "demo918": {
    "title": "Interactive Navigation of Open Data Linkages",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p1837-zhu.pdf",
    "abstract": "We developed Toronto Open Data Search to support the ad hoc, interactive discovery of connections or linkages between datasets. It can be used to efficiently navigate through the open data cloud. Our system consists of three parts: a user-interface provided by a Web application; a scalable backend infrastructure that supports navigational queries; and a dynamic repository of open data tables. Our system uses LSH Ensemble, an efficient index structure, to compute linkages (attributes in two datasets with high containment score) in real time at Internet scale. Our application allows users to navigate along these linkages by joining datasets. LSH Ensemble is scalable, providing millisecond response times for linkage discovery queries even over millions of datasets. Our system offers users a highly interactive experience making unrelated (and unlinked) dynamic collections of datasets appear as a richly connected cloud of data that can be navigated and combined easily in real time.",
    "subtype": "demo",
    "authors": [
      {
        "name": "Erkang Zhu",
        "affiliation": "University of Toronto"
      },
      {
        "name": "Ken Pu",
        "affiliation": "UOIT"
      },
      {
        "name": "Fatemeh Nargesian",
        "affiliation": "University of Toronto"
      },
      {
        "name": "Renee Miller",
        "affiliation": "University of Toronto"
      }
    ],
    "type": "demo",
    "id": "918"
  },
  "demo919": {
    "title": "noWorkflow: a Tool for Collecting, Analyzing, and Managing Provenance from Python Scripts",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p1841-pimentel.pdf",
    "abstract": "We present noWorkflow, an open-source tool that systematically and transparently collects provenance from Python scripts, including data about the script execution and how the script evolves over time. During the demo, we will show how noWorkflow collects and manages provenance, as well as how it supports the analysis of computational experiments. We will also encourage attendees to use noWorkflow for their own scripts.",
    "subtype": "demo",
    "authors": [
      {
        "name": "João Felipe Pimentel",
        "affiliation": "Universidade Federal Fluminense"
      },
      {
        "name": "Leonardo Murta",
        "affiliation": "Universidade Federal Fluminense"
      },
      {
        "name": "Vanessa Braganholo",
        "affiliation": "Universidade Federal Fluminense"
      },
      {
        "name": "Juliana Freire",
        "affiliation": "NYU"
      }
    ],
    "type": "demo",
    "id": "919"
  },
  "demo928": {
    "title": "ARShop: A Cloud-based Augmented Reality System for Shopping",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p1845-wang.pdf",
    "abstract": "ARShop is a one-stop solution for shopping in the cyber-physical world with the help of crowd knowledge and augmented reality. Its ultimate goal is to improve customers' shopping experience. When a customer enters a physical shop and snaps a shot, the enriched cyber information of the surroundings will pop up and be augmented on the screen. ARShop can also be the customer's personal shopping assistant who can show routes to the shops that the customer is interested in. In addition, ARShop provides merchants with a web-based interface to manage their shops and promote their business to customers, and provides customers with an Android App to query using images.",
    "subtype": "demo",
    "authors": [
      {
        "name": "Chao Wang",
        "affiliation": "NUS"
      },
      {
        "name": "Yihao Feng",
        "affiliation": "Dartmouth College"
      },
      {
        "name": "Qi Guo",
        "affiliation": "NUS"
      },
      {
        "name": "Zhaoxian Li",
        "affiliation": "NUS"
      },
      {
        "name": "Kexin Liu",
        "affiliation": "NUS"
      },
      {
        "name": "Zijian Tang",
        "affiliation": "NUS"
      },
      {
        "name": "Anthony Tung",
        "affiliation": "NUS"
      },
      {
        "name": "Lifu Wu",
        "affiliation": "NUS"
      },
      {
        "name": "Yuxin Zheng",
        "affiliation": "NUS"
      }
    ],
    "type": "demo",
    "id": "928"
  },
  "demo939": {
    "title": "Mind the Gap: Bridging Multi-Domain Query Workloads with EmptyHeaded",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p1849-aberger.pdf",
    "abstract": "Executing domain specific workloads from a relational data warehouse is an increasingly popular task. Unfortunately, classic relational database management systems (RDBMS) are suboptimal in many domains (e.g., graph and linear algebra queries), and it is challenging to transfer data from an RDBMS to a domain specific toolkit in an efficient manner. This demonstration showcases the EmptyHeaded engine: an interactive query processing engine that leverages a novel architecture to support efficient execution in multiple domains. To enable a unified design, the EmptyHeaded architecture is built around recent theoretical advancements in join processing and automated in-query data transformations. This demonstration highlights the strengths and weaknesses of this novel type of query processing architecture while showcasing its flexibility in multiple domains. In particular, attendees will use EmptyHeaded's Jupyter notebook front-end to interactively learn the theoretical advantages of this new (and largely unknown) approach and directly observe its performance impact in multiple domains.",
    "subtype": "demo",
    "authors": [
      {
        "name": "Christopher Aberger",
        "affiliation": "Stanford University"
      },
      {
        "name": "Andrew Lamb",
        "affiliation": "Stanford University"
      },
      {
        "name": "Kunle Olukotun",
        "affiliation": "Stanford University"
      },
      {
        "name": "Christopher Ré",
        "affiliation": "Stanford University"
      }
    ],
    "type": "demo",
    "id": "939"
  },
  "demo948": {
    "title": "Crossing the finish line faster when paddling the Data Lake with Kayak",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p1853-maccioni.pdf",
    "abstract": "Paddling in a data lake is strenuous for a data scientist. Being a loosely-structured collection of raw data with little or no meta-information available, the difficulties of extracting insights from a data lake start from the initial phases of data analysis. Indeed, data preparation, which involves many complex operations (such as source and feature selection, exploratory analysis, data profiling, and data curation), is a long and involved activity for navigating the lake before getting precious insights at the finish line. In this framework, we demonstrate Kayak, a framework that supports data preparation in a data lake with ad-hoc primitives and allows data scientists to cross the finish line sooner. Kayak takes into account the tolerance of the user in waiting for the primitives' results and it uses incremental execution strategies to produce informative previews of these results. The framework is based on a wise management of metadata and on features that limit human intervention, thus scaling smoothly when the data lake evolves.",
    "subtype": "demo",
    "authors": [
      {
        "name": "Antonio Maccioni",
        "affiliation": "Collective[i]"
      },
      {
        "name": "Riccardo Torlone",
        "affiliation": "Roma Tre University"
      }
    ],
    "type": "demo",
    "id": "948"
  },
  "demo951": {
    "title": "Debugging Transactions and Tracking their Provenance with Reenactment",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p1857-niu.pdf",
    "abstract": "Debugging transactions and understanding their execution are of immense importance for developing OLAP applications, to trace causes of errors in production systems, and to audit the operations of a database. However, debugging transactions is hard for several reasons: 1) after the execution of a transaction, its input is no longer available for debugging, 2) internal states of a transaction are typically not accessible, and 3) the execution of a transaction may be affected by concurrently running transactions. We present a debugger for transactions that enables non-invasive, post-mortem debugging of transactions with provenance tracking and supports what-if scenarios (changes to transaction code or data). Using reenactment, a declarative replay technique we have developed, a transaction is replayed over the state of the DB seen by its original execution including all its interactions with concurrently executed transactions from the history. Importantly, our approach uses the temporal database and audit logging capabilities available in many DBMS and does not require any modifications to the underlying database system nor transactional workload.",
    "subtype": "demo",
    "authors": [
      {
        "name": "Xing Niu",
        "affiliation": "IIT"
      },
      {
        "name": "Bahareh Sadat Arab",
        "affiliation": "Illinois Institute of Technology"
      },
      {
        "name": "Seokki Lee",
        "affiliation": "Illinois Institute of Technology"
      },
      {
        "name": "Su Feng",
        "affiliation": "Illinois Institute of Technology"
      },
      {
        "name": "Xun Zou",
        "affiliation": "Illinois Institute of Technology"
      },
      {
        "name": "Dieter Gawlick",
        "affiliation": "Oracle"
      },
      {
        "name": "Vasudha Krishnaswamy",
        "affiliation": "Oracle"
      },
      {
        "name": "Zhen Hua Liu",
        "affiliation": "Oracle"
      },
      {
        "name": "Boris Glavic",
        "affiliation": "Illinois Institute of Technology"
      }
    ],
    "type": "demo",
    "id": "951"
  },
  "demo956": {
    "title": "PICASSO: Exploratory Search of Connected Subgraph Substructures in Graph Databases",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p1861-bhowmick.pdf",
    "abstract": "Recently, exploratory search has received much attention in information retrieval and database fields. This search paradigm assists users who do not have a clear search intent and are unfamiliar with the underlying data space. Specifically, query formulation evolves iteratively as the user becomes more familiar with the content. Despite its growing importance, exploratory search on graph-structured data has received little attention in the literature. We demonstrate a system called PICASSO to realize exploratory substructure search on a graph database containing a set of small or medium-sized data graphs. PICASSO embodies several novel features such as progressive (i.e., iterative) formulation of queries visually and incremental processing, multistream results exploration wall to visualize, explore, and analyze search results to identify possible search directions.",
    "subtype": "demo",
    "authors": [
      {
        "name": "Kai Huang",
        "affiliation": "Fudan University"
      },
      {
        "name": "Sourav S Bhowmick",
        "affiliation": "Nanyang Technological University"
      },
      {
        "name": "Shuigeng Zhou",
        "affiliation": "Fudan University"
      },
      {
        "name": "Byron Choi",
        "affiliation": "Hong Kong Baptist University"
      }
    ],
    "type": "demo",
    "id": "956"
  },
  "demo960": {
    "title": "DITIR: Distributed Index for High Throughput Trajectory Insertion and Real-time Temporal Range Query",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p1865-wang.pdf",
    "abstract": "The prosperity of mobile social network and location-based services, e.g., Uber, is backing the explosive growth of spatial temporal streams on the Internet. It raises new challenges to the underlying data store system, which is supposed to support extremely high-throughput trajectory insertion and low-latency querying with spatial and temporal constraints. State-of-the-art solutions, e.g., HBase, do not render satisfactory performance, due to the high overhead on index update and the lack of appropriate load balancing. In this demonstration, we present our new system prototype, DITIR, tailored to efficiently processing temporal and range queries over historical data as well as latest updates. Our system provides better performance guarantee, by physically partitioning the incoming data tuples on their arrivals and exploiting a template-based B+tree insertion schema, to reach the desired ingestion throughput. Load balancing mechanism is also introduced to DITIR, by using which the system is capable of achieving reliable performance against workload dynamics. Our demonstration shows that DITIR supports over 1 million tuple insertions in a second, when running on a 10-node cluster. It also significantly outperforms HBase by 7 times on ingestion throughput and 5 times faster on query response latency.",
    "subtype": "demo",
    "authors": [
      {
        "name": "Ruichu Cai",
        "affiliation": "Guangdong University of Technology"
      },
      {
        "name": "Zijie Lu",
        "affiliation": "Guangdong University of Technology"
      },
      {
        "name": "Li Wang",
        "affiliation": "Advanced Digital Sciences Center"
      },
      {
        "name": "Zhenjie Zhang",
        "affiliation": "Advanced Digital Sciences Center"
      },
      {
        "name": "Tom Fu",
        "affiliation": "Advanced Digital Sciences Center"
      },
      {
        "name": "Marianne Winslett",
        "affiliation": "University of Illinois at Urbana-Champaign"
      }
    ],
    "type": "demo",
    "id": "960"
  },
  "demo965": {
    "title": "FlashView: An Interactive Visual Explorer for Raw Data",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p1869-pang.pdf",
    "abstract": "New data has been generated in an unexpected high speed. To get insight of those data, data analysts will perform a thorough study using state-of-the-art big data analytical tools. Before the analysis starts, a preprocessing is conducted, where data analyst tends to issue a few ad-hoc queries on a new dataset to explore and gain a better understanding. However, it is costly to perform such ad-hoc queries on large scale data using traditional data management systems, e.g., DBMS, because data loading and indexing are very expensive. In this demo, we propose a novel visual data explorer system, FlashView, which omits the loading process by directly querying raw data. FlashView applies approximate query processing technique to achieve real-time query results. It builds both in-memory index and disk index to facilitate the data scanning. It also supports tracking and updating multiple queries concurrently. Note that FlashView is not designed as a replacement of full-ﬂedged DBMS. Instead, it tries to help the analysts quickly understand the characteristics of data, so he/she can selectively load data into the DBMS to do more sophisticated analysis.",
    "subtype": "demo",
    "authors": [
      {
        "name": "Zhifei Pang",
        "affiliation": "Zhejiang University"
      },
      {
        "name": "Sai Wu",
        "affiliation": "Zhejiang University"
      },
      {
        "name": "Gang Chen",
        "affiliation": "Zhejiang University"
      },
      {
        "name": "Ke Chen",
        "affiliation": "Zhejiang University"
      },
      {
        "name": "Lidan Shou",
        "affiliation": "Zhejiang University"
      }
    ],
    "type": "demo",
    "id": "965"
  },
  "demo974": {
    "title": "Upsortable: Programming TopK Queries Over Data Streams",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p1873-subercaze.pdf",
    "abstract": "Top-k queries over data streams is a well studied problem. While there exists numerous systems allowing to process continuous queries over sliding windows, requirements that go beyond the sliding window paradigm imply ad-hoc designed solutions, e.g. tailor-made solutions using a standard programming languages. In the meantime, the Stream API and lambda expressions have been added in Java 8, thus gaining powerful operations for data stream processing. However, the Java Collections Framework does not provide data structures to safely and convienently support sorted collections of evolving data. In this paper, we demonstrate texttt{Upsortable}, an annotation-based approach that allows to use existing sorted collections from the standard Java API for dynamic data management. Our approach relies on a combination of pre-compilation abstract syntax tree modifications and runtime analysis of bytecode.  Upsortable offer the developer a safe and time-efficient solution for developing top-k queries on data streams while keeping a full compatibility with standard Java.",
    "subtype": "demo",
    "authors": [
      {
        "name": "Julien Subercaze",
        "affiliation": "Lab Hubert Curien"
      },
      {
        "name": "Christophe Gravier",
        "affiliation": "Lab Hubert Curien"
      },
      {
        "name": "Syed Gillani",
        "affiliation": "Lab Hubert Curien"
      },
      {
        "name": "Abderrahmen Kammoun",
        "affiliation": "Lab Hubert Curien"
      },
      {
        "name": "Frédérique Laforest",
        "affiliation": "Lab Hubert Curien"
      }
    ],
    "type": "demo",
    "id": "974"
  },
  "demo976": {
    "title": "QUIS: InSitu Heterogeneous Data Source Querying",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p1877-chamanara.pdf",
    "abstract": "Existing data integration frameworks are poorly suited for the special requirements of scientists. To answer a specific research question, often, excerpts of data from different sources need to be integrated. The relevant parts and the set of underlying sources may differ from query to query. The analyses also oftentimes involve frequently changing data and exploratory querying. Additionally, The data sources not only store data in different formats, but also provide inconsistent data access functionality. The classic Extract-Transform-Load (ETL) approach seems too complex and time-consuming and does not fit well with interest and expertise of the scientists.  With QUIS (QUery In-Situ), we provide a solution for this problem. QUIS is an open source heterogeneous in-situ data querying system. It utilizes a federated query virtualization approach that is built upon plugged-in adapters. QUIS takes a user query and transforms appropriate portions of it into the corresponding computation model on individual data sources and executes it. It complements the segments of the query that the target data sources can not execute. Hence, it guarantees full syntax and semantic support for its language on all data sources. QUIS's in-situ querying facility almost eliminates the time to prepare the data while maintaining a competitive performance and steady scalability.  The present demonstration illustrates interesting features of the system: virtual schemas, heterogeneous joins, and visual query results. We provide a realistic data processing scenario to examine the system's features. Users can interact with QUIS using its desktop workbench, command line interface, or from any R client including RStudio Server.",
    "subtype": "demo",
    "authors": [
      {
        "name": "Javad Chamanara",
        "affiliation": "Friedrich Schiller University of Jena"
      },
      {
        "name": "Birgitta König-Ries",
        "affiliation": "Friedrich Schiller University of Jena"
      },
      {
        "name": "H. V. Jagadish",
        "affiliation": "University of Michigan"
      }
    ],
    "type": "demo",
    "id": "976"
  },
  "demo996": {
    "title": "Automating Data Citation in CiteDB",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p1881-alawini.pdf",
    "abstract": "An increasing amount of information is being collected in structured, evolving, curated databases, driving the question of how information extracted from such datasets via queries should be cited. While several databases say how data should be cited for web-page views of the database, they leave it to users to manually construct the citations. Furthermore, they do not say how data extracted by queries other than web-page views --general queries-- should be cited. This demo shows how citations can be specified for a small set of views of the database, and used to automatically generate citations for general queries against the database.",
    "subtype": "demo",
    "authors": [
      {
        "name": "Abdussalam Alawini",
        "affiliation": "University of Pennsylvania"
      },
      {
        "name": "Susan Davidson",
        "affiliation": "University of Pennsylvania"
      },
      {
        "name": "Wei Hu",
        "affiliation": "University of Pennsylvania"
      },
      {
        "name": "Yinjun Wu",
        "affiliation": "University of Pennsylvania"
      }
    ],
    "type": "demo",
    "id": "996"
  },
  "demo1060": {
    "title": "C-Explorer: Browsing Communities in Large Graphs",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p1885-fang.pdf",
    "abstract": "Community search (CS) and detection (CD), which enable the retrieval of subgraphs from large social networks (e.g., Facebook and Twitter), have attracted a lot of interest. Various solutions, such as k-core, k-truss, and CODICIL, have been proposed to obtain communities, or subgraphs with closely-related vertices. To facilitate the process of retrieving communities, we design C-Explorer (or community explorer), a system that provides online and interactive community search. In C-Explorer, a user can view his/her interesting graphs, indicate his/her required vertex q, and display the communities that q belongs to. A seminal feature of C-Explorer is that it supports an attributed graph (i.e., a graph containing vertices associated with labels and keywords), and provides search of an attributed community (or AC), which contains vertices that are structurally and semantically related. Moreover, C-Explorer implements several state-of-the-art CS/CD algorithms, and functions for analyzing their effectiveness. We plan to make C-Explorer an open-source platform, and design API functions that enable input of graphs, output of communities, and customization of CS/CD algorithms.",
    "subtype": "demo",
    "authors": [
      {
        "name": "Yixiang Fang",
        "affiliation": "Hong Kong University"
      },
      {
        "name": "Reynold Cheng",
        "affiliation": "Hong Kong University"
      },
      {
        "name": "Siqiang Luo",
        "affiliation": "Hong Kong University"
      },
      {
        "name": "Jiafeng Hu",
        "affiliation": "Hong Kong University"
      },
      {
        "name": "Kai Huang",
        "affiliation": "Hong Kong University"
      }
    ],
    "type": "demo",
    "id": "1060"
  },
  "demo1063": {
    "title": "GRAPE: Parallelizing Sequential Graph Computations",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p1889-fan.pdf",
    "abstract": "We demonstrate GRAPE, a parallel GRAPh query Engine. GRAPE advocates a parallel model based on a simultaneous fixed point computation in terms of partial and incremental evaluation. It differs from prior systems in its ability to parallelize existing sequential graph algorithms as a whole, without the need for recasting the entire algorithms into a new model. One of its unique features is that under a monotonic condition, GRAPE parallelization guarantees to terminate with correct answers as long as the sequential algorithms “plugged in” are correct. We demonstrate its parallel computations, ease-of-use and performance compared with the start-of-the-art graph systems. We also demonstrate a use case of GRAPE in social media marketing.",
    "subtype": "demo",
    "authors": [
      {
        "name": "Wenfei Fan",
        "affiliation": "University of Edinburgh and Beihang University"
      },
      {
        "name": "Jingbo Xu",
        "affiliation": "University of Edinburgh and Beihang University"
      },
      {
        "name": "Yinghui Wu",
        "affiliation": "Washington State University"
      },
      {
        "name": "Wenyuan Yu",
        "affiliation": "Beihang University"
      },
      {
        "name": "Jiaxin Jiang",
        "affiliation": "Hong Kong Baptist University"
      }
    ],
    "type": "demo",
    "id": "1063"
  },
  "demo1081": {
    "title": "Flower: A Data Analytics Flow Elasticity Manager",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p1893-khoshkbarforoushha.pdf",
    "abstract": "A data analytics flow typically operates on three layers: ingestion, analytics, and storage, each of which is provided by a data-intensive system. These systems are often available as cloud managed services, enabling the users to have pain-free deployment of data analytics flow applications such as click-stream analytics. Despite straightforward orchestration, elasticity management of the flows is challenging. This is due to: a) heterogeneity of workloads and diversity of cloud resources such as queue partitions, compute servers and NoSQL throughputs capacity, b) workload dependencies between the layers, and c) different performance behaviours and resource consumption patterns. In this demonstration, we present Flower, a holistic elasticity management system that exploits advanced optimization and control theory techniques to manage elasticity of complex data analytics flows on clouds. Flower analyzes statistics and data collected from different data-intensive systems to provide the user with a suite of rich functionalities, including: workload dependency analysis, optimal resource share analysis, dynamic resource provisioning, and cross-platform monitoring. We will showcase various features of Flower using a real-world data analytics flow. We will allow the audience to explore Flower by visually defining and configuring a data analytics flow elasticity manager and get hands-on experience with integrated data analytics flow management.",
    "subtype": "demo",
    "authors": [
      {
        "name": "Alireza Khoshkbarforoushha",
        "affiliation": "Australian National University"
      },
      {
        "name": "Rajiv Ranjan",
        "affiliation": "Newcastle University"
      },
      {
        "name": "Qing Wang",
        "affiliation": "Australian National University"
      },
      {
        "name": "Carsten Friedrich",
        "affiliation": "CSIRO"
      }
    ],
    "type": "demo",
    "id": "1081"
  },
  "demo1086": {
    "title": "STEED: An Analytical Database System for TrEE-structured Data",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p1897-chen.pdf",
    "abstract": "Tree-structured data formats, such as JSON and Protocol Buffers, are capable of expressing sophisticated data types, including nested, repeated, and missing values. While such expressing power contributes to their popularity in real-world applications, it presents a significant challenge for systems supporting tree-structured data. Existing systems have focused on general-purpose solutions either extending RDBMSs or designing native systems. However, the general-purpose approach often results in sophisticated data structures and algorithms, which may not reflect and optimize for the actual structure patterns in the real world. In this demonstration, we showcase Steed, an analytical database System for tree-structured data. We use the insights gained by analyzing representative real-world tree structured data as guidelines in the design of Steed. Steed learns and extracts a schema tree for a data set and uses the schema tree to reduce the storage space and improve the efficiency of data field accesses. We observe that sub-structures in real world data are often simple, while the tree-structured data types can support very sophisticated structures. We optimize the storage structure, the column assembling algorithm, and the in-memory layout for the simple sub-structures (a.k.a. simple paths). Compared to representative state-of-the-art systems (i.e. PostgreSQL/JSON, MongoDB, and Hive+Parquet), Steed achieves orders of magnitude better performance for data analysis queries.",
    "subtype": "demo",
    "authors": [
      {
        "name": "Zhiyi Wang",
        "affiliation": "Chinese Academy of Sciences"
      },
      {
        "name": "Dongyan Zhou",
        "affiliation": "Chinese Academy of Sciences"
      },
      {
        "name": "Shimin Chen",
        "affiliation": "Chinese Academy of Sciences "
      }
    ],
    "type": "demo",
    "id": "1086"
  },
  "demo1112": {
    "title": "LocLok: Location Cloaking with Differential Privacy via Hidden Markov Model",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p1901-xiao.pdf",
    "abstract": "We demonstrate LocLok, a LOCation-cLOaKing system to protect the locations of a user with differential privacy. LocLok has two features: (a) it protects locations under temporal correlations described through hidden Markov model; (b) it releases the optimal noisy location with the planar isotropic mechanism (PIM), the first mechanism that achieves the lower bound of differential privacy. We show the detailed computation of LocLok with the following components: (a) how to generate the possible locations with Markov model, (b) how to perturb the location with PIM, and (c) how to make inference about the true location in Markov model. An online system with real-word dataset will be presented with the computation details.",
    "subtype": "demo",
    "authors": [
      {
        "name": "Yonghui Xiao",
        "affiliation": "Emory University"
      },
      {
        "name": "Li Xiong",
        "affiliation": "Emory University"
      },
      {
        "name": "Si Zhang",
        "affiliation": "Jianghan University"
      },
      {
        "name": "Yang Cao",
        "affiliation": "Emory University "
      }
    ],
    "type": "demo",
    "id": "1112"
  },
  "demo1116": {
    "title": "Strider: An Adaptive, Inference-enabled Distributed RDF Stream Processing Engine",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p1905-ren.pdf",
    "abstract": "Real-time processing of data streams emanating from sensors is becoming a common task in industrial scenarios. An increasing number of processing jobs executed over such platforms are requiring reasoning mechanisms. The key implementation goal is thus to efficiently handle massive incoming data streams and support reasoning, data analytic services. Moreover, in an on-going industrial project on anomaly detection in large potable water network, we are facing the effect of dynamically changing data and work characteristics in stream processing. The Strider system, an adaptive, inference-enabled query processor for continuous SPARQL queries, addresses these research and implementation challenges. By considering scalability, fault-tolerance, high throughput and acceptable latency properties, the system is designed for industrial projects. We will demonstrate the benefits of Strider on an Internet of Things-based real world and industrial setting.",
    "subtype": "demo",
    "authors": [
      {
        "name": "Xiangnan Ren",
        "affiliation": "ATOS"
      },
      {
        "name": "Olivier Curé",
        "affiliation": "UPEM LIGM - UMR CNRS 8049"
      },
      {
        "name": "Li Ke",
        "affiliation": "ATOS"
      },
      {
        "name": "Jérémy Lhez",
        "affiliation": "UPEM LIGM - UMR CNRS 8049"
      },
      {
        "name": "Badre Belabbess",
        "affiliation": "ATOS"
      },
      {
        "name": "Tendry Randriamalala",
        "affiliation": "ATOS"
      },
      {
        "name": "Yufan Zheng",
        "affiliation": "ATOS"
      },
      {
        "name": "Gabriel Kepeklian",
        "affiliation": "ATOS"
      }
    ],
    "type": "demo",
    "id": "1116"
  },
  "demo1120": {
    "title": "A Confidence-Aware Top-k Query Processing Toolkit on Crowdsourcing",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p1909-U.pdf",
    "abstract": "Ranking techniques have been widely used in ubiquitous applications like recommendation, information retrieval, etc. For ranking computation hostile but human friendly items, crowdsourcing is considered as an emerging technique to process the ranking by human power. However, there is a lack of an easy-to-use toolkit for answering crowdsourced top-k query with minimal effort. In this work, we demonstrate an interactive programming toolkit that is a unified solution for answering the crowdsourced top-k queries. The toolkit employs a new confidence-aware crowdsourced top-k algorithm, SPR. The whole progress of the algorithm is monitored and visualized to end users in a timely manner. Besides the visualized result and the statistics, the system also reports the estimation of the monetary cost and the breakdown of each phase. Based on the estimation, end users can strike a balance between the budget and the quality through the interface of this toolkit.",
    "subtype": "demo",
    "authors": [
      {
        "name": "Yan Li",
        "affiliation": "University of Macau"
      },
      {
        "name": "Ngai Meng Kou",
        "affiliation": "University of Macau"
      },
      {
        "name": "Hao Wang",
        "affiliation": "Nanjing University"
      },
      {
        "name": "Leong Hou U",
        "affiliation": "University of Macau"
      },
      {
        "name": "Zhiguo Gong",
        "affiliation": "University of Macau"
      }
    ],
    "type": "demo",
    "id": "1120"
  },
  "demo1129": {
    "title": "Explaining and Querying Knowledge Graphs by Relatedness",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p1913-fionda.pdf",
    "abstract": "We demonstrate RECAP, a tool that explains relatedness between entities in Knowledge Graphs (KGs) and implements a query by relatedness paradigm that allows to retrieve entities related to those in input. One of the peculiarities of RECAP is that it does not require any data prepro- cessing and can combine knowledge from multiple KGs. The underlying algorithmic techniques are reduced to the execution of SPARQL queries plus some local refinement. This makes the tool readily available on a large variety of KGs accessible via SPARQL endpoints. To show the general applicability of the tool, we will cover a set of use cases drawn from a variety of knowledge domains (e.g., biology, movies, co-authorship networks) and report on the concrete usage of RECAP in the SENSE4US FP7 project. We will underline the technical aspects of the system and give details on its implementation. The target audience of the demo includes both researchers and practitioners and aims at reporting on the benefits of RECAP in practical knowledge discovery ap- plications.",
    "subtype": "demo",
    "authors": [
      {
        "name": "Valeria Fionda",
        "affiliation": "University of Calabria"
      },
      {
        "name": "Giuseppe Pirrò",
        "affiliation": "ICAR-CNR"
      }
    ],
    "type": "demo",
    "id": "1129"
  },
  "demo1141": {
    "title": "Thoth in Action: Memory Management in Modern Data Analytics",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p1917-kunjir.pdf",
    "abstract": "Allocation and usage of memory in modern data-processing platforms is based on an interplay of algorithms at multiple levels: (i) at the resource-management level across containers allocated by resource managers like Mesos and Yarn, (ii) at the container level among the OS and processes such as the Java Virtual Machine (JVM), (iii) at the framework level for caching, aggregation, data shuffles, and application data structures, and (iv) at the JVM level across various pools such as the Young and Old Generation as well as the heap versus off-heap. We use Thoth, a data-driven platform for multi-system cluster management, to build a deep understanding of different interplays in memory management options. Through multiple memory management apps built in Thoth, we demonstrate how Thoth can deal with multiple levels of memory management as well as multi-tenant nature of clusters.",
    "subtype": "demo",
    "authors": [
      {
        "name": "Mayuresh Kunjir",
        "affiliation": "Duke University"
      },
      {
        "name": "Shivnath Babu",
        "affiliation": "Duke University"
      }
    ],
    "type": "demo",
    "id": "1141"
  },
  "demo1144": {
    "title": "Monopedia: Staying Single is Good Enough - The HyPer Way for Web Scale Applications",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p1921-schuele.pdf",
    "abstract": "In order to handle the database load for web scale applications, the conventional wisdom is that a cluster of database servers and a caching layer are essential. In this work, we argue that modern main-memory database systems are often fast enough to consolidate this complex architecture into a single server (plus an additional fail over system). To demonstrate this claim, we design the Monopedia Benchmark, a benchmark for web scale applications modeled after Wikipedia. Using this benchmark, we show that it is indeed possible to run the database workload of one of the largest web sites in the world on a single database server.",
    "subtype": "demo",
    "authors": [
      {
        "name": "Maximilian Schüle",
        "affiliation": "TUM"
      },
      {
        "name": "Pascal Schliski",
        "affiliation": "TUM"
      },
      {
        "name": "Thomas Hutzelmann",
        "affiliation": "TUM"
      },
      {
        "name": "Tobias Rosenberger",
        "affiliation": "TUM"
      },
      {
        "name": "Viktor Leis",
        "affiliation": "TUM"
      },
      {
        "name": "Dimitri Vorona",
        "affiliation": "TUM"
      },
      {
        "name": "Alfons Kemper",
        "affiliation": "TUM"
      },
      {
        "name": "Thomas Neumann",
        "affiliation": "TUM"
      }
    ],
    "type": "demo",
    "id": "1144"
  },
  "demo1146": {
    "title": "Dima: A Distributed In-Memory Similarity-Based Query Processing System",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p1925-sun.pdf",
    "abstract": "Data analysts in industries spend more than 80% of time on data cleaning and integration in the whole process of data analytics due to data errors and inconsistencies. It calls for effective query processing techniques to tolerate the errors and inconsistencies. In this paper, we develop a distributed in-memory similarity-based query processing system called Dima. Dima supports two core similarity-based query operations, i.e., similarity search and similarity join. Dima extends the SQL programming interface for users to easily invoke these two operations in their data analysis jobs. To avoid expensive data transformation in a distributed environment, we design selectable signatures where two records approximately match if they share common signatures. More importantly, we can adaptively select the signatures to balance the workload. Dima builds signature-based global indexes and local indexes to support efficient similarity search and join. Since Spark is one of the widely adopted distributed in-memory computing systems, we have seamlessly integrated Dima into Spark and developed effective query optimization techniques in Spark. To the best of our knowledge, this is the first full-fledged distributed in-memory system that can support similarity-based query processing. We demonstrate our system in several scenarios, including entity matching, web table integration and query recommendation.",
    "subtype": "demo",
    "authors": [
      {
        "name": "Ji Sun",
        "affiliation": "Tsinghua University"
      },
      {
        "name": "Zeyuan Shang",
        "affiliation": "Tsinghua University"
      },
      {
        "name": "Guoliang Li",
        "affiliation": "Tsinghua University"
      },
      {
        "name": "Dong Deng",
        "affiliation": "MIT"
      },
      {
        "name": "Zhifeng Bao",
        "affiliation": "RMIT University"
      }
    ],
    "type": "demo",
    "id": "1146"
  },
  "demo1154": {
    "title": "TeCoRe: Temporal Conflict Resolution in Knowledge Graphs",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p1929-schoenfisch.pdf",
    "abstract": "The management of uncertainty is crucial when harvest- ing structured content from unstructured and noisy sources. Knowledge Graphs (kgs), maintaining both numerical and non-numerical facts supported by an underlying schema, are a prominent example. Knowledge Graph management is challenging because: (i) most of existing kgs focus on static data, thus impeding the availability of timewise knowledge; (ii) facts in kgs are usually accompanied by a confidence score, which witnesses how likely it is for them to hold. We demonstrate TeCoRe, a system for temporal infer- ence and conflict resolution in uncertain temporal knowl- edge graphs (utkgs). At the heart of TeCoRe are two state-of-the-art probabilistic reasoners that are able to deal with temporal constraints efficiently. While one is scalable, the other can cope with more expressive constraints. The demonstration will focus on enabling users and applications to find inconsistencies in utkgs. TeCoRe provides an inter- face allowing to select utkgs and editing constraints; shows the maximal consistent subset of the utkg, and displays statistics (e.g., number of noisy facts removed) about the debugging process.",
    "subtype": "demo",
    "authors": [
      {
        "name": "Melisachew Chekol",
        "affiliation": "University of Mannheim"
      },
      {
        "name": "Giuseppe Pirrò",
        "affiliation": "ICAR-CNR"
      },
      {
        "name": "Joerg Schoenfisch",
        "affiliation": "University of Mannheim"
      },
      {
        "name": "Heiner Stuckenschmidt",
        "affiliation": "University of Mannheim"
      }
    ],
    "type": "demo",
    "id": "1154"
  },
  "demo1161": {
    "title": "MLog: Towards Declarative In-Database Machine Learning",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p1933-zhang.pdf",
    "abstract": "We demonstrate MLog, a high-level language that integrates machine learning into data management systems. Unlike existing machine learning frameworks (e.g., TensorFlow, Theano, and Caffe), MLog is declarative, in the sense that the system manages all data movement, data persistency, and machine-learning related optimizations (such as data batching) automatically. Our interactive demonstration will show audience how this is achieved based on the novel notion of tensoral views (TViews), which are similar to relational views but operate over tensors with linear algebra. With MLog, users can succinctly specify not only simple models such as SVM (in just two lines), but also sophisticated deep learning models that are not supported by existing in-database analytics systems (e.g., MADlib, PAL, and SciDB), as a series of cascaded TViews. Given the declarative nature of MLog, we further demonstrate how query/program optimization techniques can be leveraged to translate MLog programs into native TensorFlow programs. The performance of the automatically generated TensorFlow programs is comparable to that of hand-optimized ones.",
    "subtype": "demo",
    "authors": [
      {
        "name": "Xupeng Li",
        "affiliation": "Peking University"
      },
      {
        "name": "Bin Cui",
        "affiliation": "Peking University"
      },
      {
        "name": "Yiru Chen",
        "affiliation": "Peking University"
      },
      {
        "name": "Wentao Wu",
        "affiliation": "Microsoft Research"
      },
      {
        "name": "Ce Zhang",
        "affiliation": "ETH"
      }
    ],
    "type": "demo",
    "id": "1161"
  },
  "demo1173": {
    "title": "Foresight: Recommending Visual Insights",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p1937-parthasarathy.pdf",
    "abstract": "Current tools for exploratory data analysis (EDA) require users to manually select data attributes, statistical computations and visual encodings. This can be daunting for large-scale, complex data. We introduce Foresight, a system that helps the user rapidly discover visual insights from large high dimensional datasets. Formally, an “insight” is a strong manifestation of a statistical property of the data, e.g., high correlation between two attributes, high skewness or concentration about the mean of a single attribute, a strong clustering of values, and so on. For each insight type, Foresight initially presents visualizations of the top k instances in the data, based on an appropriate ranking metric. The user can then look at “nearby” insights by issuing “insight queries” containing constraints on insight strengths and data attributes. Thus the user can directly explore the space of insights, rather than the space of data dimensions and visual encodings as in other visual recommender systems. Foresight also provides “global” views of insight space to help orient the user and ensure a thorough exploration process. Furthermore, Foresight facilitates interactive exploration of large datasets through fast, approximate sketching.",
    "subtype": "demo",
    "authors": [
      {
        "name": "Çağatay Demiralp",
        "affiliation": "IBM"
      },
      {
        "name": "Peter Haas",
        "affiliation": "IBM"
      },
      {
        "name": "Srinivasan Parthasarathy",
        "affiliation": "IBM"
      },
      {
        "name": "Tejaswini Pedapati",
        "affiliation": "IBM"
      }
    ],
    "type": "demo",
    "id": "1173"
  },
  "demo1189": {
    "title": "A BAD Demonstration: Towards Big Active Data",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p1941-jacobs.pdf",
    "abstract": "Nearly all of today's Big Data systems are passive in nature. We demonstrate our Big Active Data (BAD) system, a scalable system that continuously and reliably captures Big Data and facilitates the timely and automatic delivery of new information to a large population of interested users as well as supporting analyses of historical information. We built our BAD project by extending an existing scalable, open-source BDMS (AsterixDB) in this active direction. In this demonstration, we allow our audience to participate in an emergency notification application built on top of our BAD platform, and highlight its capabilities.",
    "subtype": "demo",
    "authors": [
      {
        "name": "Steven Jacobs",
        "affiliation": "University of California at Riverside"
      },
      {
        "name": "Md Yusuf Sarwar Uddin",
        "affiliation": "University of California at Irvine"
      },
      {
        "name": "Michael Carey",
        "affiliation": "University of California at Irvine"
      },
      {
        "name": "Vagelis Hristidis",
        "affiliation": "University of California at Riverside"
      },
      {
        "name": "Vassilis Tsotras",
        "affiliation": "University of California at Riverside"
      },
      {
        "name": "Nalini Venkatasubram",
        "affiliation": "University of California at Irvine"
      },
      {
        "name": "Yao Wu",
        "affiliation": "University of California at Irvine"
      },
      {
        "name": "Syed Safir",
        "affiliation": "University of California at Irvine"
      },
      {
        "name": "Purvi Kaul",
        "affiliation": "University of California at Irvine"
      },
      {
        "name": "Xikui Wang",
        "affiliation": "University of California at Irvine"
      },
      {
        "name": "Mohiuddin Abdul Qader",
        "affiliation": "University of California"
      },
      {
        "name": "Yawei Li",
        "affiliation": "University of California at Riverside "
      }
    ],
    "type": "demo",
    "id": "1189"
  },
  "demo1214": {
    "title": "ClaimBuster:The First-ever End-to-end Fact-checking System",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p1945-li.pdf",
    "abstract": "Recently there has been a proliferation of fact-checking activities around the globe. However, the amount of information required to be fact-checked is still way beyond the capability of current fact-checkers. In this paper, we present ClaimBuster, the first end-to-end automated fact-checking system. ClaimBuster uses machine learning, natural language processing, and database query techniques to identify check-worthy factual claims from various sources and provides a true-or-false verdict for certain types of factual claims by leveraging existing fact-checks and querying knowledge bases. ClaimBuster is used to live cover broadcasted TV programs (e.g., 2016 U.S. presidential election debates) and monitor social media platforms and websites for identifying check-worthy factual claims. The performance of ClaimBuster is compared with professional journalists from fact-checking organizations. This paper explains the components of the system and describes a demonstration plan.",
    "subtype": "demo",
    "authors": [
      {
        "name": "Naeemul Hassan",
        "affiliation": "University of Mississippi"
      },
      {
        "name": "Gensheng Zhang",
        "affiliation": "University of Texas Arlington"
      },
      {
        "name": "Fatma Arslan",
        "affiliation": "University of Texas Arlington"
      },
      {
        "name": "Josue Caraballo",
        "affiliation": "University of Texas Arlington"
      },
      {
        "name": "Damian Jimenez",
        "affiliation": "University of Texas Arlington"
      },
      {
        "name": "Siddhant Gawsane",
        "affiliation": "University of Texas Arlington"
      },
      {
        "name": "Shohedul Hasan",
        "affiliation": "University of Texas Arlington"
      },
      {
        "name": "Minumol Joseph",
        "affiliation": "University of Texas Arlington"
      },
      {
        "name": "Aaditya Kulkarni",
        "affiliation": "University of Texas Arlington"
      },
      {
        "name": "Anil Kumar Nayak",
        "affiliation": "University of Texas Arlington"
      },
      {
        "name": "Vikas Sable",
        "affiliation": "University of Texas Arlington"
      },
      {
        "name": "Chengkai Li",
        "affiliation": "University of Texas at Arlington"
      },
      {
        "name": "Mark Tremayne",
        "affiliation": "University of Texas Arlington"
      }
    ],
    "type": "demo",
    "id": "1214"
  },
  "demo1222": {
    "title": "QIRANA Demonstration: Real time Scalable Query Pricing",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p1949-deep.pdf",
    "abstract": "The last decade has seen a deluge in data collection and dissemination across a broad range of areas. This phenomena has led to creation of online data markets where entities engage in sale and purchase of data. In this scenario, the key challenge for the data market platform is to ensure that it allows real time, scalable, arbitrage-free pricing of user queries. At the same time, the platform needs to flexible enough for sellers in order to customize the setup of the data to be sold. In this paper, we describe the demonstration of textsc{Qirana}, a light weight framework that implements query-based pricing at scale. The framework acts as a layer between the end users (buyers and sellers) and the database. textsc{Qirana}'s demonstration features that we highlight are: (i) allows sellers to choose from a variety of pricing functions based on their requirements and incorporates price points as a guide for query pricing; (ii) helps the seller set parameters by mocking workloads; (iii) buyers engage with the platform by directly asking queries and track their budget per dataset; .We demonstrate the tunable parameters of our framework over a real-world dataset, illustrating the promise of our approach.",
    "subtype": "demo",
    "authors": [
      {
        "name": "Shaleen Deep",
        "affiliation": "UW-Madison"
      },
      {
        "name": "Paris Koutris",
        "affiliation": "UW-Madison"
      },
      {
        "name": "Yash Bidasaria",
        "affiliation": "UW-Madison"
      }
    ],
    "type": "demo",
    "id": "1222"
  },
  "demo1224": {
    "title": "DataTweener: A Demonstration of a Tweening Engine for Incremental Visualization of Data Transforms",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p1953-khan.pdf",
    "abstract": "With development and advancement of new data interaction modalities, data exploration and analysis has become a highly interactive process situating the user in a session of successive queries. With continuously changing results, it becomes difficult to comprehend transformations, especially the ones corresponding to complex queries. We introduce data tweening as an informative way of visualizing structural data transforms, presenting the users with a series of incremental visual representations of a resultset transformation. We present transformations as ordered sequences of basic structural transforms and visual cues. The sequences are generated using an automated framework which utilizes differences between the consecutive resultsets and queries in a query session. We evaluate the effectiveness of tweening as a visualization method through a user study.",
    "subtype": "demo",
    "authors": [
      {
        "name": "Meraj Ahmed Khan",
        "affiliation": "Ohio State University"
      },
      {
        "name": "Larry Xu",
        "affiliation": "UC Berkeley"
      },
      {
        "name": "Arnab Nandi",
        "affiliation": "Ohio State University"
      },
      {
        "name": "Joseph Hellerstein",
        "affiliation": "UC Berkeley"
      }
    ],
    "type": "demo",
    "id": "1224"
  },
  "demo1256": {
    "title": "A Demonstration of ST-Hadoop: A MapReduce Framework for Big Spatio-temporal Data",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p1961-alarabi.pdf",
    "abstract": "This demo presents ST-Hadoop; the first full-fledged open-source MapReduce framework with a native support for spatio-temporal data. ST-Hadoop injects spatio-temporal awareness in the Hadoop base code, which results in achieving order(s) of magnitude better performance than Hadoop and SpatialHadoop when dealing with spatio-temporal data and queries. The key idea behind ST-Hadoop is its ability in indexing spatio-temporal data within Hadoop Distributed File System (HDFS). A real system prototype of ST-Hadoop, running on a local cluster of 24 machines, is demonstrated with two big-spatio-temporal datasets of Twitter and NYC Taxi data, each of around one billion records.",
    "subtype": "demo",
    "authors": [
      {
        "name": "Louai Alarabi",
        "affiliation": "University of Minnesota"
      },
      {
        "name": "Mohamed Mokbel",
        "affiliation": "University of Minnesota "
      }
    ],
    "type": "demo",
    "id": "1256"
  },
  "demo1259": {
    "title": "Creation and Interaction with Large-scale Domain-Specific Knowledge Bases",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p1965-Danilevsky.pdf",
    "abstract": "The ability to create and interact with large-scale domain-specific knowledge bases from unstructured/semi-structured data is the foundation for many industry-focused cognitive systems. We will demonstrate the Content Services system that provides cloud services for creating and querying high-quality domain-specific knowledge bases by analyzing and integrating multiple (un/semi)structured content sources. We will showcase an instantiation of the system for a financial domain. We will also demonstrate both cross-lingual natural language queries and programmatic API calls for interacting with this knowledge base.",
    "subtype": "demo",
    "authors": [
      {
        "name": "Shreyas Bharadwaj",
        "affiliation": "IBM Watson"
      },
      {
        "name": "Laura Chiticariu",
        "affiliation": "IBM Research-Almaden"
      },
      {
        "name": "Marina Danilevsky",
        "affiliation": "IBM Research-Almaden"
      },
      {
        "name": "Samarth Dhingra",
        "affiliation": "IBM Watson"
      },
      {
        "name": "Samved Divekar",
        "affiliation": "IBM Watson"
      },
      {
        "name": "Arnaldo Carreno-Fuentes",
        "affiliation": "IBM Watson"
      },
      {
        "name": "Himanshu Gupta",
        "affiliation": "IBM Research-India"
      },
      {
        "name": "Nitin Gupta",
        "affiliation": "IBM Research-India"
      },
      {
        "name": "Sang-Don Han",
        "affiliation": "IBM Watson"
      },
      {
        "name": "Mauricio Hernandez",
        "affiliation": "IBM Research-Almaden"
      },
      {
        "name": "Howard Ho",
        "affiliation": "IBM Watson"
      },
      {
        "name": "Parag Jain",
        "affiliation": "IBM Research-India"
      },
      {
        "name": "Salil Joshi",
        "affiliation": "IBM Research-India"
      },
      {
        "name": "Hima Karanam",
        "affiliation": "IBM Research-India"
      },
      {
        "name": "Saravanan Krishnan",
        "affiliation": "IBM Research-India"
      },
      {
        "name": "Rajasekar Krishnamurthy",
        "affiliation": "IBM Research-Almaden"
      },
      {
        "name": "Yunyao Li",
        "affiliation": "IBM Research-Almaden"
      },
      {
        "name": "Satishkumaar Manivannan",
        "affiliation": "IBM Watson"
      },
      {
        "name": "Ashish Mittal",
        "affiliation": "IBM Research-India"
      },
      {
        "name": "Fatma Ozcan",
        "affiliation": "IBM Research-Almaden"
      },
      {
        "name": "Abdul Quamar",
        "affiliation": "IBM Research-Almaden"
      },
      {
        "name": "Poornima Raman",
        "affiliation": "IBM Watson"
      },
      {
        "name": "Diptikalyan Saha",
        "affiliation": "IBM Research-India"
      },
      {
        "name": "Karthik Sankaranarayanan",
        "affiliation": "IBM Research-India"
      },
      {
        "name": "Jaydeep Sen",
        "affiliation": "IBM Research-India"
      },
      {
        "name": "Prithviraj Sen",
        "affiliation": "IBM Research-Almaden"
      },
      {
        "name": "Shivakumar Vaithyanathan",
        "affiliation": "IBM Watson"
      },
      {
        "name": "Mitesh Vasa",
        "affiliation": "IBM Watson"
      },
      {
        "name": "Hao Wang",
        "affiliation": "IBM Watson"
      },
      {
        "name": "Huaiyu Zhu",
        "affiliation": "IBM Research-Almaden"
      }
    ],
    "type": "demo",
    "id": "1259"
  },
  "demo1261": {
    "title": "A Demonstration of Stella: A Crowdsourcing-Based Geotagging Framework",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p1969-jonathan.pdf",
    "abstract": "This paper demonstrates Stella; an efficient crowdsourcing-based geotagging framework for any types of objects. In this demonstration, we showcase the effectiveness of Stella in geotagging images via two different scenarios: (1) we provide a graphical interface to show the process of a geotagging process that have been done by using Amazon Mechanical Turk, (2) we seek help from the conference attendees to propose an image to be geotagged or to help us geotag an image by using our application during the demonstration period. At the end of the demonstration period, we will show the geotagging result.",
    "subtype": "demo",
    "authors": [
      {
        "name": "Christopher Jonathan",
        "affiliation": "University of Minnesota"
      },
      {
        "name": "Mohamed Mokbel",
        "affiliation": "University of Minnesota"
      }
    ],
    "type": "demo",
    "id": "1261"
  },
  "demo1270": {
    "title": "Exploring big volume sensor data with Vroom",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p1973-moll.pdf",
    "abstract": "State of the art sensors within a single autonomous vehicle (AV) produce video and lidar data at more than 30 GB/hour. Unsurprisingly, even small AV research teams easily accumulate tens of terabytes of sensor data from multiple trips and multiple vehicles. AV practitioners would like to extract information about specific locations or specific situations for further study, but are often unable to. Queries over AV sensor data are different from generic analytics or spatial queries because they demand reasoning about fields of view as well as heavy computation to extract features from scenes. In this demo we present Vroom, a system for ad-hoc queries over AV sensor databases. Vroom combines domain specific properties of AV datasets with selective indexing and multi-query optimization to rise to the challenges posed by AV sensor data.",
    "subtype": "demo",
    "authors": [
      {
        "name": "Oscar Moll",
        "affiliation": "MIT"
      },
      {
        "name": "Aaron Zalewski",
        "affiliation": "MIT"
      },
      {
        "name": "Sudeep Pillai",
        "affiliation": "MIT"
      },
      {
        "name": "Samuel Madden",
        "affiliation": "MIT"
      },
      {
        "name": "Michael Stonebraker",
        "affiliation": "MIT"
      },
      {
        "name": "Vijay Gadepally",
        "affiliation": "MIT Lincoln Labs"
      }
    ],
    "type": "demo",
    "id": "1270"
  },
  "awards1": {
    "title": "Women in Database Research Award Talk: 7 Secrets That My Mother Didn't Tell Me",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p2020-milo.pdf",
    "abstract": "What does it take to be a good researcher? And, is it different when you are a woman? These are questions that many of us are wondering about throughout our career. Being honored with a VLDB Women in Database Research Award, I would like to share with you in this talk some of the secrets to successful research that I have learned over the years. These secrets highlight some of the fundamental research directions that I have taken. No less importantly, they explain how I successfully got to work on them, both personally and professionally. ",
    "subtype": "awards",
    "authors": [
      {
        "name": "Tova Milo",
        "affiliation": "Tel Aviv University"
      }
    ],
    "type": "awards",
    "id": "1"
  },
  "awards2": {
    "title": "Early Career Award Talk: Human-in-the-loop Data Integration",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p2006-li.pdf",
    "abstract": "Data integration aims to integrate data in different sources and provide users with a unified view. However, data integration cannot be completely addressed by purely automated methods. We propose a hybrid human-machine data integration framework that harnesses human ability to address this problem, and apply it initially to the problem of entity matching. The framework first uses rule-based algorithms to identify possible matching pairs and then utilizes the crowd to compute actual matching pairs from these candidate pairs. In the first step, we propose similarity-based rules and knowledge-based rules to obtain the candidate matching pairs, develop effective algorithms to learn these rules based on positive and negative examples, and build a distributed in-memory system to efficiently apply these rules. In the second step, we propose a selection-inference-refine framework that uses the crowd to verify the candidate pairs. We first select some representative tasks to ask the crowd, use transitivity rules and partial order to infer the answers of unasked tasks based on the crowd results of the asked tasks, and refine the inferred answers with low confidence to improve the quality. We develop a crowd-powered database system CDB that allows users to utilize a SQL-like language for processing crowd-based queries. Finally, we provide emerging challenges in human-in-the-loop data integration. ",
    "subtype": "awards",
    "authors": [
      {
        "name": "Guoliang Li",
        "affiliation": "Tsinghua University"
      }
    ],
    "type": "awards",
    "id": "2"
  },
  "awards3": {
    "title": "Ten Year Best Paper Award Talk: Intelligent Probing for Locality Sensitive Hashing: Multi-Probe LSH and Beyond",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p2021-lv.pdf",
    "abstract": "The past decade has been marked by the (continued) explosion of diverse data content and the fast development of intelligent data analytics techniques. One problem we identified in the mid-2000s was similarity search of feature-rich data. The challenge here was achieving both high accuracy and high efficiency in high-dimensional spaces. Locality sensitive hashing (LSH), which uses certain random space partitions and hash table lookups to find approximate nearest neighbors, was a promising approach with theoretical guarantees. But LSH alone was insufficient since a large number of hash tables were required to achieve good search quality. Building on an idea of Panigrahy, our multi-probe LSH method introduces the idea of intelligent probing. Given a query object, we strategically probe its neighboring hash buckets (in a query-dependent fashion) by calculating the statistical probabilities of similar objects falling into each bucket. Such intelligent probing can significantly reduce the number of hash tables while achieving high quality. In this paper, we revisit the problem motivation, the challenges, the key design considerations of multi-probe LSH, as well as discuss recent developments in this space and some questions for further research.",
    "subtype": "awards",
    "authors": [
      {
        "name": "Qin Lv",
        "affiliation": "University of Colorado Boulder"
      },
      {
        "name": "William Josephson",
        "affiliation": "Solano Labs"
      },
      {
        "name": "Zhe Wang",
        "affiliation": "Datrium"
      },
      {
        "name": "Moses Charikar",
        "affiliation": "Stanford University"
      },
      {
        "name": "Kai Li",
        "affiliation": "Princeton University"
      }
    ],
    "type": "awards",
    "id": "3"
  },
  "keynote1": {
    "title": "The Data Center under your Desk - How Disruptive is Modern Hardware for DB System Design?",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p2018-lehner.pdf",
    "abstract": "While we are already used to see more than 1,000 cores within a single machine, the next processing platforms for database engines will be widely heterogeneous with built-in GPU-style processors as well as specialized FPGAs and chips with domain-specific instruction sets taking advantage of the “Dark Silicon” effect. Moreover, the traditional volatile as well as the upcoming non-volatile RAM with capacities in the 100s of TBytes per machine will provide great opportunities for storage engines but also call for radical changes on the architecture of such systems. Finally, the emergence of economically affordable, high-speed/low-latency interconnects as a basis for rack-scale computing is questioning long-standing folklore algorithmic assumptions but will certainly play an important role of the big picture for building modern data management platforms. While database research on modern hardware has already produced a rich bouquet of promising results targeting a wide variety of hardware directions, the talk will try to classify and review existing approaches from a performance, robustness, as well as energy efficiency perspective. Moreover, the talk will discuss the overall question on how these results can be incorporated into the design and implementation of modern DB systems. The goal is therefore to outline current trends and research activities as well as to pinpoint to interesting starting points for further research activities.",
    "subtype": "keynote",
    "authors": [
      {
        "name": "Wolfgang Lehner",
        "affiliation": "Technische Universität Dresden"
      }
    ],
    "type": "keynote",
    "id": "1"
  },
  "keynote2": {
    "title": "Big Data Software: What’s Next?",
    "abstract": "The Big Data revolution has been enabled in part by a wealth of innovation in software platforms for data storage, analytics, and machine learning.  The design of Big Data platforms such as Hadoop and Spark focused on scalability, fault-tolerance and performance.  As these and other systems increasingly become part of the mainstream, the next set of challenges are becoming clearer.  Requirements for performance are changing as workloads evolve to include techniques such as hardware-accelerated deep learning. But more fundamentally, other issues are moving to the forefront.  These include ease of use for a wide range of users, security, concerns about privacy and potential bias in results, and the perennial problems of data quality and integration from heterogeneous sources. Fortunately, the database community has much to say about all of these topics, and can and should take a leading role in addressing them. In this talk, I will give an overview of how we got here, with an emphasis on the development of the Apache Spark system. I will then focus on these emerging issues with an eye towards where the database community can most effectively engage.",
    "subtype": "keynote",
    "authors": [
      {
        "name": "Michael Franklin",
        "affiliation": "University of Chicago"
      }
    ],
    "type": "keynote",
    "id": "2"
  },
  "plenary3": {
    "title": "Deep Learning (m)eats Databases",
    "abstract": "Imagine a machine that is able to compose music and write poems; paint realistic artificial images and dream up video from textual descriptions; paint pictures or entire videos in the style of any artist; translate in-between any pair of natural languages. A machine that can recognize any content in images and videos; diagnose diseases, imitate spoken language — in any voice. A machine that wins games thought to be exclusive to human intelligence. All of that with superhuman performance of course.  Sounds like science fiction? Well, then welcome to the year 2017! Currently we are witnessing the biggest revolution in computer science since the invention of the Internet. Deep Learning is shaking the world of computer science and overrunning entire (sub-)disciplines. In this talk I will briefly sketch some of the recent advances in deep learning and what they have to do with databases. Where are synergies? Where should we be looking at? This talk will have a particular focus on recent technical developments in the intersection of databases and/or deep learning in Europe.",
    "subtype": "plenary",
    "authors": [
      {
        "name": "Jens Dittrich",
        "affiliation": "Saarland University"
      }
    ],
    "type": "plenary",
    "id": "3"
  },
  "panel1": {
    "title": "Interdisciplinary research and the impact of data management/systems research outside our own community",
    "abstract": "In the last few years, many members of our community have moved to positions heading interdisciplinary efforts (Schools, Colleges, Institutes). Almost all are cross-disciplinary organizations with differing mixtures of disciplines. “Data Science” units is a major example in that area.   We've done lots of useful things in database research, but our impact on other areas of computing has arguably been smaller than other subfields of computing. What lessons do we have to share more broadly, and what can we do to increase our impact? How to approach people from other areas and bootstrap collaborations? Other interesting issues: Single organization or federation of disciplines / departments, methodologies for collaboration, project specific or generalizable, are there data science methods analogous to the empirical method for the scientific method, etc. ",
    "subtype": "panel",
    "authors": [
      {
        "name": "Timos Sellis (moderator)",
        "affiliation": "Swinburne University of Technology"
      },
      {
        "name": "Michael Franklin",
        "affiliation": "University of Chicago"
      },
      {
        "name": "Johann-Christoph Freytag",
        "affiliation": "Humboldt-Universität zu Berlin"
      },
      {
        "name": "Raymond Ng",
        "affiliation": "University of British Columbia"
      },
      {
        "name": "Matthias Renz",
        "affiliation": "George Mason University"
      },
      {
        "name": "Kian-Lee Tan",
        "affiliation": "NUS"
      }
    ],
    "type": "panel",
    "id": "1"
  },
  "tutorial893": {
    "title": "New Trends on Exploratory Methods for Data Analytics",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p1977-mottin.pdf",
    "abstract": "Data usually comes in a plethora of formats and dimensions, rendering the exploration and information extraction processes cumbersome. Thus, being able to cast exploratory queries in the data with the intent of having an immediate glimpse on some of the data properties is becoming crucial. An exploratory query should be simple enough to avoid complicate declarative languages (such as SQL) and mechanisms, and at the same time retain the flexibility and expressiveness of such languages. Recently, we have witnessed a rediscovery of the so called example-based methods, in which the user, or the analyst circumvent query languages by using examples as input. An example is a representative of the intended results, or in other words, an item from the result set. Example-based methods exploit inherent characteristics of the data to infer the results that the user has in mind, but may not able to (easily) express. They can be useful both in cases where a user is looking for information in an unfamiliar dataset, or simply when she is exploring the data without knowing what to find in there. In this tutorial, we present an excursus over the main methods for exploratory analysis, with a particular focus on example-based methods. We show how different data types require different techniques, and present algorithms that are specifically designed for relational, textual, and graph data.",
    "subtype": "tutorial",
    "authors": [
      {
        "name": "Davide Mottin",
        "affiliation": "HPI"
      },
      {
        "name": "Matteo Lissandrini",
        "affiliation": "University of Trento"
      },
      {
        "name": "Yannis Velegrakis",
        "affiliation": "University of Trento"
      },
      {
        "name": "Themis Palpanas",
        "affiliation": "Paris Descartes University"
      }
    ],
    "type": "tutorial",
    "id": "893"
  },
  "tutorial907": {
    "title": "Summarizing Static and Dynamic Big Graphs",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p1981-khan.pdf",
    "abstract": "Large-scale, highly-interconnected networks pervade our society and the natural world around us, including the World Wide Web, social networks, knowledge graphs, genome and scientific databases, medical and government records. The massive scale of graph data often surpasses the available computation and storage resources. Besides, users get overwhelmed by the daunting task of understanding and using such graphs due to their sheer volume and complexity. Hence, there is a critical need to summarize large graphs into concise forms that can be more easily visualized, processed, and managed. Graph summarization has indeed attracted a lot of interests from various research communities, such as sociology, physics, chemistry, bioinformatics, and computer science. Different ways of summarizing graphs have been invented that are often complementary to each other. In this tutorial, we discuss algorithmic advances on graph summarization in the context of both classical (e.g., static graphs) and emerging (e.g., dynamic and stream graphs) applications. We emphasize the current challenges and highlight some future research directions.",
    "subtype": "tutorial",
    "authors": [
      {
        "name": "Arijit Khan",
        "affiliation": "NTU Singapore"
      },
      {
        "name": "Sourav S Bhowmick",
        "affiliation": "Nanyang Technological University"
      },
      {
        "name": "Francesco Bonchi",
        "affiliation": "ISI Foundation"
      }
    ],
    "type": "tutorial",
    "id": "907"
  },
  "tutorial1125": {
    "title": "Geometric Approaches for Top-k Queries",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p1985-mouratidis.pdf",
    "abstract": "Top-k processing is a well-studied problem with numerous applications that is becoming increasingly relevant with the growing availability of recommendation systems and decision making software. The objective of this tutorial is twofold. First, we will delve into the geometric aspects of top-k processing. Second, we will cover complementary features to top-k queries, with strong practical relevance and important applications, that have a computational geometric nature. The tutorial will close with insights in the effect of dimensionality on the meaningfulness of top-k queries, and interesting similarities to nearest neighbor search.",
    "subtype": "tutorial",
    "authors": [
      {
        "name": "Kyriakos Mouratidis",
        "affiliation": "Singapore Management University"
      }
    ],
    "type": "tutorial",
    "id": "1125"
  },
  "tutorial1179": {
    "title": "Spatial Crowdsourcing: Challenges, Techniques, and Applications",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p1988-tong.pdf",
    "abstract": "Crowdsourcing is a new computing paradigm where humans are actively enrolled to participate in the procedure of computing, especially for tasks that are intrinsically easier for humans than for computers. The popularity of mobile computing and sharing economy has extended conventional web- based crowdsourcing to spatial crowdsourcing (SC), where spatial data such as location, mobility and the associated contextual information, plays a central role. In fact, spatial crowdsourcing has stimulated a series of recent industrial successes including Citizen Sensing (Waze), P2P ride- sharing (Uber) and Real-time Online-To-Offline (O2O) services (Instacart and Postmates). In this tutorial, we review the paradigm shift from web-based crowdsourcing to spatial crowdsourcing. We dive deep into the challenges and techniques brought by the unique spatio-temporal characteristics of spatial crowdsourcing. Particularly, we survey new designs in task assignment, quality control, incentive mechanism design and privacy protection on spatial crowdsourcing platforms, as well as the new trend to incorporate crowdsourcing to enhance existing spatial data processing techniques. We also discuss case studies of representative spatial crowdsourcing systems and raise open questions and current challenges for the audience to easily comprehend the tutorial and to advance this important research area.",
    "subtype": "tutorial",
    "authors": [
      {
        "name": "Yongxin Tong",
        "affiliation": "Beihang University"
      },
      {
        "name": "Lei Chen",
        "affiliation": "HKUST"
      },
      {
        "name": "Cyrus Shahabi",
        "affiliation": "USC"
      }
    ],
    "type": "tutorial",
    "id": "1179"
  },
  "tutorial1198": {
    "title": "The Era of Big Spatial Data",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p1992-eldawy.pdf",
    "abstract": "In this tutorial, we present the recent work in the database community for handling Big Spatial Data. This topic became very hot due to the recent explosion in the amount of spatial data generated by smart phones, satellites and medical devices, among others. This tutorial goes beyond the use of existing systems as-is (e.g., Hadoop, Spark or Impala), and digs deep into the core components of big systems (e.g., indexing and query processing) to describe how they are designed to handle big spatial data. During this 90-minute tutorial, we review the state-of-the-art work in the area of Big Spatial Data while classifying the existing research efforts according to the implementation approach, underlying architecture, and system components. In addition, we provide case studies of full-fledged systems and applications that handle Big Spatial Data which allows the audience to better comprehend the whole tutorial.",
    "subtype": "tutorial",
    "authors": [
      {
        "name": "Ahmed Eldawy",
        "affiliation": "UC Riverside"
      },
      {
        "name": "Mohamed Mokbel",
        "affiliation": "University of Minnesota"
      }
    ],
    "type": "tutorial",
    "id": "1198"
  },
  "tutorial1271": {
    "title": "Complex Event Recognition in the Big Data Era",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p1996-giatrakos.pdf",
    "abstract": "The concept of event processing is established as a generic computational paradigm in various application fields, ranging from data processing in Web environments, over maritime and transport, to finance and medicine. Events report on state changes of a system and its environment. Complex Event Recognition (CER) in turn, refers to the identification of complex/composite events of interest, which are collections of simple events that satisfy some pattern, thereby providing the opportunity for reactive and proactive measures. Examples include the recognition of attacks in computer network nodes, human activities on video content, emerging stories and trends on the Social Web, traffic and transport incidents in smart cities, fraud in electronic marketplaces, cardiac arrhythmias, and epidemic spread. In each scenario, CER allows to make sense of Big event Data streams and react accordingly. The goal of this tutorial is to provide a step-by-step guide for realizing CER in the Big Data era. To do so, it elaborates on major challenges and describes algorithmic toolkits for optimized manipulation of event streams characterized by high volume, velocity and/or lack of veracity, placing emphasis on distributed CER over potentially heterogeneous (data variety) event sources. Finally, we highlight future research directions in the field.",
    "subtype": "tutorial",
    "authors": [
      {
        "name": "Nikos Giatrakos",
        "affiliation": "Technical University of Crete"
      },
      {
        "name": "Alexander Artikis",
        "affiliation": "NCSR Demokritos"
      },
      {
        "name": "Antonios Deligiannakis",
        "affiliation": "Technical University of Crete"
      },
      {
        "name": "Minos Garofalakis",
        "affiliation": "Technical University of Crete"
      }
    ],
    "type": "tutorial",
    "id": "1271"
  },
  "tutorial1272": {
    "title": "Blockchains and Databases",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p2000-mohan.pdf",
    "abstract": "In the last few years, blockchain (also known as distributed ledger), the underlying technology of the permissionless or public Bitcoin network, has become very popular for use in private or permissioned environments. Computer companies like IBM and Microsoft, and many key players in different vertical industry segments have recognized the utility of blockchains for securely managing assets (physical/digital) other than cryptocurrencies. IBM did some pioneering work by architecting and implementing a private blockchain system, and then open sourcing it. That system, which has since then been named Fabric, is being enhanced via the Hyperledger Consortium set up under the auspices of the Linux Foundation. Other efforts in the industry include Enterprise Ethereum, R3 Corda and BigchainDB. While currently there is no standard in the blockchain space, all the ongoing efforts involve some combination of database, transaction, encryption, virtualization, consensus and other distributed systems technologies. Some of the application areas in which blockchain pilots are being carried out are: smart contracts, supply chain management, Know Your Customer (KYC), derivatives processing and provenance management. A couple of production deployments are also in place now. In this tutorial, I survey some of the ongoing projects with respect to their architectures in general and their approaches to some specific technical areas. Specifically, I focus on how the functionality of traditional and modern data stores are being utilized or not utilized in the different blockchain projects. Because of the attention the world is paying to blockchain technologies, it is important for the database community to become more aware of the underlying technologies and other developments in this area. Then, the community could try to influence the approaches taken and, in particular, how database technologies could be better utilized or enhanced for blockchains. Since most of the blockchain efforts are still in a nascent state, the time is right for database researchers and practitioners to get more deeply involved!",
    "subtype": "tutorial",
    "authors": [
      {
        "name": "C. Mohan",
        "affiliation": "IBM Almaden Research Center"
      }
    ],
    "type": "tutorial",
    "id": "1272"
  },
  "tutorial1276": {
    "title": "Caching at the Web Scale",
    "acm_link": "http://www.vldb.org/pvldb/vol10/p2002-zakhary.pdf",
    "abstract": "Today’s web applications and social networks are serving billions of users around the globe. These users generate billions of key lookups and millions of data object updates per second. A single user’s social network page load requires hundreds of key lookups. This scale creates many design challenges for the underlying storage systems. First, these systems have to serve user requests with low latency. Any increase in the request latency leads to a decrease in user interest. Second, storage systems have to be highly available. Failures should be handled seamlessly without affecting user requests. Third, users consume an order of magnitude more data than they produce. Therefore, storage systems have to be optimized for read-intensive workloads. To address these challenges, distributed in-memory caching services have been widely deployed on top of persistent storage. In this tutorial, we survey the recent developments in distributed caching services. We present the algorithmic and architectural efforts behind these systems focusing on the challenges in addition to open research questions.",
    "subtype": "tutorial",
    "authors": [
      {
        "name": "Victor Zakhary",
        "affiliation": "UCSB"
      },
      {
        "name": "Amr El Abbadi",
        "affiliation": "UCSB"
      },
      {
        "name": "Divyakant Agarwal",
        "affiliation": "UCSB"
      }
    ],
    "type": "tutorial",
    "id": "1276"
  },
  "VLDB-D-16-00062R2": { "title": "Disjoint Interval Partitioning", "subtype": "VLDB Journal Poster", "type": "VLDB Journal Poster", "acm_link": "https://link.springer.com/article/10.1007/s00778-017-0456-7",
    "abstract": "In databases with time interval attributes, query processing techniques that are based on sort-merge or sort-aggregate deteriorate. This happens because for intervals no total order exists and either the start or end point is used for the sorting. Doing so leads to inefficient solutions with lots of unproductive comparisons that do not produce an output tuple. Even if just one tuple with a long interval is present in the data, the number of unproductive comparisons of sort-merge and sort-aggregate gets quadratic. In this paper we propose disjoint interval partitioning (DIP), a technique to efficiently perform sort-based operators on interval data. DIP divides an input relation into the minimum number of partitions, such that all tuples in a partition are non-overlapping. The absence of overlapping tuples guarantees efficient sort-merge computations without backtracking. With DIP the number of unproductive comparisons is linear in the number of partitions. In contrast to current solutions with inefficient random accesses to the active tuples, DIP fetches the tuples in a partition sequentially. We illustrate the generality and efficiency of DIP by describing and evaluating three basic database operators over interval data: join, anti-join and aggregation.",
    "authors": [ 
      { "name": "Francesco Cafagna", "affiliation": "University of Zurich" },
      { "name": "Michael Boehlen", "affiliation": "University of Zurich" } ] },
  "VLDB-D-16-00027R2": { "title": "AutoG: A Visual Query Autocompletion Framework for Graph Databases", "subtype": "VLDB Journal Poster", "type": "VLDB Journal Poster", "acm_link": "https://link.springer.com/article/10.1007/s00778-017-0454-9",
    "abstract": "Composing queries is evidently a tedious task. This is particularly true of graph queries as they are typically complex and prone to errors, compounded by the fact that graph schemas can be missing or too loose to be helpful for query formulation. Despite the great success of query formulation aids, in particular, automatic query completion, graph query autocompletion has received much less research attention. In this paper, we propose a novel framework for subgraph query autocompletion (called AutoG). Given an initial query q and a user.s preference as input, AutoG returns ranked query suggestions  Q.  as output. Users may choose a query from  Q.  and iteratively apply AutoG to compose their queries. The novelties of AutoG are as follows: First, we formalize query composition. Second, we propose to increment a query with the logical units called c-prime features that are (i) frequent subgraphs and (ii) constructed from smaller c-prime features in no more than c ways. Third, we propose algorithms to rank candidate suggestions. Fourth, we propose a novel index called feature Dag (FDag) to optimize the ranking. We study the query suggestion quality with simulations and real users and conduct an extensive performance evaluation. The results show that the query suggestions are useful (saved roughly 40% of users. mouse clicks), and AutoG returns suggestions shortly under a large variety of parameter settings.",
    "authors": [ 
      { "name": "Peipei Yi", "affiliation": "Hong Kong Baptist University" },
      { "name": "Byron Choi", "affiliation": "Hong Kong Baptist University" },
      { "name": "Sourav S. Bhowmick", "affiliation": "Hong Kong Baptist University" },
      { "name": "Jianliang Xu", "affiliation": "Hong Kong Baptist University" } ] },
  "VLDB-D-14-00061R2": { "title": "Avoiding class warfare: Managing Continuous Queries with Differentiated Classes of Service", "subtype": "VLDB Journal Poster", "type": "VLDB Journal Poster", "acm_link": "https://link.springer.com/article/10.1007/s00778-015-0411-4",
    "abstract": "Data stream management systems (DSMSs) offer the most effective solution for processing data streams by efficiently executing continuous queries (CQs) over the incoming data. CQs inherently have different levels of criticality and hence different levels of expected quality of service (QoS) and quality of data (QoD). Adhering to such expected QoS/QoD metrics is even more important in cases of multi-tenant data stream management services. In this work, we propose DILoS, a framework that, through priority-based scheduling and load shedding, supports differentiated QoS and QoD for multiple classes of CQs. Unlike existing works that consider scheduling and load shedding separately, DILoS is a novel unified framework that exploits the synergy between scheduling and load shedding. We also propose ALoMa, a general, adaptive load manager that DILoS is built upon. By its design, ALoMa performs better than the state-of-the-art alternatives in three dimensions: (1) it automatically tunes the headroom factor, (2) it honors the delay target, (3) it is applicable to complex query networks with shared operators. We implemented DILoS and ALoMa in our real DSMS prototype system (AQSIOS) and evaluate their performance for a variety of real and synthetic workloads. Our experimental evaluation of ALoMa verified its clear superiority over the state-of-the-art approaches. Our experimental evaluation of the DILoS framework showed that it (a) allows the scheduler and load shedder to consistently honor CQs. priorities, (b) significantly increases system capacity utilization by exploiting batch processing, and (c) enables operator sharing among query classes of different priorities while avoiding priority inversion, i.e., a lower-priority class never blocks a higher-priority one.",
    "authors": [ 
      { "name": "Thao N. Pham", "affiliation": "University of Pittsburg" },
      { "name": "Panos K. Chrysanthis", "affiliation": "University of Pittsburg" },
      { "name": "Alexandros Labrinidis", "affiliation": "University of Pittsburg" } ] },
  "VLDB-D-16-00031R1": { "title": "PANDA: Towards Partial Topology-based Search on Large Networks in a Single Machine", "subtype": "VLDB Journal Poster", "type": "VLDB Journal Poster", "acm_link": "https://link.springer.com/article/10.1007/s00778-016-0447-0",
    "abstract": "A large body of research has focused on efficient and scalable processing of subgraph search queries on large networks. In these efforts, a query is posed in the form of a connected query graph. Unfortunately, in practice end users may not always have precise knowledge about the topological relationships between nodes in a query graph to formulate a connected query. In this paper, we present a novel graph querying paradigm called partial topology-based network search and propose a query processing framework called panda to efficiently process partial topology query (ptq) in a single machine. A ptq is a disconnected query graph containing multiple connected query components. ptqs allow an end user to formulate queries without demanding precise information about the complete topology of a query graph. To this end, we propose an exact and an approximate algorithm called sen-panda and po-panda, respectively, to generate top-kmatches of a ptq. We also present a subgraph simulation-based optimization technique to further speedup the processing of ptqs. Using real-life networks with millions of nodes, we experimentally verify that our proposed algorithms are superior to several baseline techniques.",
    "authors": [ 
      { "name": "Miao Xie", "affiliation": "Nanyang Technological University and Chinese Academy of Sciences and Huawei" },
      { "name": "Sourav S. Bhowmick", "affiliation": "Nanyang Technological University" },
      { "name": "Gao Cong", "affiliation": "Nanyang Technological University" },
      { "name": "Qing Wang", "affiliation": "Chinese Academy of Sciences" } ] },
  "VLDB-D-16-00044R2": { "title": "Geo-Social Group Queries with Minimum Acquaintance Constraints", "subtype": "VLDB Journal Poster", "type": "VLDB Journal Poster", "acm_link": "https://link.springer.com/article/10.1007/s00778-017-0473-6",
    "abstract": "The prosperity of location-based social networking has paved the way for new applications of group-based activity planning and marketing. While such applications heavily rely on geo-social group queries (GSGQs), existing studies fail to produce a cohesive group in terms of user acquaintance. In this paper, we propose a new family of GSGQs with minimum acquaintance constraints. They are more appealing to users as they guarantee a worst-case acquaintance level in the result group. For efficient processing of GSGQs on large location-based social networks, we devise two social-aware spatial index structures, namely SaR-tree and SaR*-tree. The latter improves on the former by considering both spatial and social distances when clustering objects. Based on SaR-tree and SaR*-tree, novel algorithms are developed to process various GSGQs. Extensive experiments on real datasets Gowalla and Twitter show that our proposed methods substantially outperform the baseline algorithms under various system settings.",
    "authors": [ 
      { "name": "Qijun Zhu", "affiliation": "Hong Kong Baptist University" },
      { "name": "Haibo Hu", "affiliation": "Hong Kong Polytechnic University" },
      { "name": "Cheng Xu", "affiliation": "Hong Kong Baptist University" },
      { "name": "Jianliang Xu", "affiliation": "Hong Kong Baptist University" },
      { "name": "Wang-Chien Lee", "affiliation": "Pennsylvania State University" } ] },
  "VLDB-D-16-00059R2": { "title": "Argument Discovery via Crowdsourcing", "subtype": "VLDB Journal Poster", "type": "VLDB Journal Poster", "acm_link": "https://link.springer.com/article/10.1007/s00778-017-0462-9",
    "abstract": "The amount of controversial issues being discussed on the Web has been growing dramatically. In articles, blogs, and wikis, people express their points of view in the form of arguments, i.e., claims that are supported by evidence. Discovery of arguments has a large potential for informing decision-making. However, argument discovery is hindered by the sheer amount of available Web data and its unstructured, free-text representation. The former calls for automatic text-mining approaches, whereas the latter implies a need for manual processing to extract the structure of arguments. In this paper, we propose a crowdsourcing-based approach to build a corpus of arguments, an argumentation base, thereby mediating the trade-off of automatic text-mining and manual processing in argument discovery. We develop an end-to-end process that minimizes the crowd cost while maximizing the quality of crowd answers by: (1) ranking argumentative texts, (2) pro-actively eliciting user input to extract arguments from these texts, and (3) aggregating heterogeneous crowd answers. Our experiments with real-world datasets highlight that our method discovers virtually all arguments in documents when processing only 25% of the text with more than 80% precision, using only 50% of the budget consumed by a baseline algorithm.",
    "authors": [ 
      { "name": "Quoc Viet Hung Nguyen", "affiliation": "University of Queensland" },
      { "name": "Chi Thang Duong", "affiliation": "EPFL" },
      { "name": "Thanh Tam Nguyen", "affiliation": "EPFL" },
      { "name": "Matthias Weidlich", "affiliation": "Humboldt-Universität zu Berlin" },
      { "name": "Karl Aberer", "affiliation": "EPFL" },
      { "name": "Hongzhi Yin", "affiliation": "University of Queensland" },
      { "name": "Xiaofang Zhou", "affiliation": "University of Queensland and Macau University of Science and Technology" } ] },
  "FADS": { "title": "FADS", "subtype": "workshop", "type": "workshop", "acm_link": "http://fads.ws",
    "abstract": "Failed Aspirations in Database Systems",
    "authors": [ 
      { "name": "Spyros Blanas", "affiliation": "Ohio State University" },
      { "name": "Justin Lewandoski", "affiliation": "Microsoft Research" },
      { "name": "Andy Pavlo", "affiliation": "CMU" } ] },
  "BIRTE": { "title": "BIRTE", "subtype": "workshop", "type": "workshop", "acm_link": " http://db.cs.pitt.edu/birte2017",
    "abstract": "Eleventh International Workshop on Real-Time Business Intelligence and Analytics",
    "authors": [ 
      { "name": "Malu Castellanos", "affiliation": "Teradata Aster" },
      { "name": "Panos K Chrysanthis", "affiliation": "University of Pittsburgh" } ] },
  "TPCTC": { "title": "TPCTC", "subtype": "workshop", "type": "workshop", "acm_link": "http://www.tpc.org/tpctc/tpctc2017/default.asp",
    "abstract": "Ninth TPC Technology Conference on Performance Evaluation & Benchmarking",
    "authors": [ 
      { "name": "Raghunath Niambar", "affiliation": "Cisco" },
      { "name": "Meikel Poess", "affiliation": "Oracle" } ] },
  "VLIoT": { "title": "VLIoT", "subtype": "workshop", "type": "workshop", "acm_link": "https://www.ifis.uni-luebeck.de/~groppe/vliot/2017",
    "abstract": "The International Workshop on Very Large Internet of Things",
    "authors": [ 
      { "name": "Sven Groppe", "affiliation": "University of Lübeck" },
      { "name": "Carlo Alberto Boano", "affiliation": "Graz University of Technology" } ] },
  "PhD Workshop": { "title": "PhD Workshop", "subtype": "workshop", "type": "workshop", "acm_link": "http://vldb.org/2017/cp_phd_workshop.php",
    "abstract": "VLDB PhD Workshop",
    "authors": [ 
      { "name": "Peter Christen", "affiliation": "The Australian National University" },
      { "name": "Bettina Kemme", "affiliation": "McGill University" },
      { "name": "Erhard Rahm", "affiliation": "University of Leipzig" } ] },
  "MATES": { "title": "MATES", "subtype": "workshop", "type": "workshop", "acm_link": " http://ai-group.ds.unipi.gr/mates17/",
    "abstract": "Workshop on Mobility Analytics for Spatio-temporal and Social Data",
    "authors": [ 
      { "name": "Christos Doulkeridis", "affiliation": "University of Piraeus" },
      { "name": "Qiang Qu", "affiliation": "Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences" } ] },
  "ADMS": { "title": "ADMS", "subtype": "workshop", "type": "workshop", "acm_link": "http://www.adms-conf.org/",
    "abstract": "Eight International Workshop on Accelerating Analytics and Data Management Systems Using Modern Processor and Storage Architectures",
    "authors": [ 
      { "name": "Rajesh Bordawekar", "affiliation": "IBM Watson" },
      { "name": "Tirthankar Lahiri", "affiliation": "Oracle" } ] },
  "DMAH": { "title": "DMAH", "subtype": "workshop", "type": "workshop", "acm_link": "http://dmah.info",
    "abstract": "The Third International Workshop on Data Management and Analytics for Medicine and Healthcare",
    "authors": [ 
      { "name": "Fusheng Wang", "affiliation": "Stony Brook University" },
      { "name": "Gang Luo", "affiliation": "University of Washington" },
      { "name": "Edmon Begoli", "affiliation": "Oak Ridge National Laboratory" } ] },
  "DBPL": { "title": "DBLP", "subtype": "workshop", "type": "workshop", "acm_link": "http://dbpl2017.org/",
    "abstract": "The Sixteenth International Symposium on Database Programming Languages",
    "authors": [ 
      { "name": "Tiark Rompf", "affiliation": "Purdue University" },
      { "name": "Alexander Alexandrov", "affiliation": "TU Berlin" } ] },
  "BOSS": { "title": "BOSS", "subtype": "workshop", "type": "workshop", "acm_link": "http://boss.dima.tu-berlin.de/",
    "abstract": "Third Workshop on Big Data Open Source Systems",
    "authors": [ 
      { "name": "Tyson Condie", "affiliation": "UCLA" },
      { "name": "Tilmann Rabl", "affiliation": "TU Berlin" } ] },
  "VLDB Octoberfest": { "title": "VLDB Octoberfest", "abstract": "Conference dinner in a typical Bavarian setting.", "subtype": "social event", "type": "social event" },
  "Welcome Reception": { "title": "Welcome Reception kindly supported by SAP", "abstract": "Evening drinks and snacks in a historic setting, to kick off VLDB2017.", "subtype": "social event", "type": "social event" },
  "Posters": { "title": "Poster Reception", "abstract": "Poster presentations for all papers that were presented that same day, accompanied by some drinks. Additionally, two VLDB Journal papers will present a poster.", "subtype": "interactive session", "type": "interactive session" },
  "empty": { "title": "empty talk slot", "abstract": "The Audimax needs to be unoccupied to allow setting up the Foyer for the poster reception.", "subtype": "break", "type": "break" },
  "unused": { "title": "empty talk slot", "abstract": " ", "subtype": "break", "type": "break" },
  "moved": { "title": "empty talk slot", "abstract": "Because its authors might not have been able to travel back from VLDB2017 due to the recent USA travel restrictions, the paper that was previously in this slot will be presented at SIGMOD2018.", "subtype": "break", "type": "break" },
  "coffee": { "title": "coffee", "abstract": " ", "subtype": "break", "type": "break" },
  "lunch": { "title": "lunch", "abstract": " ", "subtype": "break", "type": "break" }
}
