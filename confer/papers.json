entities={
"SIGMOD_Research_033": {
  "title": "Optimizing Declarative Graph Queries at Large Scale",
  "abstract": "This paper presents GraphRex, an efficient, robust, scalable, and easy-to-program framework for graph processing on datacenter infrastructure. To users, GraphRex presents a declarative, Datalog-like interface that is natural and expressive. Underneath, it compiles those queries into efficient implementations. A key technical contribution of GraphRex is the identification and optimization of a set of global operators whose efficiency is crucial to the good performance of datacenter-based, large graph analysis. Our experimental results show that GraphRex significantly outperforms existing frameworks-both high- and low-level-in scenarios ranging across a wide variety of graph workloads and network conditions, sometimes by two orders of magnitude.",
  "subtype": "SIGMOD Research",
  "authors": [
    {
      "name": "Qizhen Zhang",
      "affiliation": "University of Pennsylvania"
    },
    {
      "name": "Akash Acharya",
      "affiliation": "University of Pennsylvania"
    },
    {
      "name": "Hongzhi Chen",
      "affiliation": "The Chinese University of Hong Kong"
    },
    {
      "name": "Simran Arora",
      "affiliation": "University of Pennsylvania"
    },
    {
      "name": "Ang Chen",
      "affiliation": "Rice University"
    },
    {
      "name": "Vincent Liu",
      "affiliation": "University of Pennsylvania"
    },
    {
      "name": "Boon Loo",
      "affiliation": "University of Pennsylvania"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Research_033"
},
"SIGMOD_Research_034": {
  "title": "JOSIE: Overlap Set Similarity Search for Finding Joinable Tables in Data Lakes",
  "abstract": "We present a new solution for finding joinable tables in massive data lakes: given a table and one join column, find tables that can be joined with the given table on the largest number of distinct values. The problem can be formulated as an overlap set similarity search problem by considering columns as sets and matching values as intersection between sets. Although set similarity search is well-studied in the field of approximate string search (e.g., fuzzy keyword search), the solutions are designed for and evaluated over sets of relatively small size (average set size rarely much over 100 and maximum set size in the low thousands) with modest dictionary sizes (the total number of distinct values in all sets is only a few million). We observe that modern data lakes typically have massive set sizes (with maximum set sizes that may be tens of millions) and dictionaries that include hundreds of millions of distinct values. Our new algorithm, JOSIE (Joining Search using Intersection Estimation) minimizes the cost of set reads and inverted index probes used in finding the top-k sets. We show that JOSIE completely out performs the state-of-the-art overlap set similarity search techniques on data lakes. More surprising, we also consider state-of-the-art approximate algorithm and show that our new exact search algorithm performs almost as well, and even in some cases better, on real data lakes.",
  "subtype": "SIGMOD Research",
  "authors": [
    {
      "name": "Erkang Zhu",
      "affiliation": "University of Toronto"
    },
    {
      "name": "Dong Deng",
      "affiliation": "Inception Institute of Artificial Intelligence"
    },
    {
      "name": "Fatemeh Nargesian",
      "affiliation": "University of Toronto"
    },
    {
      "name": "Renée Miller",
      "affiliation": "Northeastern University"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Research_034"
},
"SIGMOD_Research_050": {
  "title": "Going Beyond Provenance: Explaining Query Answers with Pattern-based Counterbalances",
  "abstract": "Provenance and intervention-based techniques have been used to explain surprisingly high or low outcomes of aggregation queries. However, such techniques may miss interesting explanations emerging from data that is not in the provenance. For instance, an unusually low number of publications of a prolific researcher in a certain venue and year can be explained by an increased number of publications in another venue in the same year. We present a novel approach for explaining outliers in aggregation queries through counter- balancing. That is, explanations are outliers in the opposite direction of the outlier of interest. Outliers are defined w.r.t. patterns that hold over the data in aggregate. We present efficient methods for mining such aggregate regression pat- terns (ARPs), discuss how to use ARPs to generate and rank explanations, and experimentally demonstrate the efficiency and effectiveness of our approach.",
  "subtype": "SIGMOD Research",
  "authors": [
    {
      "name": "Zhengjie Miao",
      "affiliation": "Duke University"
    },
    {
      "name": "Qitian Zeng",
      "affiliation": "Illinois Institute of Technology"
    },
    {
      "name": "Boris Glavic",
      "affiliation": "Illinois Institute of Technology"
    },
    {
      "name": "Sudeepa Roy",
      "affiliation": "Duke University"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Research_050"
},
"SIGMOD_Research_052": {
  "title": "BriskStream: Scaling Data Stream Processing on Shared-Memory Multicore Architectures",
  "abstract": "We introduce BriskStream, an in-memory data stream processing system (DSPSs) specifically designed for modern shared-memory multicore architectures. BriskStream’s key contribution is an execution plan optimization paradigm, namely RLAS, which takes relative-location (i.e., NUMA distance) of each pair of producer-consumer operators into consideration. We propose a branch and bound based approach with three heuristics to resolve the resulting nontrivial optimization problem. The experimental evaluations demonstrate that BriskStream yields much higher throughput and better scalability than existing DSPSs on multi-core architectures when processing different types of workloads.",
  "subtype": "SIGMOD Research",
  "authors": [
    {
      "name": "Shuhao Zhang",
      "affiliation": "National University of Singapore"
    },
    {
      "name": "Jiong He",
      "affiliation": "Advanced Digital Sciences Center"
    },
    {
      "name": "Amelie Zhou",
      "affiliation": "Shenzhen University"
    },
    {
      "name": "Bingsheng He",
      "affiliation": "National University of Singapore"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Research_052"
},
"SIGMOD_Research_056": {
  "title": "Strongly Truthful Interactive Regret Minimization",
  "abstract": "When faced with a database containing millions of tuples, an end user might be only interested in finding his/her (close to) favorite tuple in the database. Recently, a regret minimization query was proposed to obtain a small subset from the database that fits the user's needs, which are expressed through an unknown utility function. Specifically, it minimizes the 'regret' level of a user, which we quantify as the regret ratio if s/he gets the best tuple in the selected subset but not the best tuple among all tuples in the database. We study how to enhance the regret minimization query with user interactions: when presented with a small number of tuples (which can be artificial tuples or true tuples inside the database), a user is asked to indicate the tuple s/he favors the most among them. In particular, we are also interested in the special case of determining the favorite tuple for a user in the entire database with a small amount of interaction, measured by the number of questions we ask the user. Different from the previous work which displays artificial tuples to users, we achieve a stronger result in this paper by always displaying true tuples in the database. Specifically, we present a generic framework for interactive regret minimization, under which we propose algorithms that ask an asymptotically optimal number of questions in 2-dimensional spaces and algorithms with provable performance guarantees in d-dimensional spaces (d geq 2) where each dimension corresponds to a description of a tuple. Experiments on real and synthetic datasets showed that our algorithms outperform the existing one by locating the favorite tuple and guaranteeing a small regret ratiowith much fewer questions.",
  "subtype": "SIGMOD Research",
  "authors": [
    {
      "name": "Min Xie",
      "affiliation": "Hong Kong University of Science and Technology"
    },
    {
      "name": "Raymond Chi-Wing Wong",
      "affiliation": "Hong Kong University of Science and Technology"
    },
    {
      "name": "Ashwin Lall",
      "affiliation": "Denison University"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Research_056"
},
"SIGMOD_Research_057": {
  "title": "Fast General Distributed Transactions with Opacity",
  "abstract": "Transactions can simplify distributed applications by hiding data distribution, concurrency, and failures from the application developer. Ideally the developer would see the abstraction of a single large machine that runs transactions sequentially and never fails. This requires the transactional subsystem to provide opacity (strict serializability for both committed and aborted transactions), as well as transparent fault tolerance with high availability. As even the best abstractions are unlikely to be used if they perform poorly, the system must also provide high performance. Existing distributed transactional designs either weaken this abstraction or are not designed for the best performance within a data center. This paper extends the design of FaRM - which provides strict serializability only for committed transactions - to provide opacity while maintaining FaRM's high throughput, low latency, and high availability within a modern data center. It uses timestamp ordering based on real time with clocks synchronized to within tens of microseconds across a cluster, and a failover protocol to ensure correctness across clock master failures. FaRM with opacity can commit 5.4 million neworder transactions per second when running the TPC-C transaction mix on 90 machines with 3-way replication.",
  "subtype": "SIGMOD Research",
  "authors": [
    {
      "name": "Alex Shamis",
      "affiliation": "Microsoft Research"
    },
    {
      "name": "Matthew Renzelmann",
      "affiliation": "Microsoft"
    },
    {
      "name": "Stanko Novakovic",
      "affiliation": "VMware"
    },
    {
      "name": "Georgios Chatzopoulos",
      "affiliation": "EPFL"
    },
    {
      "name": "Aleksandar Dragojevi&#263;",
      "affiliation": "Microsoft Research"
    },
    {
      "name": "Dushyanth Narayanan",
      "affiliation": "Microsoft Research"
    },
    {
      "name": "Miguel Castro",
      "affiliation": "Microsoft Research"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Research_057"
},
"SIGMOD_Research_072": {
  "title": "Tuple-oriented Compression for Large-scale Mini-batch Stochastic Gradient Descent",
  "abstract": "Data compression is a popular technique for improving the efficiency of data processing workloads such as SQL queries and more recently, machine learning (ML) with classical batch gradient methods. But the efficacy of such ideas for mini-batch stochastic gradient descent (MGD), arguably the workhorse algorithm of modern ML, is an open question. MGD's unique data access pattern renders prior art, including those designed for batch gradient methods, less effective. We fill this crucial research gap by proposing a new lossless compression scheme we call tuple-oriented compression (TOC) that is inspired by an unlikely source, the string/ text compression scheme Lempel-Ziv-Welch, but tailored to MGD in a way that preserves tuple boundaries within mini-batches. We then present a suite of novel compressed matrix operation execution techniques tailored to the TOC compression scheme that operate directly over the compressed data representation and avoid decompression overheads. An extensive empirical evaluation with real-world datasets shows that TOC consistently achieves substantial compression ratios by up to 51x and reduces runtimes for MGD workloads by up to 10.2x in popular ML systems.",
  "subtype": "SIGMOD Research",
  "authors": [
    {
      "name": "Fengan Li",
      "affiliation": "University of Wisconsin, Madison"
    },
    {
      "name": "Lingjiao Chen",
      "affiliation": "University of Wisconsin, Madison"
    },
    {
      "name": "Yijing Zeng",
      "affiliation": "University of Wisconsin, Madison"
    },
    {
      "name": "Arun Kumar",
      "affiliation": "University of California, San Diego"
    },
    {
      "name": "Xi Wu",
      "affiliation": "University of Wisconsin, Madison"
    },
    {
      "name": "Jeffrey Naughton",
      "affiliation": "University of Wisconsin, Madison"
    },
    {
      "name": "Jignesh Patel",
      "affiliation": "University of Wisconsin, Madison"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Research_072"
},
"SIGMOD_Research_073": {
  "title": "Border-Collie: A Wait-free, Read-optimal Algorithm for Database Logging on Multicore Hardware",
  "abstract": "Actions changing the state of databases are all logged with proper ordering being imposed. Database engines obeying this golden rule of logging enforce total ordering on all events, and this poses challenges in addressing the scalability bottlenecks of database logging on multicore hardware. We reexamined the problem of database logging and realized that in any given log history, obtaining an upper bound on the size of a set that preserves the happen-before relation is the essence of the matter. Based on our understanding, we propose Border-Collie, a wait-free and read-optimal algorithm for database logging that finds such an upper bound even with some worker threads often being idle. We show that (1) Border-Collie always finds the largest set of logged events satisfying the condition in a finite number of steps (i.e., wait-free), (2) the number of logged events to be read is also minimal (i.e., read-optimal), and (3) both properties hold even with threads being in intermittent work. Experimental results demonstrated that Border-Collie proves our claims under various workloads; Border-Collie outperforms the state-of-the-art centralized logging techniques (i.e., Eleda and ERMIA) by up to ~2X and exhibits almost the same throughput with much shorter commit latency than the state-of-the-art decentralized logging techniques (i.e., Silo and FOEDUS).",
  "subtype": "SIGMOD Research",
  "authors": [
    {
      "name": "Jongbin Kim",
      "affiliation": "Hanyang University"
    },
    {
      "name": "Hyeongwon Jang",
      "affiliation": "Hanyang University"
    },
    {
      "name": "Seohui Son",
      "affiliation": "Hanyang University"
    },
    {
      "name": "Hyuck Han",
      "affiliation": "Dongduk Women's University"
    },
    {
      "name": "Sooyong Kang",
      "affiliation": "Hanyang University"
    },
    {
      "name": "Hyungsoo Jung",
      "affiliation": "Hanyang University"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Research_073"
},
"SIGMOD_Research_110": {
  "title": "CATAPULT: Data-driven Selection of Canned Patterns for Efficient Visual Graph Query Formulation",
  "abstract": "Visual graph query interfaces (a.k.a textscgui) widen the reach of graph querying frameworks across different users by enabling non-programmers to use them. Consequently, several commercial and academic frameworks for querying a large collection of small- or medium-sized data graphs (e.g., chemical compounds) provide such visual interfaces. Majority of these interfaces expose a fixed set of canned patterns (i.e., small subgraph patterns) to expedite query formulation by enabling pattern-at-a-time in lieu of edge-at-a-time construction mode. Canned patterns to be displayed on a textscgui are typically selected manually based on domain knowledge. However, manual generation of canned patterns is labour intensive. Furthermore, these patterns may not sufficiently cover the underlying data graphs to expedite visual formulation of a wide range of subgraph queries. In this paper, we present a generic and extensible framework called textscCatapult to address these limitations. textscCatapult takes a data-driven approach to automatically select canned patterns, thereby taking a concrete step towards the vision of data-driven construction of visual query interfaces. Specifically, it first clusters the underlying data graphs based on their topological similarities and then summarize each cluster to create a cluster summary graph (textsccsg). The canned patterns within a user-specified pattern budget are then generated from these textsccsgs by maximizing coverage and diversity, and minimizing cognitive load of the patterns. Experimental study with real-world datasets and visual graph interfaces demonstrates the superiority of textscCatapult compared to traditional techniques.",
  "subtype": "SIGMOD Research",
  "authors": [
    {
      "name": "Kai Huang",
      "affiliation": "Fudan University"
    },
    {
      "name": "Huey Chua",
      "affiliation": "Nanyang Technological University"
    },
    {
      "name": "Sourav Bhowmick",
      "affiliation": "Nanyang Technological University"
    },
    {
      "name": "Byron Choi",
      "affiliation": "Hong Kong Baptist University"
    },
    {
      "name": "Shuigeng Zhou",
      "affiliation": "Fudan University"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Research_110"
},
"SIGMOD_Research_118": {
  "title": "DeepBase: Deep Inspection of Neural Networks",
  "abstract": "Although deep learning models perform remarkably well across a range of tasks such as language translation and object recognition, it remains unclear what high-level logic, if any, they follow. Understanding this logic may lead to more transparency, better model design, and faster experimentation. Recent machine learning research has leveraged statistical methods to identify hidden units that behave (e.g., activate) similarly to human understandable logic, but those analyses require considerable manual effort. Our insight is that many of those studies follow a common analysis pattern, and therefore there is opportunity to provide a declarative abstraction to easily express, execute and optimize them. This paper describes DeepBase, a system to inspect neural network behaviors through a unified interface. We model logic with user-provided hypothesis functions that annotate the data with high-level labels (e.g., part-of-speech tags, image captions). DeepBase lets users quickly identify individual or groups of units that have strong statistical dependencies with desired hypotheses. We discuss how DeepBase can express existing analyses, propose a set of simple and effective optimizations to speed up a standard Python implementation by up to 72x, and reproduce recent studies from the NLP literature.",
  "subtype": "SIGMOD Research",
  "authors": [
    {
      "name": "Thibault Sellam",
      "affiliation": "Columbia University"
    },
    {
      "name": "Kevin Lin",
      "affiliation": "Columbia University"
    },
    {
      "name": "Ian Huang",
      "affiliation": "Columbia University"
    },
    {
      "name": "Michelle Yang",
      "affiliation": "University of California, Berkeley"
    },
    {
      "name": "Carl Vondrick",
      "affiliation": "Columbia University"
    },
    {
      "name": "Eugene Wu",
      "affiliation": "Columbia University"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Research_118"
},
"SIGMOD_Research_119": {
  "title": "Verifying Text Summaries of Relational Data Sets",
  "abstract": "We present a novel natural language query interface, the AggChecker, aimed at text summaries of relational data sets. The tool focuses on natural language claims that translate into an SQL query and a claimed query result. Similar in spirit to a spell checker, the AggChecker marks up text passages that seem to be inconsistent with the actual data. At the heart of the system is a probabilistic model that reasons about the input document in a holistic fashion. Based on claim keywords and the document structure, it maps each text claim to a probability distribution over associated query translations. By efficiently executing tens to hundreds of thousands of candidate translations for a typical input document, the system maps text claims to correctness probabilities. This process becomes practical via a specialized processing backend, avoiding redundant work via query merging and result caching. Verification is an interactive process in which users are shown tentative results, enabling them to take corrective actions if necessary. We tested our system on 53 publicly available articles containing 392 claims. Our tool revealed erroneous claims in roughly a third of test cases. Also, AggChecker compares favorably against several automated and semi-automated fact checking baselines.",
  "subtype": "SIGMOD Research",
  "authors": [
    {
      "name": "Saehan Jo",
      "affiliation": "Cornell University"
    },
    {
      "name": "Immanuel Trummer",
      "affiliation": "Cornell University"
    },
    {
      "name": "Weicheng Yu",
      "affiliation": "Cornell University"
    },
    {
      "name": "Xuezhi Wang",
      "affiliation": "Google Research"
    },
    {
      "name": "Cong Yu",
      "affiliation": "Google Research"
    },
    {
      "name": "Daniel Liu",
      "affiliation": "Cornell University"
    },
    {
      "name": "Niyati Mehta",
      "affiliation": "Cornell University"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Research_119"
},
"SIGMOD_Research_122": {
  "title": "Efficiently Searching In-Memory Sorted Arrays: Revenge of the Interpolation Search?",
  "abstract": "In this paper, we focus on the problem of searching sorted, in-memory datasets. This is a key data operation, and Binary Search is the de facto algorithm that is used in practice. We consider an alternative, namely Interpolation Search, which can take advantage of hardware trends by using complex calculations to save memory accesses. Historically, Interpolation Search was found to underperform compared to other search algorithms in this setting, despite its superior asymptotic complexity. Also, Interpolation Search is known to perform poorly on non-uniform data. To address these issues, we introduce SIP (Slope reuse Interpolation), an optimized implementation of Interpolation Search, and TIP (Three point Interpolation), a new search algorithm that uses linear fractions to interpolate on non-uniform distributions. We evaluate these two algorithms against a similarly optimized Binary Search method using a variety of real and synthetic datasets. We show that SIP is up to 4 times faster on uniformly distributed data and TIP is 2-3 times faster on non-uniformly distributed data in some cases. We also design a meta-algorithm to switch between these different methods to automate picking the higher performing search algorithm, which depends on factors like data distribution.",
  "subtype": "SIGMOD Research",
  "authors": [
    {
      "name": "Peter Van Sandt",
      "affiliation": "University of Wisconsin, Madison"
    },
    {
      "name": "Yannis Chronis",
      "affiliation": "University of Wisconsin, Madison"
    },
    {
      "name": "Jignesh Patel",
      "affiliation": "University of Wisconsin, Madison"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Research_122"
},
"SIGMOD_Research_128": {
  "title": "Experimental Analysis of Streaming Algorithms for Graph Partitioning",
  "abstract": "We report a systematic performance study of streaming graph partitioning algorithms. Graph partitioning plays a crucial role in overall system performance as it has a significant impact on both load balancing and inter-machine communication. The streaming model for graph partitioning has recently gained attention due to its ability to scale to very large graphs with limited resources.  The main objective of this study is to understand how the choice of graph partitioning algorithm affects system performance, resource usage and scalability. We focus on both offline graph analytics and online graph query workloads. The study considers both edge-cut and vertex-cut approaches. Our results show that the no partitioning algorithms performs best in all cases, and the choice of graph partitioning algorithm depends on: (i) type and degree distribution of the graph, (ii) characteristics of the workloads, and (iii) specific application requirements.",
  "subtype": "SIGMOD Research",
  "authors": [
    {
      "name": "Anil Pacaci",
      "affiliation": "University of Waterloo"
    },
    {
      "name": "Tamer Özsu",
      "affiliation": "University of Waterloo"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Research_128"
},
"SIGMOD_Research_131": {
  "title": "BlinkML: Efficient Maximum Likelihood Estimation with Probabilistic Guarantees",
  "abstract": "The rising volume of datasets has made training machine learning (ML) models a major computational cost in the enterprise. Given the iterative nature of model and parameter tuning, many analysts use a small sample of their entire data during their initial stage of analysis to make quick decisions (e.g., what features or hyperparameters to use) and use the entire dataset only in later stages (i.e., when they have converged to a specific model). This sampling, however, is performed in an ad-hoc fashion. Most practitioners cannot precisely capture the effect of sampling on the quality of their model, and eventually on their decision-making process during the tuning phase. Moreover, without systematic support for sampling operators, many optimizations and reuse opportunities are lost. In this paper, we introduce BlinkML, a system for fast, quality-guaranteed ML training. BlinkML allows users to make error-computation tradeoffs: instead of training a model on their full data (i.e., full model), BlinkML can quickly train an approximate model with quality guarantees using a sample. The quality guarantees ensure that, with high probability, the approximate model makes the same predictions as the full model. BlinkML currently supports any ML model that relies on maximum likelihood estimation (MLE), which includes Generalized Linear Models (e.g., linear regression, logistic regression, max entropy classifier, Poisson regression) as well as PPCA (Probabilistic Principal Component Analysis). Our experiments show that BlinkML can speed up the training of large-scale ML tasks by 6.26×–629× while guaranteeing the same predictions, with 95% probability, as the full model.",
  "subtype": "SIGMOD Research",
  "authors": [
    {
      "name": "Yongjoo Park",
      "affiliation": "University of Michigan"
    },
    {
      "name": "Jingyi Qing",
      "affiliation": "University of Michigan"
    },
    {
      "name": "Xiaoyang Shen",
      "affiliation": "University of Michigan"
    },
    {
      "name": "Barzan Mozafari",
      "affiliation": "University of Michigan"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Research_131"
},
"SIGMOD_Research_135": {
  "title": "Towards Model-based Pricing for Machine Learning in a Data Marketplace",
  "abstract": "Data analytics using machine learning (ML) has become ubiquitous in science, business intelligence, journalism and many other domains. While a lot of work focuses on reducing the training cost, inference runtime and storage cost of ML models, little work studies how to reduce the cost of data acquisition, which potentially leads to a loss of sellers' revenue and buyers' affordability and efficiency. In this paper, we propose a model-based pricing (MBP) framework, which instead of pricing the data, directly prices ML model instances. We first formally describe the desired properties of the MBP framework, with a focus on avoiding arbitrage. Next, we show a concrete realization of the MBP framework via a noise injection approach, which provably satisfies the desired formal properties. Based on the proposed framework, we then provide algorithmic solutions on how the seller can assign prices to models under different market scenarios (such as to maximize revenue). Finally, we conduct extensive experiments, which validate that the MBP framework can provide high revenue to the seller, high affordability to the buyer, and also operate on low runtime cost.",
  "subtype": "SIGMOD Research",
  "authors": [
    {
      "name": "Lingjiao Chen",
      "affiliation": "University of Wisconsin, Madison"
    },
    {
      "name": "Paraschos Koutris",
      "affiliation": "University of Wisconsin, Madison"
    },
    {
      "name": "Arun Kumar",
      "affiliation": "University of California, San Diego"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Research_135"
},
"SIGMOD_Research_147": {
  "title": "Designing Fair Ranking Schemes",
  "abstract": "Items from a database are often ranked based on a combination of criteria. The weight given to each criterion in the combination can greatly affect the fairness of the produced ranking, for example, preferring men over women. A user may have the flexibility to choose combinations that weigh these criteria differently, within limits.  In this paper, we develop a system that helps users choose criterion weights that lead to greater fairness. We consider ranking functions that compute the score of each item as a weighted sum of (numeric) attribute values, and then sort items on their score. Each ranking function can be expressed as a point in a multi-dimensional space. For a broad range of fairness criteria, including proportionality, we show how to efficiently identify regions in this space that satisfy these criteria. Using this identification method, our system is able to tell users whether their proposed ranking function satisfies the desired fairness criteria and, if it does not, to suggest the smallest modification that does. Our extensive experiments on real datasets demonstrate that our methods are able to find solutions that satisfy fairness criteria effectively (usually with only small changes to proposed weight vectors) and efficiently (in interactive time, after some initial pre-processing).",
  "subtype": "SIGMOD Research",
  "authors": [
    {
      "name": "Abolfazl Asudeh",
      "affiliation": "University of Michigan"
    },
    {
      "name": "H. V. Jagadish",
      "affiliation": "University of Michigan"
    },
    {
      "name": "Julia Stoyanovich",
      "affiliation": "New York University"
    },
    {
      "name": "Gautam Das",
      "affiliation": "University of Texas at Arlington"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Research_147"
},
"SIGMOD_Research_148": {
  "title": "RRR: Rank-Regret Representative",
  "abstract": "Selecting the best items in a dataset is a common task in data exploration. However, the concept of 'best' lies in the eyes of the beholder: different users may consider different attributes more important, and hence arrive at different rankings. Nevertheless, one can remove 'dominated' items and create a 'representative' subset of the data, comprising the 'best items' in it. A Pareto-optimal representative is guaranteed to contain the best item of each possible ranking, but it can be a large portion of data. A much smaller representative can be found if we relax the requirement to include the best item for each user, and instead just limit the users' 'regret'. Existing work defines regret as the loss in score by limiting consideration to the representative instead of the full data set, for any chosen ranking function. However, the score is often not a meaningful number and users may not understand its absolute value. Sometimes small ranges in score can include large fractions of the data set. In contrast, users do understand the notion of rank ordering. Therefore, we consider the position of the items in the ranked list for defining the regret and propose the rank-regret representative as the minimal subset of the data containing at least one of the top-k of any possible ranking function. This problem is NP-complete. We use a geometric interpretation of items to bound their ranks on ranges of functions and to utilize combinatorial geometry notions for developing effective and efficient approximation algorithms for the problem. Experiments on real datasets demonstrate that we can efficiently find small subsets with small rank-regrets.",
  "subtype": "SIGMOD Research",
  "authors": [
    {
      "name": "Abolfazl Asudeh",
      "affiliation": "University of Michigan"
    },
    {
      "name": "Azade Nazi",
      "affiliation": "Google AI"
    },
    {
      "name": "Nan Zhang",
      "affiliation": "Pennsylvania State University"
    },
    {
      "name": "Gautam Das",
      "affiliation": "University of Texas at Arlington"
    },
    {
      "name": "H. V. Jagadish",
      "affiliation": "University of Michigan"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Research_148"
},
"SIGMOD_Research_156": {
  "title": "Designing Distributed Tree-based Index Structures for Fast RDMA-capable Networks",
  "abstract": "Over the past decade, in-memory database systems have become prevalent in academia and industry. However, large data sets often need to be stored distributed across the memory of several nodes in a cluster, since they often do not fit into the memory of a single machine. A database architecture that has recently been proposed for building distributed in-memory databases for fast RDMA-capable networks is the Network-Attached-Memory (NAM) architecture. The NAM architecture logically separates compute and memory servers and thus provides independent scalability of both resources. One important key challenge in the NAM architecture, is to provide efficient remote access methods for compute nodes to access data residing in memory nodes. In this paper, we therefore discuss design alternatives for distributed tree-based index structures in the NAM architecture. The two main aspects that we focus on in our paper are: (1) how the index itself should be distributed across several memory servers and (2) which RDMA primitives should be used by compute servers to access the distributed index structure in the most efficient manner. Our experimental evaluation shows the trade-offs for different distributed index design alternatives using a variety of workloads. While the focus of this paper is on the NAM architecture, we believe that the findings can also help to understand the design space on how to build distributed tree-based indexes for other RDMA-based distributed database architectures in general.",
  "subtype": "SIGMOD Research",
  "authors": [
    {
      "name": "Tobias Ziegler",
      "affiliation": "TU Darmstadt"
    },
    {
      "name": "Sumukha Tumkur Vani",
      "affiliation": "Brown University"
    },
    {
      "name": "Carsten Binnig",
      "affiliation": "TU Darmstadt"
    },
    {
      "name": "Rodrigo Fonseca",
      "affiliation": "Brown University"
    },
    {
      "name": "Tim Kraska",
      "affiliation": "MIT"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Research_156"
},
"SIGMOD_Research_166": {
  "title": "Unboundedness and Efficiency of Truss Maintenance in Evolving Graphs",
  "abstract": "Due to the ubiquity of graphs, graph analytics has attracted much attention from both research and industry communities. The notion of k-truss is widely used in graph analytics. Since graphs are continuously evolving in real applications and it is costly to compute trusses from scratch, we study the problem of truss maintenance which aims at designing efficient incremental algorithms to update trusses when graphs are updated with changes. An incremental algorithm is desired to be bounded; that is, its cost is of O(f(|CHANGED|_c)) for some polynomial function f and some positive integer c, where CHANGED comprises the changes to both the graph and the result and |CHANGED|_c is the size of the c-hop neighborhood of CHANGED. An incremental problem is bounded if it has a bounded incremental algorithm and is unbounded otherwise. Under the model of locally persistent algorithms, we prove that truss maintenance is bounded under edge removals but is unbounded even for unit edge insertions. To address the unboundedness, we formulate a new notion AFF^preceq which, as a practically effective alternative to CHANGED, represents a set of edges affected by the changes to the graph, and devise an insertion algorithm that is bounded with respect to AFF^preceq, while retaining the boundedness for edge removals. More specifically, our insertion algorithm runs in O(f(|AFF^preceq|_c)) time for some polynomial function f and some positive integer c with |AFF^preceq|_c being the size of the c-hop neighborhood of AFF^preceq. Our extensive performance studies show that our new algorithms can significantly outperform the state-of-the-art by up to 3 orders of magnitude for the 12 large real graphs tested and are more efficient than computing trusses from scratch even for changes of non-trivial size. We report our findings in this paper.",
  "subtype": "SIGMOD Research",
  "authors": [
    {
      "name": "Yikai Zhang",
      "affiliation": "Chinese University of Hong Kong"
    },
    {
      "name": "Jeffrey Yu",
      "affiliation": "Chinese University of Hong Kong"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Research_166"
},
"SIGMOD_Research_177": {
  "title": "vChain: Enabling Verifiable Boolean Range Queries over Blockchain Databases",
  "abstract": "Blockchains have recently been under the spotlight due to the boom of cryptocurrencies and decentralized applications. There is an increasing demand for querying the data stored in a blockchain database. To ensure query integrity, the user can maintain the entire blockchain database and query the data locally. However, this approach is not economic, if not infeasible, because of the blockchain's huge data size and considerable maintenance costs. In this paper, we take the first step toward investigating the problem of verifiable query processing over blockchain databases. We propose a novel framework, called vChain, that alleviates the storage and computing costs of the user and employs verifiable queries to guarantee the results' integrity. To support verifiable Boolean range queries, we propose an accumulator-based authenticated data structure that enables dynamic aggregation over arbitrary query attributes. Two new indexes are further developed to aggregate intra-block and inter-block data records for efficient query verification. We also propose an inverted prefix tree structure to accelerate the processing of a large number of subscription queries simultaneously. Security analysis and empirical study validate the robustness and practicality of the proposed techniques.",
  "subtype": "SIGMOD Research",
  "authors": [
    {
      "name": "Cheng Xu",
      "affiliation": "Hong Kong Baptist University"
    },
    {
      "name": "Ce Zhang",
      "affiliation": "Hong Kong Baptist University"
    },
    {
      "name": "Jianliang Xu",
      "affiliation": "Hong Kong Baptist University"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Research_177"
},
"SIGMOD_Research_179": {
  "title": "Hypothetical Reasoning via Provenance Abstraction",
  "abstract": "Data analytics often involves hypothetical reasoning: repeatedly modifying the data and observing the induced effect on the computation result of a data-centric application. Previous work has shown that fine-grained data provenance can help make such an analysis more efficient: instead of a costly re-execution of the underlying application, hypothetical scenarios are applied to a pre-computed provenance expression. However, storing provenance for complex queries and large-scale data leads to a significant overhead, which is often a barrier to the incorporation of provenance-based solutions.   To this end, we present a framework that allows to reduce provenance size. Our approach is based on reducing the provenance granularity using user defined abstraction trees over the provenance variables; the granularity is based on the anticipated hypothetical scenarios. We formalize the tradeoff between provenance size and supported granularity of the hypothetical reasoning, and study the complexity of the resulting optimization problem, provide efficient algorithms for tractable cases and heuristics for others. We experimentally study the performance of our solution for various queries and abstraction trees. Our study shows that the algorithms generally lead to substantial speedup of hypothetical reasoning, with a reasonable loss of accuracy.",
  "subtype": "SIGMOD Research",
  "authors": [
    {
      "name": "Daniel Deutch",
      "affiliation": "Tel Aviv University"
    },
    {
      "name": "Yuval Moskovitch",
      "affiliation": "Tel Aviv University"
    },
    {
      "name": "Noam Rinetzky",
      "affiliation": "Tel Aviv University"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Research_179"
},
"SIGMOD_Research_193": {
  "title": "An End-to-End Automatic Cloud Database Tuning System Using Deep Reinforcement Learning",
  "abstract": "Configuration tuning is vital to optimize the performance of database management system (DBMS). It becomes more tedious and urgent for cloud databases (CDB) due to the diverse database instances and query workloads, which make the database administrator (DBA) incompetent. Although there are some studies on automatic DBMS configuration tuning, they have several limitations. Firstly, they adopt a pipelined learning model but cannot optimize the overall performance in an end-to-end manner. Secondly, they rely on large-scale high-quality training samples which are hard to obtain. Thirdly, there are a large number of knobs that are in continuous space and have unseen dependencies, and they cannot recommend reasonable configurations in such high-dimensional continuous space. Lastly, in cloud environment, they can hardly cope with the changes of hardware configurations and workloads, and have poor adaptability. To address these challenges, we design an end-to-end automatic CDB tuning system, CDBTune, using deep reinforcement learning (RL). CDBTune utilizes the deep deterministic policy gradient method to find the optimal configurations in high-dimensional continuous space. CDBTune adopts a try-and-error strategy to learn knob settings with a limited number of samples to accomplish the initial training, which alleviates the difficulty of collecting massive high-quality samples. CDBTune adopts the reward-feedback mechanism in RL instead of traditional regression, which enables end-to-end learning and accelerates the convergence speed of our model and improves efficiency of online tuning. We conducted extensive experiments under 6 different workloads on real cloud databases to demonstrate the superiority of CDBTune. Experimental results showed that CDBTune had a good adaptability and significantly outperformed the state-of-the-art tuning tools and DBA experts.",
  "subtype": "SIGMOD Research",
  "authors": [
    {
      "name": "Ji Zhang",
      "affiliation": "Huazhong University of Science and Technology"
    },
    {
      "name": "Yu Liu",
      "affiliation": "Huazhong University of Science and Technology"
    },
    {
      "name": "Ke Zhou",
      "affiliation": "Huazhong University of Science and Technology"
    },
    {
      "name": "Guoliang Li",
      "affiliation": "Tsinghua University"
    },
    {
      "name": "Zhili Xiao",
      "affiliation": "Tencent Inc."
    },
    {
      "name": "Bin Cheng",
      "affiliation": "Tencent Inc."
    },
    {
      "name": "Jiashu Xing",
      "affiliation": "Tencent Inc."
    },
    {
      "name": "Yangtao Wang",
      "affiliation": "Huazhong University of Science and Technology"
    },
    {
      "name": "Tianheng Cheng",
      "affiliation": "Huazhong University of Science and Technology"
    },
    {
      "name": "Li Liu",
      "affiliation": "Huazhong University of Science and Technology"
    },
    {
      "name": "Minwei Ran",
      "affiliation": "Huazhong University of Science and Technology"
    },
    {
      "name": "Zekang Li",
      "affiliation": "Huazhong University of Science and Technology"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Research_193"
},
"SIGMOD_Research_200": {
  "title": "CECI: Compact Embedding Cluster Index for Scalable Subgraph Matching",
  "abstract": "Subgraph matching finds all distinct isomorphic embeddings of a query graph on a data graph. For large graphs, current solutions face the scalability challenge due to expensive joins, excessive false candidates, and workload imbalance. In this paper, we propose a novel framework for subgraph listing based on Compact Embedding Cluster Index (idx), which divides the data graph into multiple embedding clusters for parallel processing. The sub has three unique techniques: utilizing the BFS-based filtering and reverse-BFS-based refinement to prune the unpromising candidates early on, replacing the edge verification with set intersection to speed up the candidate verification, and using search cardinality based cost estimation for detecting and dividing large embedding clusters in advance. The experiments performed on several real and synthetic datasets show that the sub outperforms state-of-the-art solutions on average by 20.4times for listing all embeddings and by 2.6times for enumerating the first 1,024 embeddings.",
  "subtype": "SIGMOD Research",
  "authors": [
    {
      "name": "Bibek Bhattarai",
      "affiliation": "George Washington University"
    },
    {
      "name": "Hang Liu",
      "affiliation": "University of Massachusetts Lowell"
    },
    {
      "name": "H. Howie Huang",
      "affiliation": "George Washington University"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Research_200"
},
"SIGMOD_Research_202": {
  "title": "Exact Cardinality Query Optimization with Bounded Execution Cost",
  "abstract": "Query optimizers often produce sub-optimal query plans due to inaccurate cardinality estimates. The goal of exact cardinality query optimization (ECQO) is to produce optimal plans based on guaranteed exact cardinality values, obtained via probe executions. We propose a novel algorithm for ECQO. It improves over prior work by limiting the overheads of probe executions. Instead of fully generating each relation in the optimizer's plan space, it calculates cardinality bounds based on partially generated relations. Thereby, it is often able to prune relations early if they are too large to participate in any optimal plan. Our algorithm exploits dependencies between relations to propagate exclusions, it selects probing targets for maximum information gain, and it caps probing overheads by conservative bounds on execution cost. Those bounds are iteratively increased once lower bounds on optimal query execution cost have been established. The algorithm is non-intrusive and can be used on top of any database management system supporting SQL limit clauses. We formally prove that probing costs are bounded as a function of the optimal execution cost and evaluate our algorithm experimentally. Our algorithm is in average six times and up to 69 times faster than a state-of-the-art baseline for ECQO on the recently proposed join order benchmark.",
  "subtype": "SIGMOD Research",
  "authors": [
    {
      "name": "Immanuel Trummer",
      "affiliation": "Cornell University"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Research_202"
},
"SIGMOD_Research_207": {
  "title": "SkinnerDB: Regret-Bounded Query Evaluation via Reinforcement Learning",
  "abstract": "SkinnerDB is designed from the ground up for reliable join ordering. It maintains no data statistics and uses no cost or cardinality models. Instead, it uses reinforcement learning to learn optimal join orders on the fly, during the execution of the current query. To that purpose, we divide the execution of a query into many small time slices. Different join orders are tried in different time slices. We merge result tuples generated according to different join orders until a complete result is obtained. By measuring execution progress per time slice, we identify promising join orders as execution proceeds. Along with SkinnerDB, we introduce a new quality criterion for query execution strategies. We compare expected execution cost against execution cost for an optimal join order. SkinnerDB features multiple execution strategies that are optimized for that criterion. Some of them can be executed on top of existing database systems. For maximal performance, we introduce a customized execution engine, facilitating fast join order switching via specialized multi-way join algorithms and tuple representations. We experimentally compare SkinnerDB's performance against various baselines, including MonetDB, Postgres, and adaptive processing methods. We consider various benchmarks, including the join order benchmark and TPC-H variants with user-defined functions. Overall, the overheads of reliable join ordering are negligible compared to the performance impact of the occasional, catastrophic join order choice.",
  "subtype": "SIGMOD Research",
  "authors": [
    {
      "name": "Immanuel Trummer",
      "affiliation": "Cornell University"
    },
    {
      "name": "Junxiong Wang",
      "affiliation": "Cornell University"
    },
    {
      "name": "Deepak Maram",
      "affiliation": "Cornell University"
    },
    {
      "name": "Samuel Moseley",
      "affiliation": "Cornell University"
    },
    {
      "name": "Saehan Jo",
      "affiliation": "Cornell University"
    },
    {
      "name": "Joseph Antonakakis",
      "affiliation": "Cornell University"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Research_207"
},
"SIGMOD_Research_210": {
  "title": "A Holistic Approach for Query Evaluation andResult Vocalization in Voice-Based OLAP",
  "abstract": "We focus on the problem of answering OLAP queries via voice output. We present a holistic approach that combines query processing and result vocalization. We use the following key ideas to minimize processing overheads and maximize answer quality. First, our approach samples from the database to evaluate alternative speech fragments. OLAP queries are not fully evaluated. Instead, sampling focuses on result aspects that are relevant for voice output. To guide sampling, we rely on methods from the area of Monte-Carlo Tree Search. Second, we use pipelining to interleave query processing and voice output. The system starts providing the user with high-level insights while generating more fine-grained results in the background. Third, we optimize speech output to maximize the user's information gain under speaking time constraints. We use a maximum-entropy model to predict the user's belief about OLAP results, after listening to voice output. Based on that model, we select the most informative speech fragments (i.e., the ones minimizing the distance between user belief and actual data). We analyze formal properties of the proposed speech structure and analyze complexity of our algorithm. Also, we compare alternative vocalization approaches in an extensive user study.",
  "subtype": "SIGMOD Research",
  "authors": [
    {
      "name": "Immanuel Trummer",
      "affiliation": "Cornell University"
    },
    {
      "name": "Yicheng Wang",
      "affiliation": "Cornell University"
    },
    {
      "name": "Saketh Mahankali",
      "affiliation": "Cornell University"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Research_210"
},
"SIGMOD_Research_223": {
  "title": "Concurrent Prefix Recovery: Performing CPR on a Database",
  "abstract": "With increasing multi-core parallelism, modern databases and key-value stores are designed for scalability and presently yield very high throughput for the in-memory working set. These systems typically depend on group commit using a write-ahead log (WAL) to provide durability and crash recovery. However, a WAL is expensive, particularly for update-intensive workloads, where it also introduces a concurrency bottleneck (the log) besides log creation and I/O overheads. In this paper, we propose a new recovery model based on group commit, called concurrent prefix recovery (CPR). CPR differs from traditional group commit implementations in two ways: (1) it provides a semantic description of committed operations, of the form all operations until time Ti from session i; and (2) it uses asynchronous incremental checkpointing instead of a WAL to implement group commit in a scalable bottleneck-free manner. CPR provides the same consistency as a point-in-time commit, but allows a scalable concurrent implementation. We used CPR to make two systems durable: (1) a custom in-memory transactional database; and (2) Faster, our state-of-the-art, scalable, larger-than-memory key-value store. Our detailed evaluation of these modified systems shows that CPR is highly scalable and supports concurrent performance reaching hundreds of millions of operations per second on a multi-core machine.",
  "subtype": "SIGMOD Research",
  "authors": [
    {
      "name": "Guna Prasaad",
      "affiliation": "University of Washington"
    },
    {
      "name": "Badrish Chandramouli",
      "affiliation": "Microsoft Research"
    },
    {
      "name": "Donald Kossmann",
      "affiliation": "Microsoft Research"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Research_223"
},
"SIGMOD_Research_226": {
  "title": "Ariadne: Online Provenance for Big Graph Analytics",
  "abstract": "Data provenance is a powerful tool for debugging large-scale analytics on batch processing systems. This paper presents Ariadne, a system for capturing and querying provenance from Vertex-Centric graph processing systems. While the size of provenance from map-reduce-style workflows is often a fraction of the input data size, graph algorithms iterate over the input graph many times, producing provenance much larger than the input graph. And though current provenance tracing procedures support explicit debugging scenarios, like crash-culprit determination, developers are increasingly interested in the behavior of analytics when a crash or exception does not occur. To address this challenge, Ariadne offers developers a concise declarative query language to capture and query graph analytics provenance. Exploiting the formal semantics of this datalog-based language, we identify useful query classes that can run while an analytic computes. Experiments with various analytics and real-world datasets show the overhead of online querying is 1.3x over the baseline vs. 8x for the traditional approach. These experiments also illustrate how Ariadne's query language supports execution monitoring and performance optimization for graph analytics.",
  "subtype": "SIGMOD Research",
  "authors": [
    {
      "name": "Vicky Papavasileiou",
      "affiliation": "University of California, San Diego"
    },
    {
      "name": "Ken Yocum",
      "affiliation": "Intuit,Inc. & University of California, San Diego"
    },
    {
      "name": "Alin Deutsch",
      "affiliation": "University of California, San Diego"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Research_226"
},
"SIGMOD_Research_240": {
  "title": "APEx: Accuracy-Aware Differentially Private Data Exploration",
  "abstract": "Organizations are increasingly interested in allowing external data scientists to explore their sensitive datasets. Due to the popularity of differential privacy, data owners want the data exploration to ensure provable privacy guarantees. However, current systems for answering queries with differential privacy place an inordinate burden on the data analysts to understand differential privacy, manage their privacy budget, and even implement new algorithms for noisy query answering. Moreover, current systems do not provide any guarantees to the data analyst on the quality they care about, namely accuracy of query answers. We present APEx, a novel system that allows data analysts to pose adaptively chosen sequences of queries along with required accuracy bounds. By translating queries and accuracy bounds into differentially private algorithms with the least privacy loss, APEx returns query answers to the data analyst that meet the accuracy bounds, and proves to the data owner that the entire data exploration process is differentially private. Our comprehensive experimental study on real datasets demonstrates that APEx can answer a variety of queries accurately with moderate to small privacy loss, and can support data exploration for entity resolution with high accuracy under reasonable privacy settings.",
  "subtype": "SIGMOD Research",
  "authors": [
    {
      "name": "Chang Ge",
      "affiliation": "University of Waterloo"
    },
    {
      "name": "Xi He",
      "affiliation": "University of Waterloo"
    },
    {
      "name": "Ihab Ilyas",
      "affiliation": "University of Waterloo"
    },
    {
      "name": "Ashwin Machanavajjhala",
      "affiliation": "Duke University"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Research_240"
},
"SIGMOD_Research_262": {
  "title": "MNC: Structure-Exploiting Sparsity Estimation for Matrix Expressions",
  "abstract": "Efficiently computing linear algebra expressions is central to machine learning (ML) systems. Most systems support sparse formats and operations because sparse matrices are ubiquitous and their dense representation can cause prohibitive overheads. Estimating the sparsity of intermediates, however, remains a key challenge when generating execution plans or performing sparse operations. These sparsity estimates are used for cost and memory estimates, format decisions, and result allocation. Existing estimators tend to focus on matrix products only, and struggle to attain good accuracy with low estimation overhead. However, a key observation is that real-world sparse matrices commonly exhibit structural properties such as a single non-zero per row, or columns with varying sparsity. In this paper, we introduce MNC (Matrix Non-zero Count), a remarkably simple, count-based matrix synopsis that exploits these structural properties for efficient, accurate, and general sparsity estimation. We describe estimators and sketch propagation for realistic linear algebra expressions. Our experiments - on a new estimation benchmark called SparsEst - show that the MNC estimator yields good accuracy with very low overhead. This behavior makes MNC practical and broadly applicable in ML systems.",
  "subtype": "SIGMOD Research",
  "authors": [
    {
      "name": "Johanna Sommer",
      "affiliation": "IBM Germany"
    },
    {
      "name": "Matthias Boehm",
      "affiliation": "Graz University of Technology"
    },
    {
      "name": "Alexandre Evfimievski",
      "affiliation": "IBM Almaden Research Center"
    },
    {
      "name": "Berthold Reinwald",
      "affiliation": "IBM Almaden Research Center"
    },
    {
      "name": "Peter Haas",
      "affiliation": "University of Massachusetts Amherst"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Research_262"
},
"SIGMOD_Research_267": {
  "title": "Uni-Detect: A Unified Approach to Automated Error Detection in Tables",
  "abstract": "Data errors are ubiquitous in tables. Extensive research in this area has resulted in a rich variety of techniques, each often targeting a specific type of errors, e.g., numeric outliers, constraint violations, etc. While these diverse techniques clearly improve data quality, it places a significant burden on humans to configure these techniques with suitable rules and parameters for each data set. For example, an expert is expected to define suitable functional-dependencies between column pairs, or tune appropriate thresholds for outlier-detection algorithms, all of which are specific to one individual data set. As a result, users today often hire experts to cleanse only their high-value data sets. We propose sj, a unified framework to automatically detect diverse types of errors. Our approach employs a novel 'what-if' analysis that performs local data perturbations to reason about data abnormality, leveraging classical hypothesis-tests on a large corpus of tables. We test sj on a wide variety of tables including Wikipedia tables, and make surprising discoveries of thousands of FD violations, numeric outliers, spelling mistakes, etc., with better accuracy than existing algorithms specifically designed for each type of errors. For example, for spelling mistakes, sj outperforms the state-of-the-art spell-checker from a commercial search engine.",
  "subtype": "SIGMOD Research",
  "authors": [
    {
      "name": "Pei Wang",
      "affiliation": "Simon Fraser University"
    },
    {
      "name": "Yeye He",
      "affiliation": "Microsoft Research"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Research_267"
},
"SIGMOD_Research_269": {
  "title": "Active Sparse Mobile Crowd Sensing Based on Matrix Completion",
  "abstract": "A major factor that prevents the large scale deployment of Mobile Crowd Sensing (MCS) is its sensing and communication cost. Given the spatio-temporal correlation among the environment monitoring data, matrix completion (MC) can be exploited to only monitor a small part of locations and time, and infer the remaining data. Rather than only taking random measurements following the basic MC theory, to further reduce the cost of MCS while ensuring the quality of missing data inference, we propose an Active Sparse MCS (AS-MCS) scheme which includes a bipartite-graph-based sensing scheduling scheme to actively determine the sampling positions in each upcoming time slot, and a bipartite-graph-based matrix completion algorithm to robustly and accurately recover the un-sampled data in the presence of sensing and communications errors. We also incorporate the sensing cost into the bipartite-graph to facilitate low cost sample selection and consider the incentives for MCS. We have conducted extensive performance studies using the data sets from the monitoring of PM 2.5 air condition and road traffic speed, respectively. Our results demonstrate that our AS-MCS scheme can recover the missing data at very high accuracy with the sampling ratio only around 11%, while the peer matrix completion algorithms with similar recovery performance requires up to 4-9 times the number of samples of ours for both the data sets.",
  "subtype": "SIGMOD Research",
  "authors": [
    {
      "name": "Kun Xie",
      "affiliation": "Hunan University"
    },
    {
      "name": "Xiaocan Li",
      "affiliation": "Hunan University"
    },
    {
      "name": "Xin Wang",
      "affiliation": "Stony Brook University"
    },
    {
      "name": "Gaogang Xie",
      "affiliation": "Institute of Computing Technology & Chinese Academy of Sciences"
    },
    {
      "name": "Jigang Wen",
      "affiliation": "Institute of Computing Technology & Chinese Academy of Sciences"
    },
    {
      "name": "Dafang Zhang",
      "affiliation": "Hunan University"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Research_269"
},
"SIGMOD_Research_278": {
  "title": "Top-k Queries over Digital Traces",
  "abstract": "Recent advances in social and mobile technology have enabled an abundance of digital traces (in the form of mobile check-ins, association of mobile devices to specific WiFi hotspots, etc.) revealing the physical presence history of diverse sets of entities (e.g., humans, devices, and vehicles). One challenging yet important task is to identify k entities that are most closely associated with a given query entity based on their digital traces. We propose a suite of indexing techniques and algorithms to enable fast query processing for this problem at scale. We first define a generic family of functions measuring the association between entities, and then propose algorithms to transform digital traces into a lower-dimensional space for more efficient computation. We subsequently design a hierarchical indexing structure to organize entities in a way that closely associated entities tend to appear together. We then develop algorithms to process top-k queries utilizing the index. We theoretically analyze the pruning effectiveness of the proposed methods based on a mobility model which we propose and validate in real life situations. Finally, we conduct extensive experiments on both synthetic and real datasets at scale, evaluating the performance of our techniques both analytically and experimentally, confirming the effectiveness and superiority of our approach over other applicable approaches across a variety of parameter settings and datasets.",
  "subtype": "SIGMOD Research",
  "authors": [
    {
      "name": "Yifan Li",
      "affiliation": "York University"
    },
    {
      "name": "Xiaohui Yu",
      "affiliation": "York University"
    },
    {
      "name": "Nick Koudas",
      "affiliation": "University of Toronto"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Research_278"
},
"SIGMOD_Research_279": {
  "title": "Autocompletion for Prefix-Abbreviated Input",
  "abstract": "Query autocompletion (QAC) is an important interactive feature that assists users in formulating queries and saving keystrokes. Due to the convenience it brings to users, QAC has been adopted in many applications, including Web search engines, integrated development environments (IDEs), and mobile devices. For existing QAC methods, users have to manually type delimiters to separate keywords in their inputs. In this paper, we propose a novel QAC paradigm through which users may abbreviate keywords by prefixes and do not have to explicitly separate them. Such paradigm is useful for applications where it is inconvenient to specify delimiters, such as desktop search, text editors, and input method editors. E.g., in an IDE, users may input getnev and we suggest GetNextValue. We show that the query processing method for traditional QAC, which utilizes a trie index, is inefficient under the new problem setting. A novel indexing and query processing scheme is hence proposed to efficiently complete queries. To suggest meaningful results, we devise a ranking method based on a Gaussian mixture model, taking into consideration the way in which users abbreviate keywords, as opposed to the traditional ranking method that merely considers popularity. Efficient top-k query processing techniques are developed on top of the new index structure. Experiments demonstrate the effectiveness of the new QAC paradigm and the efficiency of the proposed query processing method.",
  "subtype": "SIGMOD Research",
  "authors": [
    {
      "name": "Sheng Hu",
      "affiliation": "Nagoya University & Kyoto University"
    },
    {
      "name": "Chuan Xiao",
      "affiliation": "Nagoya University & Osaka University"
    },
    {
      "name": "Jianbin Qin",
      "affiliation": "Shenzhen University"
    },
    {
      "name": "Yoshiharu Ishikawa",
      "affiliation": "Nagoya University"
    },
    {
      "name": "Qiang Ma",
      "affiliation": "Kyoto University"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Research_279"
},
"SIGMOD_Research_285": {
  "title": "Cache-oblivious High-performance Similarity Join",
  "abstract": "A similarity join combines vectors based on a distance condition. Typically, such algorithms apply a filter step (by indexing or sorting) and then refine pairs of candidate vectors. In this paper, we propose to refine the pairs in an order defined by a space-filling curve which dramatically improves data locality. Modern multi-core microprocessors are supported by a deep memory hierarchy including RAM, various levels of cache, and registers. The space-filling curve makes our proposed algorithm cache-oblivious to fully exploit the memory hierarchy and to reach the possible peak performance of a multi-core processor. Our novel space-filling curve called Fast General Form (FGF) Hilbert solves a number of limitations of well-known approaches: it is non-recursive, it is not restricted to traverse squares, and it has a constant time and space complexity. As we demonstrate the easy transformation from conventional into cache-oblivious loops we believe that many algorithms for complex joins and other database operators could be transformed systematically into cache-oblivious SIMD and MIMD parallel algorithms.",
  "subtype": "SIGMOD Research",
  "authors": [
    {
      "name": "Martin Perdacher",
      "affiliation": "University of Vienna"
    },
    {
      "name": "Claudia Plant",
      "affiliation": "University of Vienna"
    },
    {
      "name": "Christian Böhm",
      "affiliation": "Ludwig-Maximilians-Universität"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Research_285"
},
"SIGMOD_Research_289": {
  "title": "FITing-Tree: A Data-aware Index Structure",
  "abstract": "Index structures are one of the most important tools that DBAs leverage to improve the performance of analytics and transactional workloads. However, building several indexes over large datasets can often become prohibitive and consume valuable system resources. In fact, a recent study showed that indexes created as part of the TPC-C benchmark can account for 55% of the total memory available in a modern DBMS. This overhead consumes valuable and expensive main memory, and limits the amount of space available to store new data or process existing data. In this paper, we present a novel data-aware index structure called FITing-Tree which approximates an index using piece-wise linear functions with a bounded error specified at construction time. This error knob provides a tunable parameter that allows a DBA to FIT an index to a dataset and workload by being able to balance lookup performance and space consumption. To navigate this tradeoff, we provide a cost model that helps determine an appropriate error parameter given either (1) a lookup latency requirement (e.g., 500ns) or (2) a storage budget (e.g., 100MB). Using a variety of real-world datasets, we show that our index is able to provide performance that is comparable to full index structures while reducing the storage footprint by orders of magnitude.",
  "subtype": "SIGMOD Research",
  "authors": [
    {
      "name": "Alex Galakatos",
      "affiliation": "Brown University"
    },
    {
      "name": "Michael Markovitch",
      "affiliation": "Brown University"
    },
    {
      "name": "Carsten Binnig",
      "affiliation": "TU Darmstadt"
    },
    {
      "name": "Rodrigo Fonseca",
      "affiliation": "Brown University"
    },
    {
      "name": "Tim Kraska",
      "affiliation": "MIT"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Research_289"
},
"SIGMOD_Research_294": {
  "title": "Designing Succinct Secondary Indexing Mechanism by Exploiting Column Correlations",
  "abstract": "Database administrators construct secondary indexes on data tables to accelerate query processing in relational database management systems (RDBMSs). These indexes are built on top of the most frequently queried columns according to the data statistics. Unfortunately, maintaining multiple secondary indexes in the same database can be extremely space consuming, causing significant performance degradation due to the potential exhaustion of memory space. In this paper, we demonstrate that there exist many opportunities to exploit column correlations for accelerating data access. We propose HERMIT, a succinct secondary indexing mechanism for modern RDBMSs. HERMIT judiciously leverages the rich soft functional dependencies hidden among columns to prune out redundant structures for indexed key access. Instead of building a complete index that stores every single entry in the key columns, HERMIT navigates any incoming key access queries to an existing index built on the correlated columns. This is achieved through the Tiered Regression Search Tree (TRS-Tree), a succinct, ML-enhanced data structure that performs fast curve fitting to adaptively and dynamically capture both column correlations and outliers. Our extensive experimental study in two different RDBMSs have confirmed that HERMIT can significantly reduce space consumption with limited performance overhead, especially when supporting complex range queries.",
  "subtype": "SIGMOD Research",
  "authors": [
    {
      "name": "Yingjun Wu",
      "affiliation": "IBM Almaden Research Center"
    },
    {
      "name": "Jia Yu",
      "affiliation": "Arizona State University"
    },
    {
      "name": "Yuanyuan Tian",
      "affiliation": "IBM Almaden Research Center"
    },
    {
      "name": "Richard Sidle",
      "affiliation": "IBM"
    },
    {
      "name": "Ronald Barber",
      "affiliation": "IBM Almaden Research Center"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Research_294"
},
"SIGMOD_Research_295": {
  "title": "Event Trend Aggregation Under Rich Event Matching Semantics",
  "abstract": "Streaming applications from cluster monitoring to algorithmic trading deploy Kleene queries to detect and aggregate event trends. Rich event matching semantics determine how to compose events into trends. The expressive power of state-of-the-art streaming systems remains limited since they do not support many of these semantics. Worse yet, they suffer from long delays and high memory costs because they maintain aggregates at a fine granularity. To overcome these limitations, our Coarse-Grained Event Trend Aggregation (Cogra) approach supports a rich variety of event matching semantics within one system. Better yet, Cogra incrementally maintains aggregates at the coarsest granularity possible for each of these semantics. In this way, Cogra minimizes the number of aggregates -- reducing both time and space complexity. Our experiments demonstrate that Cogra achieves up to six orders of magnitude speed-up and up to seven orders of magnitude memory reduction compared to state-of-the-art approaches.",
  "subtype": "SIGMOD Research",
  "authors": [
    {
      "name": "Olga Poppe",
      "affiliation": "Microsoft Gray Systems Lab"
    },
    {
      "name": "Chuan Lei",
      "affiliation": "IBM Almaden Research Center"
    },
    {
      "name": "Elke Rundensteiner",
      "affiliation": "Worcester Polytechnic Institute"
    },
    {
      "name": "David Maier",
      "affiliation": "Portland State University"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Research_295"
},
"SIGMOD_Research_298": {
  "title": "Democratizing Data Science through Interactive Curation of ML Pipelines",
  "abstract": "Statistical knowledge and domain expertise are key to extract actionable insights out of data, yet such skills rarely coexist together. In Machine Learning, high-quality results are only attainable via mindful data preprocessing, hyperparameter tuning and model selection. Domain experts are often overwhelmed by such complexity, de-facto inhibiting a wider adoption of ML techniques in other fields. Existing libraries that claim to solve this problem, still require well-trained practitioners. Those frameworks involve heavy data preparation steps and are often too slow for interactive feedback from the user, severely limiting the scope of such systems. In this paper we present Alpine Meadow, a first Interactive Automated Machine Learning tool. What makes our system unique is not only the focus on interactivity, but also the combined systemic and algorithmic design approach; on one hand we leverage ideas from query optimization, on the other we devise novel selection and pruning strategies combining cost-based Multi-Armed Bandits and Bayesian Optimization. We evaluate our system on over 300 datasets and compare against other AutoML tools, including the current NIPS winner, as well as expert solutions. Not only is Alpine Meadow able to significantly outperform the other AutoML systems while - in contrast to the other systems - providing interactive latencies, but also outperforms in 80% of the cases expert solutions over data sets we have never seen before. As a result of this, since March 2018 Alpine Meadow leads the DARPA D3M Automatic Machine Learning competition.",
  "subtype": "SIGMOD Research",
  "authors": [
    {
      "name": "Zeyuan Shang",
      "affiliation": "MIT"
    },
    {
      "name": "Emanuel Zgraggen",
      "affiliation": "MIT"
    },
    {
      "name": "Benedetto Buratti",
      "affiliation": "Brown University"
    },
    {
      "name": "Ferdinand Kossmann",
      "affiliation": "MIT"
    },
    {
      "name": "Philipp Eichmann",
      "affiliation": "Brown University"
    },
    {
      "name": "Yeounoh Chung",
      "affiliation": "Brown University"
    },
    {
      "name": "Carsten Binnig",
      "affiliation": "TU Darmstadt"
    },
    {
      "name": "Eli Upfal",
      "affiliation": "Brown University"
    },
    {
      "name": "Tim Kraska",
      "affiliation": "MIT"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Research_298"
},
"SIGMOD_Research_300": {
  "title": "Visual Road: A Video Data Management Benchmark",
  "abstract": "Recently, video database management systems (VDBMSs) have re-emerged as an active area of research and development. To accelerate innovation in this area, we present Visual Road, a benchmark that evaluates the performance of these systems. Visual Road comes with a data generator and a suite of queries over cameras positioned within a simulated metropolitan environment. Visual Road's video data is automatically generated with a high degree of realism, and annotated using a modern simulation and visualization engine. This allows for VDBMS performance evaluation while scaling up the size of the input data. Visual Road is designed to evaluate a broad variety of VDBMSs: real-time systems, systems for longitudinal analytical queries, systems processing traditional videos, and systems designed for 360 videos. We use the benchmark to evaluate three recent VDBMSs both in capabilities and performance.",
  "subtype": "SIGMOD Research",
  "authors": [
    {
      "name": "Brandon Haynes",
      "affiliation": "University of Washington"
    },
    {
      "name": "Amrita Mazumdar",
      "affiliation": "University of Washington"
    },
    {
      "name": "Magdalena Balazinska",
      "affiliation": "University of Washington"
    },
    {
      "name": "Luis Ceze",
      "affiliation": "University of Washington"
    },
    {
      "name": "Alvin Cheung",
      "affiliation": "University of Washington"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Research_300"
},
"SIGMOD_Research_329": {
  "title": "Raha: A Configuration-Free Error Detection System",
  "abstract": "Detecting erroneous values is a key step in data cleaning. Error detection algorithms usually require a user to provide input configurations in the form of rules or statistical parameters. However, providing a complete, yet correct, set of configurations for each new dataset is not trivial, as the user has to know about both the dataset and the error detection algorithms upfront. In this paper, we present Raha, a new configuration-free error detection system. By generating a limited number of configurations for error detection algorithms that cover various types of data errors, we can generate an expressive feature vector for each tuple value. Leveraging these feature vectors, we propose a novel sampling and classification scheme that effectively chooses the most representative values for training. Furthermore, our system can exploit historical data to filter out irrelevant error detection algorithms and configurations. In our experiments, Raha outperforms the state-of-the-art error detection techniques with no more than 20 labeled tuples on each dataset.",
  "subtype": "SIGMOD Research",
  "authors": [
    {
      "name": "Mohammad Mahdavi",
      "affiliation": "TU Berlin"
    },
    {
      "name": "Ziawasch Abedjan",
      "affiliation": "TU Berlin"
    },
    {
      "name": "Raul Castro Fernandez",
      "affiliation": "MIT"
    },
    {
      "name": "Samuel Madden",
      "affiliation": "MIT"
    },
    {
      "name": "Mourad Ouzzani",
      "affiliation": "QCRI, HBKU"
    },
    {
      "name": "Michael Stonebraker",
      "affiliation": "MIT"
    },
    {
      "name": "Nan Tang",
      "affiliation": "QCRI, HBKU"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Research_329"
},
"SIGMOD_Research_351": {
  "title": "An Efficient Index for RDF Query Containment",
  "abstract": "Query containment is a fundamental operation used to expedite query processing in view materialisation and query caching techniques. Since query containment has been shown to be NP-complete for arbitrary conjunctive queries on RDF graphs, we introduce a simpler form of conjunctive queries that we name f-graph queries. We first show that containment checking for f-graph queries can be solved in polynomial time. Based on this observation, we propose a novel indexing structure, named mv-index, that allows for fast containment checking between a single f-graph query and an arbitrary number of stored queries. Search is performed in polynomial time in the combined size of the query and the index. We then show how our algorithms and structures can be extended for arbitrary conjunctive queries on RDF graphs by introducing f-graph witnesses, i.e., f-graph representatives of conjunctive queries. F-graph witnesses have the following interesting property, a conjunctive query for RDF graphs is contained in another query only if its corresponding f-graph witness is also contained in it. The latter allows to use our indexing structure for the general case of conjunctive query containment. This translates in practice to microseconds or less for the containment test against hundreds of thousands of queries that are indexed within our structure.",
  "subtype": "SIGMOD Research",
  "authors": [
    {
      "name": "Theofilos Mailis",
      "affiliation": "Athena Research Centre & University of Athens"
    },
    {
      "name": "Yannis Kotidis",
      "affiliation": "Athens University of Economics and Business"
    },
    {
      "name": "Vaggelis Nikolopoulos",
      "affiliation": "University of Athens"
    },
    {
      "name": "Evgeny Kharlamov",
      "affiliation": "University of Oslo & Bosch Center for AI"
    },
    {
      "name": "Ian Horrocks",
      "affiliation": "University of Oxford"
    },
    {
      "name": "Yannis Ioannidis",
      "affiliation": "Athena Research Centre & University of Athens"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Research_351"
},
"SIGMOD_Research_354": {
  "title": "DistME: A Fast and Elastic Distributed Matrix Computation Engine using GPUs",
  "abstract": "Matrix computation, in particular, matrix multiplication is time-consuming, but essentially and widely used in a large number of applications in science and industry. The existing distributed matrix multiplication methods only focus on either low communication cost (i.e., high performance) with the risk of out of memory or large-scale processing with high communication overhead. We propose a distributed elastic matrix multiplication method called CuboidMM that achieves both high performance and large-scale processing. We also propose a GPU acceleration method that can be combined with CuboidMM. CuboidMM partitions matrices into cuboids for optimizing the network communication cost with considering memory usage per task, and the GPU acceleration method partitions a cuboid into subcuboids for optimizing the PCI-E communication cost with considering GPU memory usage. We implement a fast and elastic matrix computation engine called DistME by integrating CuboidMM with GPU acceleration on top of Apache Spark. Through extensive experiments, we have demonstrated that CuboidMM and DistME significantly outperform the state-of-the-art methods and systems, respectively, in terms of both performance and data size.",
  "subtype": "SIGMOD Research",
  "authors": [
    {
      "name": "Donghyoung Han",
      "affiliation": "Daegu Gyeongbuk Institute of Science & Technology (DGIST)"
    },
    {
      "name": "Yoon-Min Nam",
      "affiliation": "Daegu Gyeongbuk Institute of Science & Technology (DGIST)"
    },
    {
      "name": "Jihye Lee",
      "affiliation": "Daegu Gyeongbuk Institute of Science & Technology (DGIST)"
    },
    {
      "name": "Kyongseok Park",
      "affiliation": "Korea Institute of Science and Technology Information (KISTI)"
    },
    {
      "name": "Hyunwoo Kim",
      "affiliation": "Korea Institute of Science and Technology Information (KISTI)"
    },
    {
      "name": "Min-Soo Kim",
      "affiliation": "Daegu Gyeongbuk Institute of Science & Technology (DGIST)"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Research_354"
},
"SIGMOD_Research_361": {
  "title": "Explaining Wrong Queries Using Small Examples",
  "abstract": "For testing the correctness of SQL queries, a standard practice is to execute the query in question on some test database instance and compare its result with that of the correct query. Given two queries Q_1 and Q_2, we say that a database instance D is a counterexample (for Q_1 and Q_2) if Q_1(D) differs from Q_2(D); such a counterexample can serve as an explanation of why Q_1 and Q_2 are not equivalent. While the test database instance may serve as a counterexample, it may be too large or complex to understand where the inequivalence arises. Therefore, in this paper, given a known counterexample D for Q_1 and Q_2, we aim to find the smallest counterexample D' subseteq D where Q_1(D') neq Q_2(D'). The problem in general is NP-hard. Drawing techniques from provenance and constraint solving, we develop a suite of algorithms for finding small counterexamples for different classes of queries, including those involving negation and aggregation. We evaluate the effectiveness and scalability of our algorithms on student queries from an undergraduate database course, and on queries from the TPC-H benchmark. We also report a user study from the course where we deployed our tool to help students with an assignment on relational algebra.",
  "subtype": "SIGMOD Research",
  "authors": [
    {
      "name": "Zhengjie Miao",
      "affiliation": "Duke University"
    },
    {
      "name": "Sudeepa Roy",
      "affiliation": "Duke University"
    },
    {
      "name": "Jun Yang",
      "affiliation": "Duke University"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Research_361"
},
"SIGMOD_Research_367": {
  "title": "Visual Segmentation for Information Extraction from Heterogeneous Visually Rich Documents",
  "abstract": "Physical and digital documents often contain visually rich information. With such information, there is no strict ordering or positioning in the document where the data values must appear. Along with textual cues, these documents often also rely on salient visual features to define distinct semantic boundaries and augment the information they disseminate. When performing information extraction (IE), traditional techniques fall short, as they use a text-only representation and do not consider the visual cues inherent to the layout of these documents. We propose VS2, a generalized approach for information extraction from heterogeneous visually rich documents. There are two major contributions of this work. First, we propose a robust segmentation algorithm that decomposes a visually rich document into a bag of visually isolated but semantically coherent areas, called logical blocks. Document type agnostic low-level visual and semantic features are used in this process. Our second contribution is a distantly supervised search-and-select method for identifying the named entities within these documents by utilizing the context boundaries defined by these logical blocks. Experimental results on three heterogeneous datasets suggest that the proposed approach significantly outperforms its text-only counterparts on all datasets. Comparing it against the state-of-the-art methods also reveal that VS2 performs comparably or better on all datasets.",
  "subtype": "SIGMOD Research",
  "authors": [
    {
      "name": "Ritesh Sarkhel",
      "affiliation": "Ohio State University"
    },
    {
      "name": "Arnab Nandi",
      "affiliation": "Ohio State University"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Research_367"
},
"SIGMOD_Research_380": {
  "title": "Elasticutor: Rapid Elasticity for Realtime Stateful Stream Processing",
  "abstract": "Elasticity is highly desirable for stream systems to guarantee low latency against workload dynamics, such as surges in arrival rate and fluctuations in data distribution. Existing systems achieve elasticity using a resource-centric approach that repartitions keys across the parallel instances, i.e., executors, to balance the workload and scale operators. However, such operator-level repartitioning requires global synchronization and prohibits rapid elasticity. We propose an executor-centric approach that avoids operator-level key repartitioning and implements executors as the building blocks of elasticity. By this new approach, we design the Elasticutor framework with two level of optimizations: i) a novel implementation of executors, i.e., elastic executors, that perform elastic multi-core execution via efficient intra-executor load balancing and executor scaling and ii) a global model-based scheduler that dynamically allocates CPU cores to executors based on the instantaneous workloads. We implemented a prototype of Elasticutor and conducted extensive experiments. We show that Elasticutor doubles the throughput and achieves up to two orders of magnitude lower latency than previous methods for dynamic workloads of real-world applications.",
  "subtype": "SIGMOD Research",
  "authors": [
    {
      "name": "Li Wang",
      "affiliation": "Yitu Technology"
    },
    {
      "name": "Tom Z. J. Fu",
      "affiliation": "Advanced Digital Sciences Center"
    },
    {
      "name": "Richard T. B. Ma",
      "affiliation": "National University of Singapore"
    },
    {
      "name": "Marianne Winslett",
      "affiliation": "University of Illinois Urbana-Champaign"
    },
    {
      "name": "Zhenjie Zhang",
      "affiliation": "Yitu Technology"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Research_380"
},
"SIGMOD_Research_385": {
  "title": "Real-Time Multi-Pattern Detection over Event Streams",
  "abstract": "Rapid advances in data-driven applications over recent years have intensified the need for efficient mechanisms capable of monitoring and detecting arbitrarily complex patterns in massive data streams. This task is usually performed by complex event processing (CEP) systems. CEP engines are required to process hundreds or even thousands of user-defined patterns in parallel under tight real-time constraints. To enhance the performance of this crucial operation, multiple techniques have been developed, utilizing well-known optimization approaches such as pattern rewriting and sharing common subexpressions. However, the scalability of these methods is limited by the high computation overhead, and the quality of the produced plans is compromised by ignoring significant parts of the solution space. In this paper, we present a novel framework for real-time multi-pattern complex event processing. Our approach is based on formulating the above task as a global optimization problem and applying a combination of sharing and pattern reordering techniques to construct an optimal plan satisfying the problem constraints. To the best of our knowledge, no such fusion was previously attempted in the field of CEP optimization. To locate the best possible evaluation plan in the resulting hyperexponential solution space, we design efficient local search algorithms that utilize the unique problem structure. An extensive theoretical and empirical analysis of our system demonstrates its superiority over state-of-the-art solutions.",
  "subtype": "SIGMOD Research",
  "authors": [
    {
      "name": "Ilya Kolchinsky",
      "affiliation": "Technion"
    },
    {
      "name": "Assaf Schuster",
      "affiliation": "Technion"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Research_385"
},
"SIGMOD_Research_392": {
  "title": "Hyperion: Building the Largest In-memory Search Tree",
  "abstract": "Indexes are essential in data management systems to increase the speed of data retrievals. Widespread data structures to provide fast and memory-efficient indexes are prefix tries. Implementations like Judy, ART, or HOT optimize their internal alignments for cache and vector unit efficiency. While these measures usually improve the performance substantially, they can have a negative impact on memory efficiency. In this paper we present Hyperion, a trie-based main-memory key-value store achieving extreme space efficiency. In contrast to other data structures, Hyperion does not depend on CPU vector units, but scans the data structure linearly. Combined with a custom memory allocator, Hyperion accomplishes a remarkable data density while achieving a competitive point query and an exceptional range query performance. Hyperion can significantly reduce the index memory footprint and its performance-to-memory ratio is more than two times better than the best implemented alternative strategy for randomized string data sets.",
  "subtype": "SIGMOD Research",
  "authors": [
    {
      "name": "Markus Mäsker",
      "affiliation": "Johannes Gutenberg University Mainz"
    },
    {
      "name": "Tim Süß",
      "affiliation": "University of Applied Science Fulda"
    },
    {
      "name": "Lars Nagel",
      "affiliation": "Loughborough University"
    },
    {
      "name": "Lingfang Zeng",
      "affiliation": "Huazhong University of Science and Technology"
    },
    {
      "name": "André Brinkmann",
      "affiliation": "Johannes Gutenberg University Mainz"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Research_392"
},
"SIGMOD_Research_396": {
  "title": "AI Meets AI: Leveraging Query Executions to Improve Index Recommendations",
  "abstract": "State-of-the-art index tuners rely on query optimizer's cost estimates to search for the index configuration with the largest estimated execution cost improvement`. Due to well-known limitations in optimizer's estimates, in a significant fraction of cases, an index estimated to improve a query's execution cost, e.g., CPU time, makes that worse when implemented. Such errors are a major impediment for automated indexing in production systems. We observe that comparing the execution cost of two plans of the same query corresponding to different index configurations is a key step during index tuning. Instead of using optimizer's estimates for such comparison, our key insight is that formulating it as a classification task in machine learning results in significantly higher accuracy. We present a study of the design space for this classification problem. We further show how to integrate this classifier into the state-of-the-art index tuners with minimal modifications, i.e., how artificial intelligence (AI) can benefit automated indexing (AI). Our evaluation using industry-standard benchmarks and a large number of real customer workloads demonstrates up to 5x reduction in the errors in identifying the cheaper plan in a pair, which eliminates almost all query execution cost regressions when the model is used in index tuning.",
  "subtype": "SIGMOD Research",
  "authors": [
    {
      "name": "Bailu Ding",
      "affiliation": "Microsoft Research"
    },
    {
      "name": "Sudipto Das",
      "affiliation": "Microsoft Research"
    },
    {
      "name": "Ryan Marcus",
      "affiliation": "Brandeis University"
    },
    {
      "name": "Wentao Wu",
      "affiliation": "Microsoft Research"
    },
    {
      "name": "Surajit Chaudhuri",
      "affiliation": "Microsoft Research"
    },
    {
      "name": "Vivek Narasayya",
      "affiliation": "Microsoft Research"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Research_396"
},
"SIGMOD_Research_403": {
  "title": "GPU-based Graph Traversal on Compressed Graphs",
  "abstract": "Graph processing on GPUs received much attention in the industry and the academia recently, as the hardware accelerator offers attractive potential for performance boost. However, the high-bandwidth device memory on GPUs has limited capacity that constrains the size of the graph to be loaded on chip. In this paper, we introduce GPU-based graph traversal on compressed graphs, so as to enable the processing of graphs having a larger size than the device memory. Designed towards GPU’s SIMT architecture, we propose two novel parallel scheduling strategies Two-Phase Traversal and Task-Stealing to handle thread divergence and workload imbalance issues when decoding the compressed graph. We further optimize our solution against power-law graphs by proposing Warp-centric Decoding and Residual Segmentation to facilitate parallelism on processing skewed out-degree distribution. Extensive experiments show that with 2x-18x compression rate, our proposed GPU-based graph traversal on compressed graphs (GCGT) achieves competitive efficiency compared with the state-of-the-art graph traversal approaches on non-compressed graphs.",
  "subtype": "SIGMOD Research",
  "authors": [
    {
      "name": "Mo Sha",
      "affiliation": "National University of Singapore"
    },
    {
      "name": "Yuchen Li",
      "affiliation": "Singapore Management University"
    },
    {
      "name": "Kian-Lee Tan",
      "affiliation": "National University of Singapore"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Research_403"
},
"SIGMOD_Research_406": {
  "title": "Mining Precision Interfaces From Query Logs",
  "abstract": "Interactive tools make data analysis more efficient and more accessible to end-users by hiding the underlying query complexity and exposing interactive widgets for the parts of the query that matter to the analysis. However, creating custom tailored (i.e., precise) interfaces is very costly, and automated approaches are desirable.  We propose a syntactic approach that uses queries from an analysis to generate a tailored interface. We model interface widgets as functions I(q) -> q' that modify the current analysis query q, and interfaces as the set of queries that its widgets can express.  Our system, Precision Interfaces, analyzes structural changes between input queries from an analysis, and generates an output interface with widgets to express those changes. Our experiments on the Sloan Digital Sky Survey query log suggest that Precision Interfaces can generate useful interfaces for simple unanticipated tasks, and our optimizations can generate interfaces from logs of up to 10,000 queries in <10s.",
  "subtype": "SIGMOD Research",
  "authors": [
    {
      "name": "Qianrui Zhang",
      "affiliation": "Tsinghua University"
    },
    {
      "name": "Haoci Zhang",
      "affiliation": "Columbia University"
    },
    {
      "name": "Thibault Sellam",
      "affiliation": "Columbia University"
    },
    {
      "name": "Eugene Wu",
      "affiliation": "Columbia University"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Research_406"
},
"SIGMOD_Research_410": {
  "title": "DBEst: Revisiting Approximate Query Processing Engines with Machine Learning Models",
  "abstract": "In the era of big data, computing exact answers to analytical queries becomes prohibitively expensive. This greatly increases the value of approaches that can compute efficiently approximate, but highly-accurate, answers to analytical queries. Alas, the state of the art still suffers from many shortcomings: Errors are still high unless large memory investments are made. Many important analytics tasks are not supported. Query response times are too long and thus approaches rely on parallel execution of queries atop large big data analytics clusters, in-situ or in the cloud, whose acquisition/use costs dearly. Hence, the following questions are crucial: Can we develop AQP engines that reduce response times by orders of magnitude, ensure high accuracy, and support most aggregate functions? With smaller memory footprints and small overheads to build the state upon which they are based? With this paper, we show that the answers to all questions above can be positive. The paper presents DBEst, a system based on Machine Learning models (regression models and probability density estimators). It will discuss its limitations, promises, and how it can complement existing systems. It will substantiate its advantages using queries and data from the TPC-DS benchmark and real-life datasets, compared against state of the art AQP engines.",
  "subtype": "SIGMOD Research",
  "authors": [
    {
      "name": "Qingzhi Ma",
      "affiliation": "University of Warwick"
    },
    {
      "name": "Peter Triantafillou",
      "affiliation": "University of Warwick"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Research_410"
},
"SIGMOD_Research_411": {
  "title": "PRSim: Sublinear Time SimRank Computation on Large Power-Law Graphs",
  "abstract": "it SimRank is a classic measure of the similarities of nodes in a graph. Given a node u in graph G =(V, E), a single-source SimRank query returns the SimRank similarities s(u, v) between node u and each node v in V. This type of queries has numerous applications in web search and social networks analysis, such as link prediction, web mining, and spam detection. Existing methods for single-source SimRank queries, however, incur query cost at least linear to the number of nodes n, which renders them inapplicable for real-time and interactive analysis. This paper proposes prsim, an algorithm that exploits the structure of graphs to efficiently answer single-source SimRank queries. prsim uses an index of size O(m), where m is the number of edges in the graph, and guarantees a query time that depends on the reverse PageRank distribution of the input graph. In particular, we prove that prsim runs in sub-linear time if the degree distribution of the input graph follows the power-law distribution, a property possessed by many real-world graphs. Based on the theoretical analysis, we show that the empirical query time of all existing SimRank algorithms also depends on the reverse PageRank distribution of the graph. Finally, we present the first experimental study that evaluates the absolute errors of various SimRank algorithms on large graphs, and we show that prsim outperforms the state of the art in terms of query time, accuracy, index size, and scalability.",
  "subtype": "SIGMOD Research",
  "authors": [
    {
      "name": "Zhewei Wei",
      "affiliation": "Renmin University of China"
    },
    {
      "name": "Xiaodong He",
      "affiliation": "4Paradigm Inc."
    },
    {
      "name": "Xiaokui Xiao",
      "affiliation": "National University of Singapore"
    },
    {
      "name": "Sibo Wang",
      "affiliation": "The Chinese University of Hong Kong"
    },
    {
      "name": "Yu Liu",
      "affiliation": "Peking University"
    },
    {
      "name": "Xiaoyong Du",
      "affiliation": "Renmin University of China"
    },
    {
      "name": "Ji-Rong Wen",
      "affiliation": "Renmin University of China"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Research_411"
},
"SIGMOD_Research_429": {
  "title": "Incremental and Approximate Inference for Faster Occlusion-based Deep CNN Explanations",
  "abstract": "Deep Convolutional Neural Networks (CNNs) now match human accuracy in many image prediction tasks, resulting in a growing adoption in e-commerce, radiology, and other domains. Naturally, explaining CNN predictions is a key concern for many users. Since the internal workings of CNNs are unintuitive for most users, occlusion-based explanations (OBE) are popular for understanding which parts of an image matter most for a prediction. One occludes a region of the image using a patch and moves it around to produce a heat map of changes to the prediction probability. Alas, this approach is computationally expensive due to the large number of re-inference requests produced, which wastes time and raises resource costs. We tackle this issue by casting the OBE task as a new instance of the classical incremental view maintenance problem. We create a novel and comprehensive algebraic framework for incremental CNN inference combining materialized views with multi-query optimization to reduce computational costs. We then present two novel approximate inference optimizations that exploit the semantics of CNNs and the OBE task to further reduce runtimes. We prototype our ideas in Python to create a tool we call Krypton that supports both CPUs and GPUs. Experiments with real data and CNNs show that Krypton reduces runtimes by up to 5X (resp. 35X) to produce exact (resp. high-quality approximate) results without raising resource requirements.",
  "subtype": "SIGMOD Research",
  "authors": [
    {
      "name": "Supun Nakandala",
      "affiliation": "University of California, San Diego"
    },
    {
      "name": "Arun Kumar",
      "affiliation": "University of California, San Diego"
    },
    {
      "name": "Yannis Papakonstantinou",
      "affiliation": "University of California, San Diego"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Research_429"
},
"SIGMOD_Research_432": {
  "title": "Fractal: A General-Purpose Graph Pattern Mining System",
  "abstract": "In this paper we propose Fractal, a high performance and high productivity system for supporting distributed graph pattern mining (GPM) applications. Fractal employs a dynamic (auto-tuned) load-balancing based on a hierarchical and locality-aware work stealing mechanism, allowing the system to adapt to different workload characteristics. Additionally, Fractal enumerates subgraphs by combining a depth-first strategy with a from scratch processing paradigm to avoid storing large amounts of intermediate state and, thus, improves memory efficiency. Regarding programmer productivity, Fractal presents an intuitive, expressive and modular API, allowing for rapid compositional expression of many GPM algorithms. Fractal-based implementations outperform both existing systemic solutions and specialized distributed solutions on many problems - from frequent graph mining to subgraph querying, over a range of datasets.",
  "subtype": "SIGMOD Research",
  "authors": [
    {
      "name": "Vinicius Dias",
      "affiliation": "Universidade Federal de Minas Gerais"
    },
    {
      "name": "Carlos Teixeira",
      "affiliation": "Universidade Federal de Minas Gerais"
    },
    {
      "name": "Dorgival Guedes",
      "affiliation": "Universidade Federal de Minas Gerais"
    },
    {
      "name": "Wagner Meira",
      "affiliation": "Universidade Federal de Minas Gerais"
    },
    {
      "name": "Srinivasan Parthasarathy",
      "affiliation": "Ohio State University"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Research_432"
},
"SIGMOD_Research_436": {
  "title": "Anti-Freeze for Large and Complex Spreadsheets: Asynchronous Formula Computation",
  "abstract": "Spreadsheet systems enable users to store and analyze data in an intuitive and flexible interface. Yet the scale of data being analyzed often leads to spreadsheets hanging and freezing on small changes. We propose a new asynchronous formula computation framework: instead of freezing the interface we return control to users quickly to ensure interactivity, while computing the formulae in the background. To ensure consistency, we indicate formulae being computed in the background via visual cues on the spreadsheet. Our asynchronous computation framework introduces two novel challenges: (a) How do we identify dependencies for a given change in a bounded time? (b) How do we schedule computation to maximize the number of spreadsheet cells available to the user over time? We bound the dependency identification time by compressing the formula dependency graph lossily, a problem we show to be NP-Hard. A compressed dependency table enables us to quickly identify the spreadsheet cells that need recomputation and indicate them as such to users. Finding an optimal computation schedule to maximize cell availability is also NP-Hard, and even merely obtaining a schedule can be expensive—we propose an on-the-fly scheduling technique to address this. We have incorporated asynchronous computation in DataSpread, a scalable spreadsheet system targeted at operating on arbitrarily large datasets on a spreadsheet frontend.",
  "subtype": "SIGMOD Research",
  "authors": [
    {
      "name": "Mangesh Bendre",
      "affiliation": "University of Illinois Urbana-Champaign"
    },
    {
      "name": "Tana Wattanawaroon",
      "affiliation": "University of Illinois Urbana-Champaign"
    },
    {
      "name": "Kelly Mack",
      "affiliation": "University of Illinois Urbana-Champaign"
    },
    {
      "name": "Kevin Chang",
      "affiliation": "University of Illinois Urbana-Champaign"
    },
    {
      "name": "Aditya Parameswaran",
      "affiliation": "University of Illinois Urbana-Champaign"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Research_436"
},
"SIGMOD_Research_442": {
  "title": "RaSQL: Greater Power and Performance for Big Data Analytics with Recursive-aggregate-SQL on Spark",
  "abstract": "Thanks to a simple SQL extension, Recursive-aggregate-SQL (RaSQL) can express very powerful queries and declarative algorithms, such as classical graph algorithms and data mining algorithms. A novel compiler implementation allows RaSQL to map declarative queries into one basic fixpoint operator supporting aggregates in recursive queries. A fully optimized implementation of this fixpoint operator leads to superior performance, scalability and portability. Thus, our RaSQL system, which extends Spark SQL with the before-mentioned new constructs and implementation techniques, matches and often surpasses the performance of other systems, including Apache Giraph, GraphX and Myria.",
  "subtype": "SIGMOD Research",
  "authors": [
    {
      "name": "Jiaqi Gu",
      "affiliation": "University of California, Los Angeles"
    },
    {
      "name": "Yugo Watanabe",
      "affiliation": "University of California, Los Angeles"
    },
    {
      "name": "William Mazza",
      "affiliation": "University of Naples Federico II"
    },
    {
      "name": "Alexander Shkapsky",
      "affiliation": "Workday, Inc."
    },
    {
      "name": "Mohan Yang",
      "affiliation": "Google"
    },
    {
      "name": "Ling Ding",
      "affiliation": "University of California, Los Angeles"
    },
    {
      "name": "Carlo Zaniolo",
      "affiliation": "University of California, Los Angeles"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Research_442"
},
"SIGMOD_Research_443": {
  "title": "Scaling Distance Labeling on Small-World Networks",
  "abstract": "Distance labeling approaches are widely adopted to speed up the online performance of shortest distance queries. The construction of the distance labeling, however, can be exhaustive especially on big graphs. For a major category of large graphs, small-world networks, the state-of-the-art approach is Pruned Landmark Labeling (PLL). PLL prunes distance labels based on a node order and directly constructs the pruned labels by performing breadth-first searches in the node order. The pruning technique, as well as the index construction, has a strong sequential nature which hinders PLL from being parallelized. It becomes an urgent issue on massive small-world networks whose index can hardly be constructed by a single thread within a reasonable time. This paper scales distance labeling on small-world networks by proposing a Parallel Shortest-distance Labeling (PSL) scheme and further reducing the index size by exploiting graph and label properties. PSL insightfully converts the PLL's node-order dependency to a shortest-distance dependence, which leads to a propagation-based parallel labeling in D rounds where D denotes the diameter of the graph. Extensive experimental results verify our efficiency on billion-scale graphs and near-linear speedup in a multi-core environment.",
  "subtype": "SIGMOD Research",
  "authors": [
    {
      "name": "Wentao Li",
      "affiliation": "University of Technology Sydney"
    },
    {
      "name": "Miao Qiao",
      "affiliation": "University of Auckland"
    },
    {
      "name": "Lu Qin",
      "affiliation": "University of Technology Sydney"
    },
    {
      "name": "Ying Zhang",
      "affiliation": "University of Technology Sydney"
    },
    {
      "name": "Lijun Chang",
      "affiliation": "University of Sydney"
    },
    {
      "name": "Xuemin Lin",
      "affiliation": "University of New South Wales"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Research_443"
},
"SIGMOD_Research_447": {
  "title": "Enabling and Optimizing Non-linear Feature Interactions in Factorized Linear Algebra",
  "abstract": "Accelerating machine learning (ML) over relational data is a key focus of the database community. While many real-world datasets are multi-table, most ML tools expect single-table inputs, forcing users to materialize joins before ML, leading to data redundancy and runtime waste. Recent works on 'factorized ML' address such issues by pushing ML through joins. However, they have hitherto been restricted to ML models linear in the feature space, rendering them less effective when users construct non-linear feature interactions such as pairwise products to boost ML accuracy. In this work, we take a first step towards closing this gap by introducing a new abstraction to enable pairwise feature interactions in multi-table data and present an extensive framework of algebraic rewrite rules for factorized LA operators over feature interactions. Our rewrite rules carefully exploit the interplay of the redundancy caused by both joins and interactions. We prototype our framework in Python to build a tool we call MorpheusFI. An extensive empirical evaluation with both synthetic and real datasets shows that MorpheusFI yields up to 5x speedups over materialized execution for a popular second-order gradient method and even an order of magnitude speedups over a popular stochastic gradient method.",
  "subtype": "SIGMOD Research",
  "authors": [
    {
      "name": "Side Li",
      "affiliation": "University of California, San Diego"
    },
    {
      "name": "Lingjiao Chen",
      "affiliation": "University of Wisconsin, Madison"
    },
    {
      "name": "Arun Kumar",
      "affiliation": "University of California, San Diego"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Research_447"
},
"SIGMOD_Research_449": {
  "title": "Maximizing Welfare in Social Networks under A Utility Driven Influence Diffusion model",
  "abstract": "Motivated by applications such as viral marketing, the problem of influence maximization (IM) has been extensively studied in the literature. The goal is to select a small number of users to adopt an item such that it results in a large cascade of adoptions by others. Existing works have three key limitations. (1) They do not account for economic considerations of a user in buying/adopting items. (2) Most studies on multiple items focus on competition, with complementary items receiving limited attention. (3) For the network owner, maximizing social welfare is important to ensure customer loyalty, which is not addressed in prior work in the IM literature. In this paper, we address all three limitations and propose a novel model called UIC that combines utility-driven item adoption with influence propagation over networks. Focusing on the mutually complementary setting, we formulate the problem of social welfare maximization in this novel setting. We show that while the objective function is neither submodular nor supermodular, surprisingly a simple greedy allocation algorithm achieves a factor of (1-1/e-epsilon) of the optimum expected social welfare. We develop textsfbundleGRD, a scalable version of this approximation algorithm, and demonstrate, with comprehensive experiments on real and synthetic datasets, that it significantly outperforms all baselines.",
  "subtype": "SIGMOD Research",
  "authors": [
    {
      "name": "Prithu Banerjee",
      "affiliation": "University of British Columbia"
    },
    {
      "name": "Wei Chen",
      "affiliation": "Microsoft Research"
    },
    {
      "name": "Laks Lakshmanan",
      "affiliation": "University of British Columbia"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Research_449"
},
"SIGMOD_Research_450": {
  "title": "Efficient Subgraph Matching: Harmonizing Dynamic Programming, Adaptive Matching Order, and Failing Set Together",
  "abstract": "Subgraph matching (or subgraph isomorphism) is one of the fundamental problems in graph analysis. Extensive research has been done to develop practical solutions for subgraph matching. The state-of-the-art algorithms such as textsfCFL-Match and textsfTurbotextsubscriptiso convert a query graph into a spanning tree for obtaining candidates for each query vertex and obtaining a good matching order with the spanning tree. However, by using the spanning tree instead of the original query graph, it could lead to lower pruning power and a sub-optimal matching order. Another limitation is that they perform redundant computation in search without utilizing the knowledge learned from past computation. In this paper, we introduce three novel concepts to address these inherent limitations: 1) dynamic programming between a directed acyclic graph (DAG) and a graph, 2) adaptive matching order with DAG ordering, and 3) pruning by failing sets, which together lead to a much faster algorithm textsfDAF for subgraph matching. Extensive experiments with real datasets show that textsfDAF outperforms the fastest existing solution by up to orders of magnitude in terms of recursive calls as well as in terms of the elapsed time.",
  "subtype": "SIGMOD Research",
  "authors": [
    {
      "name": "Myoungji Han",
      "affiliation": "Seoul National University"
    },
    {
      "name": "Hyunjoon Kim",
      "affiliation": "Seoul National University"
    },
    {
      "name": "Geonmo Gu",
      "affiliation": "Seoul National University"
    },
    {
      "name": "Kunsoo Park",
      "affiliation": "Seoul National University"
    },
    {
      "name": "Wook-Shin Han",
      "affiliation": "Pohang University of Science and Technology (POSTECH)"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Research_450"
},
"SIGMOD_Research_457": {
  "title": "Efficient Approximation Algorithms for Adaptive Seed Minimization",
  "abstract": "As a dual problem of influence maximization, the seed minimization problem asks for the minimum number of seed nodes to influence a required number eta of users in a given social network G. Existing algorithms for seed minimization mostly consider the it non-adaptive setting, where all seed nodes are selected in one batch without observing how they may influence other users. In this paper, we study seed minimization in the it adaptive setting, where the seed nodes are selected in several batches, such that the choice of a batch may exploit information about the actual influence of the previous batches. We propose a novel algorithm, it ASTI, which addresses the adaptive seed minimization problem in OBig(fraceta cdot (m+n)varepsilon^2ln n Big) expected time and offers an approximation guarantee of frac(ln eta+1)^2(1 - (1-1/b)^b) (1-1/e)(1-varepsilon) in expectation, where eta is the targeted number of influenced nodes, b is size of each seed node batch, and varepsilon in (0, 1) is a user-specified parameter. To the best of our knowledge, ASTI is the first algorithm that provides such an approximation guarantee without incurring prohibitive computation overhead. With extensive experiments on a variety of datasets, we demonstrate the effectiveness and efficiency of ASTI over competing methods.",
  "subtype": "SIGMOD Research",
  "authors": [
    {
      "name": "Jing Tang",
      "affiliation": "National University of Singapore"
    },
    {
      "name": "Keke Huang",
      "affiliation": "Nanyang Technological University"
    },
    {
      "name": "Xiaokui Xiao",
      "affiliation": "National University of Singapore"
    },
    {
      "name": "Laks Lakshmanan",
      "affiliation": "University of British Columbia"
    },
    {
      "name": "Xueyan Tang",
      "affiliation": "Nanyang Technological University"
    },
    {
      "name": "Aixin Sun",
      "affiliation": "Nanyang Technological University"
    },
    {
      "name": "Andrew Lim",
      "affiliation": "National University of Singapore"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Research_457"
},
"SIGMOD_Research_465": {
  "title": "Efficiently Answering Regular Simple Path Queries on Large Labeled Networks",
  "abstract": "A fundamental query in labeled graphs is to determine if there exists a path between a given source and target vertices, such that the path satisfies a given label constraint. One of the powerful forms of specifying label constraints is through regular expressions, and the resulting problem of reachability queries under regular simple paths (RSP) form the core of many practical graph query languages such as SPARQL from W3C, Cypher of Neo4J, Oracle's PGQL and LDBC's G-CORE. Despite its importance, since it is known that answering RSP queries is NP-Hard, there are no scalable and practical solutions for answering reachability with full-range of regular expressions as constraints. In this paper, we circumvent this computational bottleneck by designing a random-walk based sampling algorithm called ARRIVAL, which is backed by theoretical guarantees on its expected quality. Extensive experiments on billion-sized real graph datasets with thousands of labels show that ARRIVAL to be 100 times faster than baseline strategies with an average accuracy of 95%.",
  "subtype": "SIGMOD Research",
  "authors": [
    {
      "name": "Sarisht Wadhwa",
      "affiliation": "IIT Delhi"
    },
    {
      "name": "Anagh Prasad",
      "affiliation": "IIT Delhi"
    },
    {
      "name": "Sayan Ranu",
      "affiliation": "IIT Delhi"
    },
    {
      "name": "Amitabha Bagchi",
      "affiliation": "IIT Delhi"
    },
    {
      "name": "Srikanta Bedathur",
      "affiliation": "IIT Delhi"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Research_465"
},
"SIGMOD_Research_482": {
  "title": "Iterative Query Processing based on Unified Optimization Techniques",
  "abstract": "Hybrid transactional and analytical processing (HTAP) systems like SAP HANA make it much simpler to manage both operational load and analytical queries without ETL, separate data warehouses, et al. To represent both transactional and analytical business logic in a single database system, stored procedures are often used to express analytical queries using control flow logic and DMLs. Optimizing these complex procedures requires a fair knowledge of imperative programming languages as well as the declarative query language. Therefore, unified optimization techniques considering both program and query optimization techniques are essential for achieving optimal query performance. In this paper, we propose a novel unified optimization technique for efficient iterative query processing. We present a notion of query motion that allows the movement of SQL queries in and out of a loop. Additionally, we exploit a new cost model that measures the quality of the execution plan with consideration for queries and loop iterations. We describe our experimental evaluation that demonstrates the benefit of our technique using both a standard decision support benchmark and real-world workloads. An extensive evaluation shows that our unified optimization technique enumerates plans that achieve performance improvements of up to an order of magnitude faster than plans generated by the existing loop-invariant code motion technique.",
  "subtype": "SIGMOD Research",
  "authors": [
    {
      "name": "Kisung Park",
      "affiliation": "Kyung Hee University"
    },
    {
      "name": "Hojin Seo",
      "affiliation": "Kyung Hee University"
    },
    {
      "name": "Mostofa Rasel",
      "affiliation": "Kyung Hee University"
    },
    {
      "name": "Young-Koo Lee",
      "affiliation": "Kyung Hee University"
    },
    {
      "name": "Chanho Jeong",
      "affiliation": "SAP Labs Korea"
    },
    {
      "name": "Sung Yeol Lee",
      "affiliation": "SAP Labs Korea"
    },
    {
      "name": "Chungmin Lee",
      "affiliation": "SAP Labs Korea"
    },
    {
      "name": "Dong-Hun Lee",
      "affiliation": "SAP Labs Korea"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Research_482"
},
"SIGMOD_Research_485": {
  "title": "Blurring the Lines between Blockchains and Database Systems: the Case of Hyperledger Fabric",
  "abstract": "Within the last few years, a countless number of blockchain systems have emerged on the market, each one claiming to revolutionize the way of distributed transaction processing in one way or the other. Many blockchain features, such as byzantine fault tolerance, are indeed valuable additions in modern environments. However, despite all the hype around the technology, many of the challenges that blockchain systems have to face are fundamental transaction management problems. These are largely shared with traditional database systems, which have been around for decades already. These similarities become especially visible for systems, that blur the lines between blockchain systems and classical database systems. A great example of this is Hyperledger Fabric, an open-source permissioned blockchain system under development by IBM. By implementing parallel transaction processing, Fabric's workflow is highly motivated by optimistic concurrency control mechanisms in classical database systems. This raises two questions: (1)~Which conceptual similarities and differences do actually exist between a system such as Fabric and a classical distributed database system? (2)~Is it possible to improve on the performance of Fabric by transitioning technology from the database world to blockchains and thus blurring the lines between these two types of systems even further?  To tackle these questions, we first explore Fabric from the perspective of database research, where we observe weaknesses in the transaction pipeline. We then solve these issues by transitioning well-understood database concepts to Fabric, namely transaction reordering as well as early transaction abort. Our experimental evaluation under the Smallbank benchmark as well as under a custom workload shows that our improved version Fabric++ significantly increases the throughput of successful transactions over the vanilla version by up to a factor of 12x, while decreasing the average latency to almost half.",
  "subtype": "SIGMOD Research",
  "authors": [
    {
      "name": "Ankur Sharma",
      "affiliation": "Saarland University"
    },
    {
      "name": "Felix Schuhknecht",
      "affiliation": "Saarland University"
    },
    {
      "name": "Divya Agrawal",
      "affiliation": "Saarland University"
    },
    {
      "name": "Jens Dittrich",
      "affiliation": "Saarland University"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Research_485"
},
"SIGMOD_Research_501": {
  "title": "AStream: Ad-hoc Shared Stream Processing",
  "abstract": "In the last decade, many distributed stream processing engines (SPEs) were developed to perform continuous queries on massive online data. The central design principle of these engines is to handle queries that potentially run forever on data streams with a query-at-a-time model, i.e., each query is optimized and executed separately. In many real applications, streams are not only processed with long-running queries, but also thousands of short-running ad-hoc queries. To support this efficiently, it is essential to share resources and computation for stream ad-hoc queries in a multi-user environment. The goal of this paper is to bridge the gap between stream processing and ad-hoc queries in SPEs by sharing computation and resources. We define three main requirements for ad-hoc shared stream processing: (1) Integration: Ad-hoc query processing should be a composable layer which can extend stream operators, such as join, aggregation, and window operators; (2) Consistency: Ad-hoc query creation and deletion must be performed in a consistent manner and ensure exactly-once semantics and correctness; (3) Performance: In contrast to state-of-the-art SPEs, ad-hoc SPE should not only maximize data throughput but also query throughout via incremental computation and resource sharing. Based on these requirements, we have developed AStream, an ad-hoc, shared computation stream processing framework. To the best of our knowledge, AStream is the first system that supports distributed ad-hoc stream processing. AStream is built on top of Apache Flink. Our experiments show that AStream shows comparable results to Flink for single query deployments and outperforms it in orders of magnitude with multiple queries.",
  "subtype": "SIGMOD Research",
  "authors": [
    {
      "name": "Jeyhun Karimov",
      "affiliation": "DFKI GmbH"
    },
    {
      "name": "Tilmann Rabl",
      "affiliation": "DFKI GmbH & TU Berlin"
    },
    {
      "name": "Volker Markl",
      "affiliation": "DFKI GmbH & TU Berlin"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Research_501"
},
"SIGMOD_Research_527": {
  "title": "Interactive Graph Search",
  "abstract": "We study interactive graph search (IGS), with the conceptual objective of departing from the conventional 'top-down' strategy in searching a poly-hierarchy, a.k.a. a decision graph. In IGS, a machine assists a human in looking for a target node z in an acyclic directed graph G, by repetitively asking questions. In each question, the machine picks a node u in G, asks a human ' is there a path from u to z?', and takes a boolean answer from the human. The efficiency goal is to locate z with as few questions as possible. We describe algorithms that solve the problem by asking a provably small number of questions, and establish lower bounds indicating that the algorithms are optimal up to a small additive factor. An experimental evaluation is presented to demonstrate the usefulness of our solutions in real-world scenarios.",
  "subtype": "SIGMOD Research",
  "authors": [
    {
      "name": "Yufei Tao",
      "affiliation": "Chinese University of Hong Kong"
    },
    {
      "name": "Yuanbing Li",
      "affiliation": "Tsinghua University"
    },
    {
      "name": "Guoliang Li",
      "affiliation": "Tsinghua University"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Research_527"
},
"SIGMOD_Research_531": {
  "title": "Efficient Estimation of Heat Kernel PageRank for Local Clustering",
  "abstract": "Given an undirected graph G and a seed node s, the local clustering problem aims to identify a high-quality cluster containing s in time roughly proportional to the size of the cluster, regardless of the size of G. This problem finds numerous applications on large-scale graphs. Recently, heat kernel PageRank (HKPR), which is a measure of the proximity of nodes in graphs, is applied to this problem and found to be more efficient compared with prior methods. However, existing solutions for computing HKPR either are prohibitively expensive or provide unsatisfactory error approximation on HKPR values, rendering them impractical especially on billion-edge graphs. In this paper, we present TEA and TEA+, two novel local graph clustering algorithms based on HKPR, to address the aforementioned limitations. Specifically, these algorithms provide non-trivial theoretical guarantees in relative error of HKPR values and the time complexity. The basic idea is to utilize deterministic graph traversal to produce a rough estimation of exact HKPR vector, and then exploit Monte-Carlo random walks to refine the results in an optimized and non-trivial way. In particular, TEA+ offers practical efficiency and effectiveness due to non-trivial optimizations. Extensive experiments on real-world datasets demonstrate that TEA+ outperforms the state-of-the-art algorithm by more than four times on most benchmark datasets in terms of computational time when achieving the same clustering quality, and in particular, is an order of magnitude faster on large graphs including the widely studied Twitter and Friendster datasets.",
  "subtype": "SIGMOD Research",
  "authors": [
    {
      "name": "Renchi Yang",
      "affiliation": "Nanyang Technological University"
    },
    {
      "name": "Xiaokui Xiao",
      "affiliation": "National University of Singapore"
    },
    {
      "name": "Zhewei Wei",
      "affiliation": "Renmin University of China"
    },
    {
      "name": "Sourav Bhowmick",
      "affiliation": "Nanyang Technological University"
    },
    {
      "name": "Jun Zhao",
      "affiliation": "Nanyang Technological University"
    },
    {
      "name": "Rong-Hua Li",
      "affiliation": "Beijing Institute of Technology"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Research_531"
},
"SIGMOD_Research_534": {
  "title": "Uncertainty Annotated Databases - A Lightweight Approach for Approximating Certain Answers",
  "abstract": "Certain answers are a principled method for coping with uncertainty that arises in many practical data management tasks. Unfortunately, this method is expensive and may ex- clude useful (if uncertain) answers. Thus, users frequently resort to less principled approaches to resolve uncertainty. In this paper, we propose Uncertainty Annotated Databases (UA-DBs), which combine an under- and over-approximation of certain answers to achieve the reliability of certain answers, with the performance of a classical database system. Furthermore, in contrast to prior work on certain answers, UA-DBs achieve a higher utility by including some (explicitly marked) answers that are not certain. UA-DBs are based on incomplete K-relations, which we introduce to generalize the classical set-based notion of incomplete databases and certain answers to a much larger class of data models. Using an implementation of our approach, we demonstrate experimentally that it efficiently produces tight approximations of certain answers that are of high utility.",
  "subtype": "SIGMOD Research",
  "authors": [
    {
      "name": "Su Feng",
      "affiliation": "Illinois Institute of Technology"
    },
    {
      "name": "Aaron Huber",
      "affiliation": "University at Buffalo"
    },
    {
      "name": "Boris Glavic",
      "affiliation": "Illinois Institute of Technology"
    },
    {
      "name": "Oliver Kennedy",
      "affiliation": "University at Buffalo"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Research_534"
},
"SIGMOD_Research_539": {
  "title": "HoloDetect: Few-Shot Learning for Error Detection",
  "abstract": "We introduce a few-shot learning framework for error detection. We show that data augmentation (a form of weak supervision) is key to training high-quality, ML-based error detection models that require minimal human involvement. Our framework consists of two parts: (1) an expressive model to learn rich representations that capture the inherent syntactic and semantic heterogeneity of errors; and (2) a data augmentation model that, given a small seed of clean records, uses dataset-specific transformations to automatically generate additional training data. Our key insight is to learn data augmentation policies from the noisy input dataset in a weakly supervised manner. We show that our framework detects errors with an average precision of ~94% and an average recall of ~93% across a diverse array of datasets that exhibit different types and amounts of errors. We compare our approach to a comprehensive collection of error detection methods, ranging from traditional rule-based methods to ensemble-based and active learning approaches. We show that data augmentation yields an average improvement of 20 F1 points while it requires access to 3x fewer labeled examples compared to other ML approaches.",
  "subtype": "SIGMOD Research",
  "authors": [
    {
      "name": "Alireza Heidari",
      "affiliation": "University of Waterloo"
    },
    {
      "name": "Joshua McGrath",
      "affiliation": "University of Wisconsin, Madison"
    },
    {
      "name": "Ihab Ilyas",
      "affiliation": "University of Waterloo"
    },
    {
      "name": "Theodoros Rekatsinas",
      "affiliation": "University of Wisconsin, Madison"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Research_539"
},
"SIGMOD_Research_543": {
  "title": "Towards Scaling Blockchain Systems via Sharding",
  "abstract": "Existing blockchain systems scale poorly because of their distributed consensus protocols. Current attempts at improving blockchain scalability are limited to cryptocurrency. Scaling blockchain systems under general workloads (i.e., non-cryptocurrency applications) remains an open question.  This work takes a principled approach to apply sharding to blockchain systems in order to improve their transaction throughput at scale. This is challenging, however, due to the fundamental difference in failure models between databases and blockchain. To achieve our goal, we first enhance the performance of Byzantine consensus protocols, improving individual shards' throughput. Next, we design an efficient shard formation protocol that securely assigns nodes into shards. We rely on trusted hardware, namely Intel SGX, to achieve high performance for both consensus and shard formation protocol. Third, we design a general distributed transaction protocol that ensures safety and liveness even when transaction coordinators are malicious. Finally, we conduct an extensive evaluation of our design both on a local cluster and on Google Cloud Platform. The results show that our consensus and shard formation protocols outperform state-of-the-art solutions at scale. More importantly, our sharded blockchain reaches a high throughput that can handle Visa-level workloads, and is the largest ever reported in a realistic environment.",
  "subtype": "SIGMOD Research",
  "authors": [
    {
      "name": "Hung Dang",
      "affiliation": "National University of Singapore"
    },
    {
      "name": "Tien Tuan Anh Dinh",
      "affiliation": "National University of Singapore"
    },
    {
      "name": "Dumitrel Loghin",
      "affiliation": "National University of Singapore"
    },
    {
      "name": "Ee-Chien Chang",
      "affiliation": "National University of Singapore"
    },
    {
      "name": "Qian Lin",
      "affiliation": "National University of Singapore"
    },
    {
      "name": "Beng Chin Ooi",
      "affiliation": "National University of Singapore"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Research_543"
},
"SIGMOD_Research_560": {
  "title": "A Layered Aggregate Engine for Analytics Workloads",
  "abstract": "This paper introduces LMFAO (Layered Multiple Functional Aggregate Optimization), an in-memory optimization and execution engine for batches of aggregates over the input database. The primary motivation for this work stems from the observation that for a variety of analytics over databases, their data-intensive tasks can be decomposed into group-by aggregates over the join of the input database relations. We exemplify the versatility and competitiveness of LMFAO for a handful of widely used analytics: learning ridge linear regression, classification trees, regression trees, and the structure of Bayesian networks using Chow-Liu trees; and data cubes used for exploration in data warehousing. LMFAO consists of several layers of logical and code optimizations that systematically exploit sharing of computation, parallelism, and code specialization. We conducted two types of performance benchmarks. In experiments with four datasets, LMFAO outperforms by several orders of magnitude on one hand, a commercial database system and MonetDB for computing batches of aggregates, and on the other hand, TensorFlow, Scikit, R, and AC/DC for learning a variety of models over databases.",
  "subtype": "SIGMOD Research",
  "authors": [
    {
      "name": "Maximilian Schleich",
      "affiliation": "University of Oxford"
    },
    {
      "name": "Dan Olteanu",
      "affiliation": "University of Oxford"
    },
    {
      "name": "Mahmoud Abo Khamis",
      "affiliation": "RelationalAI"
    },
    {
      "name": "Hung Ngo",
      "affiliation": "RelationalAI"
    },
    {
      "name": "XuanLong Nguyen",
      "affiliation": "University of Michigan"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Research_560"
},
"SIGMOD_Research_565": {
  "title": "Answering Why-questions by Exemplars in Attributed Graphs",
  "abstract": "This paper studies the problem of answering Why-questions for graph pattern queries.  Given a query Q, its answers Q(G) in a graph G, and an exemplar E that describes desired answers, it aims to compute a query rewrite Q', such that Q'(G) incorporates relevant entities and excludes irrelevant ones wrt E under a closeness measure. (1) We characterize the problem by Q-Chase. It rewrites Q by applying a sequence of applicable operators guided by E, and backtracks to derive optimal query rewrite. (2) We develop feasible Q-Chase-based algorithms, from anytime solutions to fixed-parameter approximations to compute query rewrites. These algorithms implement Q-Chase by detecting picky operators at run time, which discriminately enforce E to retain answers that are closer to exemplars, and effectively prune both operators and irrelevant matches, by consulting a cache of star patterns (called star views). Using real-world graphs, we experimentally verify the efficiency and effectiveness of qchase techniques and their applications.",
  "subtype": "SIGMOD Research",
  "authors": [
    {
      "name": "Mohammad Hossein Namaki",
      "affiliation": "Washington State University"
    },
    {
      "name": "Qi Song",
      "affiliation": "Washington State University"
    },
    {
      "name": "Yinghui Wu",
      "affiliation": "Washington State University"
    },
    {
      "name": "Shengqi Yang",
      "affiliation": "WeWork Technology"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Research_565"
},
"SIGMOD_Research_566": {
  "title": "Answering Multi-Dimensional Analytical Queries under Local Differential Privacy",
  "abstract": "Multi-dimensional analytical (MDA) queries are often issued against a fact table with predicates on (categorical or ordinal) dimensions and aggregations on one or more measures. In this paper, we study the problem of answering MDA queries under local differential privacy (LDP). In the absence of a trusted agent, sensitive dimensions are encoded in a privacy-preserving (LDP) way locally before being sent to the data collector. The data collector estimates the answers to MDA queries, based on the encoded dimensions. We propose several LDP encoders and estimation algorithms, to handle a large class of MDA queries with different types of predicates and aggregation functions. Our techniques are able to answer these queries with tight error bounds and scale well in high-dimensional settings (i.e., error is polylogarithmic in dimension sizes). We conduct experiments on real and synthetic data to verify our theoretical results, and compare our solution with marginal-estimation based solutions.",
  "subtype": "SIGMOD Research",
  "authors": [
    {
      "name": "Tianhao Wang",
      "affiliation": "Purdue University"
    },
    {
      "name": "Bolin Ding",
      "affiliation": "Alibaba Group"
    },
    {
      "name": "Jingren Zhou",
      "affiliation": "Alibaba Group"
    },
    {
      "name": "Cheng Hong",
      "affiliation": "Alibaba Group"
    },
    {
      "name": "Zhicong Huang",
      "affiliation": "Alibaba Group"
    },
    {
      "name": "Ninghui Li",
      "affiliation": "Purdue University"
    },
    {
      "name": "Somesh Jha",
      "affiliation": "University of Wisconsin, Madison"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Research_566"
},
"SIGMOD_Research_567": {
  "title": "A Scalable Index for Top-k Subtree Similarity Queries",
  "abstract": "Given a query tree Q, the top-k subtree similarity query retrieves the k subtrees in a large document tree T that are closest to Q in terms of tree edit distance. The classical solution scans the entire document, which is slow. The state-of-the-art approach precomputes an index to reduce the query time. However, the index is large (quadratic in the document size), building the index is expensive, updates are not supported, and data-specific tuning is required.</par><par>We present a scalable solution for the top-k subtree similarity problem that does not assume specific data types, nor does it require any tuning. The key idea is to process promising subtrees first. A subtree is promising if it shares many labels with the query. We develop a new technique based on inverted lists that efficiently retrieves subtrees in the required order and supports incremental updates of the document. To achieve linear space, we avoid full list materialization but build relevant parts of a list on the fly.</par><par>In an extensive empirical evaluation on synthetic and real-world data, our technique consistently outperforms the state-of-the-art index w.r.t. memory usage, indexing time, and the number of candidates that must be verified. In terms of query time, we clearly outperform the state of the art and achieve runtime improvements of up to four orders of magnitude.",
  "subtype": "SIGMOD Research",
  "authors": [
    {
      "name": "Daniel Kocher",
      "affiliation": "University of Salzburg"
    },
    {
      "name": "Nikolaus Augsten",
      "affiliation": "University of Salzburg"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Research_567"
},
"SIGMOD_Research_573": {
  "title": "Distance-generalized Core Decomposition",
  "abstract": "The k-core of a graph is defined as the maximal subgraph in which every vertex is connected to at least k other vertices within that subgraph. In this work we introduce a distance-based generalization of the notion of k-core, which we refer to as the (k,h)-core, i.e., the maximal subgraph in which every vertex has at least k other vertices at distance leq h within that subgraph. We study the properties of the (k,h)-core showing that it preserves many of the nice features of the classic core decomposition (e.g., its connection with the notion of distance-generalized chromatic number) and it preserves its usefulness to speed-up or approximate distance-generalized notions of dense structures, such as h-club. Computing the distance-generalized core decomposition over large networks is intrinsically complex. However, by exploiting clever upper and lower bounds we can partition the computation in a set of totally independent subcomputations, opening the door to top-down exploration and to multithreading, and thus achieving an efficient algorithm.",
  "subtype": "SIGMOD Research",
  "authors": [
    {
      "name": "Francesco Bonchi",
      "affiliation": "ISI Foundation & Eurecat"
    },
    {
      "name": "Arijit Khan",
      "affiliation": "Nanyang Technological University"
    },
    {
      "name": "Lorenzo Severini",
      "affiliation": "ISI Foundation"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Research_573"
},
"SIGMOD_Research_575": {
  "title": "Dissecting the Performance of Strongly-Consistent Replication Protocols",
  "abstract": "Many distributed databases employ consensus protocols to ensure that data is replicated in a strongly-consistent manner on multiple machines despite failures and concurrency. Unfortunately, these protocols show widely varying performance under different network, workload, and deployment conditions, and no previous study offers a comprehensive dissection and comparison of their performance. To fill this gap, we study single-leader, multi-leader, hierarchical multi-leader, and leaderless (opportunistic leader) consensus protocols, and present a comprehensive evaluation of their performance in local area networks (LANs) and wide area networks (WANs). We take a two-pronged systematic approach. We present an analytic modeling of the protocols using queuing theory and show simulations under varying controlled parameters. To cross-validate the analytic model, we also present empirical results from our prototyping and evaluation framework, Paxi. We distill our findings to simple throughput and latency formulas over the most significant parameters. These formulas enable the developers to decide which category of protocols would be most suitable under given deployment conditions.",
  "subtype": "SIGMOD Research",
  "authors": [
    {
      "name": "Ailidani Ailijiang",
      "affiliation": "Microsoft"
    },
    {
      "name": "Aleksey Charapko",
      "affiliation": "University at Buffalo, SUNY"
    },
    {
      "name": "Murat Demirbas",
      "affiliation": "University at Buffalo, SUNY"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Research_575"
},
"SIGMOD_Research_576": {
  "title": "Pessimistic Cardinality Estimation",
  "abstract": "In this work we introduce a novel approach to the problem of cardinality estimation over multijoin queries. Our approach leveraging randomized hashing and data sketching to tighten these bounds beyond the current state of the art. We demonstrate that the bounds can be injected directly into the cost based query optimizer framework enabling it to avoid expensive physical join plans. We outline our base data structures and methodology, and how these bounds may be introduced to the optimizer's parameterized cost function as a new statistic for physical join plan selection. We demonstrate a complex tradeoff space between the tightness of our bounds and the size and complexity of our data structures. This space is not always monotonic as one might expect. In order combat this non-monotonicity, we introduce a partition budgeting scheme that guarantees monotonic behavior. We evaluate our methods on GooglePlus community graphs~citegoogleplus, and the Join Order Benchmark (JOB)~citeLeis:2015:GQO:2850583.2850594. In the presence of foreign key indexes, we demonstrate a 1.7times improvement in aggregate (time summed over all queries in benchmark) physical query plan runtime compared to plans chosen by Postgres using the default cardinality estimation methods. When foreign key indexes are absent, this advantage improves to over 10times.",
  "subtype": "SIGMOD Research",
  "authors": [
    {
      "name": "Walter Cai",
      "affiliation": "University of Washington"
    },
    {
      "name": "Magdalena Balazinska",
      "affiliation": "University of Washington"
    },
    {
      "name": "Dan Suciu",
      "affiliation": "University of Washington"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Research_576"
},
"SIGMOD_Research_577": {
  "title": "Towards Scalable Hybrid Stores: Constraint-Based Rewriting to the Rescue",
  "abstract": "Big data applications routinely involve diverse datasets: relations flat or nested, complex-structure graphs, documents, poorly structured logs, or even text data. To handle the data, application designers usually rely on several data stores used side-by-side, each capable of handling one or a few data models, and each very efficient for some, but not all, kinds of processing on the data. A current limitation is that applications are written taking into account which part of the data is stored in which store and how. This fails to take advantage of (i) possible redundancy, when the same data may be accessible (with different performance) from distinct data stores; (ii) partial query results (in the style of materialized views) which may be available in the stores. We present ESTOCADA, a novel approach connecting applications to the potentially heterogeneous systems where their input data resides. ESTOCADA can be used in a polystore setting to transparently enable each query to benefit from the best combination of stored data and available processing capabilities. ESTOCADA leverages recent advances in the area of view-based query rewriting under constraints, which we use to describe the various data models and stored data. Our experiments illustrate the significant performance gains achieved by ESTOCADA.",
  "subtype": "SIGMOD Research",
  "authors": [
    {
      "name": "Rana Alotaibi",
      "affiliation": "University of California, San Diego"
    },
    {
      "name": "Damian Bursztyn",
      "affiliation": "Thales"
    },
    {
      "name": "Alin Deutsch",
      "affiliation": "University of California, San Diego"
    },
    {
      "name": "Ioana Manolescu",
      "affiliation": "Inria & Ecole polytechnique"
    },
    {
      "name": "Stamatis Zampetakis",
      "affiliation": "Orchestra Networks"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Research_577"
},
"SIGMOD_Research_580": {
  "title": "FishStore: Faster Ingestion with Subset Hashing",
  "abstract": "The last decade has witnessed a huge increase in data being ingested into the cloud, in forms such as JSON, CSV, and binary formats. Traditionally, data is either ingested into storage in raw form, indexed ad-hoc using range indices, or cooked into analytics-friendly columnar formats. None of these solutions is able to handle modern requirements on storage: making the data available immediately for ad-hoc and streaming queries while ingesting at extremely high throughputs. This paper builds on recent advances in parsing and indexing techniques to propose FishStore, a concurrent latch-free storage layer for data with flexible schema, based on multi-chain hash indexing of dynamically registered predicated subsets of data. We find predicated subset hashing to be a powerful primitive that supports a broad range of queries on ingested data and admits a high-performance concurrent implementation. Our detailed evaluation on real datasets and queries shows that FishStore can handle a wide range of workloads and can ingest and retrieve data at an order of magnitude lower cost than state-of-the-art alternatives.",
  "subtype": "SIGMOD Research",
  "authors": [
    {
      "name": "Dong Xie",
      "affiliation": "University of Utah"
    },
    {
      "name": "Badrish Chandramouli",
      "affiliation": "Microsoft Research"
    },
    {
      "name": "Yinan Li",
      "affiliation": "Microsoft Research"
    },
    {
      "name": "Donald Kossmann",
      "affiliation": "Microsoft Research"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Research_580"
},
"SIGMOD_Research_588": {
  "title": "Approximate Distinct Counts for Billions of Datasets",
  "abstract": "Cardinality estimation plays an important role in processing big data. We consider the challenging problem of computing millions or more distinct count aggregations in a single pass and allowing these aggregations to be further combined into coarser aggregations. These arise naturally in many applications including networking, databases, and real-time business reporting. We demonstrate existing approaches to solve this problem are inherently flawed, exhibiting bias that can be arbitrarily large, and propose new methods for solving this problem that have theoretical guarantees of correctness and tight, practical error estimates. This is achieved by carefully combining CountMin and HyperLogLog sketches and a theoretical analysis using statistical estimation techniques. These methods also advance cardinality estimation for individual multisets, as they provide a provably consistent estimator and tight confidence intervals that have exactly the correct asymptotic coverage.",
  "subtype": "SIGMOD Research",
  "authors": [
    {
      "name": "Daniel Ting",
      "affiliation": "Tableau Software"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Research_588"
},
"SIGMOD_Research_589": {
  "title": "Speculative Distributed CSV Data Parsing for Big Data Analytics",
  "abstract": "There has been a recent flurry of interest in providing query capability on raw data in today's big data systems. These raw data must be parsed before processing or use in analytics. Thus, a fundamental challenge in distributed big data systems is that of efficient parallel parsing of raw data. The difficulties come from the inherent ambiguity while independently parsing chunks of raw data without knowing the context of these chunks. Specifically, it can be difficult to find the beginnings and ends of fields and records in these chunks of raw data. To parallelize parsing, this paper proposes a speculation-based approach for the CSV format, arguably the most commonly used raw data format. Due to the syntactic and statistical properties of the format, speculative parsing rarely fails and therefore parsing is efficiently parallelized in a distributed setting. Our speculative approach is also robust, meaning that it can reliably detect syntax errors in CSV data. We experimentally evaluate the speculative, distributed parsing approach in Apache Spark using more than 11,000 real-world datasets, and show that our parser produces significant performance benefits over existing methods.",
  "subtype": "SIGMOD Research",
  "authors": [
    {
      "name": "Chang Ge",
      "affiliation": "University of Waterloo"
    },
    {
      "name": "Yinan Li",
      "affiliation": "Microsoft Research"
    },
    {
      "name": "Eric Eilebrecht",
      "affiliation": "Microsoft Research"
    },
    {
      "name": "Badrish Chandramouli",
      "affiliation": "Microsoft Research"
    },
    {
      "name": "Donald Kossmann",
      "affiliation": "Microsoft Research"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Research_589"
},
"SIGMOD_Research_595": {
  "title": "Progressive Deep Web Crawling Through Keyword Queries For Data Enrichment",
  "abstract": "Data enrichment is the act of extending a local database with new attributes from external data sources. In this paper, we study a novel problem—how to progressively crawl the deep web (i.e., a hidden database) through a keyword-search API to enrich a local database in an e ective way. This is chal- lenging because these interfaces often limit the data access by enforcing the top-k constraint or limiting the number of queries that can be issued within a time window. In response, we propose SmartCrawl, a new framework to collect re- sults e ectively. Given a query budget b, SmartCrawl rst constructs a query pool based on the local database, and then iteratively issues a set of most bene cial queries to the hidden database such that the union of the query results can cover the maximum number of local records. The key technical challenge is how to estimate query bene t, i.e., the number of local records that can be covered by a given query. A simple approach is to estimate it as the query frequency in the local database. We nd that this is ine ective due to i) the impact of |?D|, where |?D| represents the number of local records that cannot be found in the hidden database, and ii) the top-k constraint enforced by the hidden database. We study how to mitigate the negative impacts of the two factors and propose e ective optimization techniques to improve performance. The experimental results show that on both simulated and real-world hidden databases, SmartCrawl signi cantly increases coverage over the local database as compared to the baselines.",
  "subtype": "SIGMOD Research",
  "authors": [
    {
      "name": "Pei Wang",
      "affiliation": "Simon Fraser University"
    },
    {
      "name": "Ryan Shea",
      "affiliation": "Simon Fraser University"
    },
    {
      "name": "Jiannan Wang",
      "affiliation": "Simon Fraser University"
    },
    {
      "name": "Eugene Wu",
      "affiliation": "Columbia University"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Research_595"
},
"SIGMOD_Research_598": {
  "title": "Anytime Approximation in Probabilistic Databases via Scaled Dissociations",
  "abstract": "Speeding up probabilistic inference remains a key challenge in probabilistic databases (PDBs) and the related area of statistical relational learning (SRL). Since computing probabilities for query answers is #P-hard, even for fairly simple conjunctive queries, both the PDB and SRL communities have proposed a number of approximation techniques over the years. The two prevalent techniques are either (i) MCMC-style sampling or (ii) branch-and-bound (B&B) algorithms that iteratively improve model-based bounds using a combination of variable substitution and elimination. We propose a new anytime B&B approximation scheme that encompasses all prior model-based approximation schemes proposed in the PDB and SRL literature. Our approach relies on the novel idea of “scaled dissociation” which can improve both the upper and lower bounds of existing modelbased algorithms. We apply our approach to the well-studied problem of evaluating self-join-free conjunctive queries over tuple-independent PDBs, and show a consistent reduction in approximation error in our experiments on TPC-H, Yago3, and a synthetic benchmark setting.",
  "subtype": "SIGMOD Research",
  "authors": [
    {
      "name": "Maarten Van den Heuvel",
      "affiliation": "University of Antwerp"
    },
    {
      "name": "Peter Ivanov",
      "affiliation": "Northeastern University"
    },
    {
      "name": "Wolfgang Gatterbauer",
      "affiliation": "Northeastern University"
    },
    {
      "name": "Floris Geerts",
      "affiliation": "University of Antwerp"
    },
    {
      "name": "Martin Theobald",
      "affiliation": "University of Luxembourg"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Research_598"
},
"SIGMOD_Research_604": {
  "title": "Interventional Fairness : Causal Database Repair for Algorithmic Fairness",
  "abstract": "Fairness is increasingly recognized as a critical component of machine learning systems. However, it is the underlying data on which these systems are trained that often reflect discrimination, suggesting a database repair problem. Existing treatments of fairness rely on statistical correlations that can be fooled by statistical anomalies, such as Simpson's paradox. Proposals for causality-based definitions of fairness can correctly model some of these situations, but they require specification of the underlying causal models. In this paper, we formalize the situation as a database repair problem, proving sufficient conditions for fair classifiers in terms of admissible variables as opposed to a complete causal model. We show that these conditions correctly capture subtle fairness violations. We then use these conditions as the basis for database repair algorithms that provide provable fairness guarantees about classifiers trained on their training labels. We evaluate our algorithms on real data, demonstrating improvement over the state of the art on multiple fairness metrics proposed in the literature while retaining high utility.",
  "subtype": "SIGMOD Research",
  "authors": [
    {
      "name": "Babak Salimi",
      "affiliation": "University of Washington"
    },
    {
      "name": "Luke Rodriguez",
      "affiliation": "University of Washington"
    },
    {
      "name": "Bill Howe",
      "affiliation": "University of Washington"
    },
    {
      "name": "Dan Suciu",
      "affiliation": "University of Washington"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Research_604"
},
"SIGMOD_Research_613": {
  "title": "MIFO: A Query-Semantic Aware Resource Allocation Policy",
  "abstract": "Data Analytics Frameworks encourage sharing of clusters for execution of mixed workloads by promising fairness and isolation along with high performance and resource utilization. However, concurrent query executions on such shared clusters result in increased queue and resource waiting times for queries affecting their overall performance. MIFO is a dataflow aware scheduling policy that mitigates the impacts due to queue and resource contentions by reducing the waiting times for queries near completion. We present heuristics that exploit query semantics to proactively trigger MIFO-based allocations in a workload. Our experiments on Apache Spark using TPCDS benchmark show that compared to a FAIR policy, MIFO provides an improved mean response time, reduced makespan of the workload and average speedup between 1.2x-2.7x in highly concurrent setting with only a momentary deviation in fairness.",
  "subtype": "SIGMOD Research",
  "authors": [
    {
      "name": "Prajakta Kalmegh",
      "affiliation": "Duke University"
    },
    {
      "name": "Shivnath Babu",
      "affiliation": "Unravel Data Systems"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Research_613"
},
"SIGMOD_Research_616": {
  "title": "The Log-Structured Merge-Bush & the Wacky Continuum",
  "abstract": "Write-optimized key-value stores use Log-Structured Merge-tree (LSM-tree) as their core data structure. They are utilized as the storage solution across diverse industries, such as social media, data science, and cloud infrastructures. We show that existing designs exhibit two critical problems: 1)~they do not scale optimally with data, and 2) no individual design achieves optimal performance for all possible workloads. We identify the source of the problem to be an intrinsic three-way trade-off between the required memory and the performance of reads and writes. This trade-off is dictated by the capacity ratio among levels of LSM-tree and by the number of bits per entry assigned to the Bloom filters at each level. In existing designs, both knobs are fixed, which causes the trade-off to get worse with more data due to the increased need for costly log compactions. Weintroduce a new data structure, the Log-Structured Merge-Bush (LSM-Bush), for which the capacity ratio and filter size can be set independently for every level. For write-intensive workloads with only point reads, we show that increasing the level capacity ratios and Bloom filters' bits per entry across smaller levels decreases write cost from O(log N) to O(loglog N) by taming compaction overheads as the data grows without hurting point read performance or memory. Wethen introduce Wacky, a data structure design continuum that includes LSM-Bush as well as all state-of-the-art merge policies and can be tuned to assume each of them using five knobs. Weshow that Wacky can assume a massive space of performance properties, including ones that favor range reads, and that it can optimize performance for any workload without requiring manual tuning by an expert.",
  "subtype": "SIGMOD Research",
  "authors": [
    {
      "name": "Niv Dayan",
      "affiliation": "Harvard University"
    },
    {
      "name": "Stratos Idreos",
      "affiliation": "Harvard University"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Research_616"
},
"SIGMOD_Research_626": {
  "title": "iQCAR: inter-Query Contention Analyzer for Data Analytics Frameworks",
  "abstract": "Resource interferences caused by concurrent queries is one of the key reasons for unpredictable performance and missed workload SLAs in cluster computing systems. Analyzing these inter-query resource interactions is critical in order to answer time-sensitive questions like 'who is creating resource conflicts to my query'. More importantly, diagnosing whether the resource blocked times of a 'victim' query are caused by other queries or some other external factor can help the database administrator narrow down the many possibilities of query performance degradation. We introduce iQCAR, an inter-Query Contention Analyzer, that attributes blame for the slowdown of a query to concurrent queries. iQCAR models the resource conflicts using a multi-level directed acyclic graph that can help administrators compare impacts from concurrent queries, identify most contentious queries, resources and hosts in an online execution for a selected time window. Our experiments using TPCDS queries on Apache Spark show that our approach is substantially more accurate than other methods based on overlap time between concurrent queries.",
  "subtype": "SIGMOD Research",
  "authors": [
    {
      "name": "Prajakta Kalmegh",
      "affiliation": "Duke University"
    },
    {
      "name": "Shivnath Babu",
      "affiliation": "Unravel Data Systems"
    },
    {
      "name": "Sudeepa Roy",
      "affiliation": "Duke University"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Research_626"
},
"PODS_Research_008": {
  "title": "Ranked Enumeration of Minimal Triangulations",
  "abstract": "A tree decomposition of a graph facilitates computations by grouping vertices into bags that are interconnected in an acyclic structure; hence their importance in a plethora of problems such as query evaluation over databases and inference over probabilistic graphical models. The relative benefit from different tree decompositions is measured by diverse (sometime complex) cost functions that vary from one application to another. For generic cost functions like width and fill-in, an optimal tree decomposition can be efficiently computed in some cases, notably when the number of minimal separators is bounded by a polynomial (due to Bouchitte and Todinca); we refer to this assumption as 'poly-MS.' To cover the variety of cost functions in need, it has recently been proposed to devise algorithms for enumerating many decomposition candidates for applications to choose from using specialized, or even machine-learned, cost functions. We explore the ability to produce a large collection of 'high quality' tree decompositions. We present the first algorithm for ranked enumeration of the proper (non-redundant) tree decompositions, or equivalently minimal triangulations, under a wide class of cost functions that substantially generalizes the generic ones above. On the theoretical side, we establish the guarantee of polynomial delay if poly-MS is assumed, or if we are interested in tree decompositions of a width bounded by a constant. We describe an experimental evaluation on graphs of various domains (including join queries, Bayesian networks, treewidth benchmarks and random), and explore both the applicability of the poly-MS assumption and the performance of our algorithm relative to the state of the art.",
  "subtype": "PODS Research",
  "authors": [
    {
      "name": "Noam Ravid",
      "affiliation": "Technion"
    },
    {
      "name": "Dori Medini",
      "affiliation": "Technion"
    },
    {
      "name": "Benny Kimelfeld",
      "affiliation": "Technion"
    }
   ],
  "type": "PODS",
  "id": "PODS_Research_008"
},
"PODS_Research_009": {
  "title": "Testability of Homomorphism Inadmissibility: Property Testing Meets Database Theory",
  "abstract": "In this paper, we utilize the perspective of property testing to consider the testability of relational database queries. A primary motivation is the desire to avoid reading an entire database to decide a property thereof. We focus on conjunctive queries, which are the most basic and heavily studied database queries. Each conjunctive query can be represented as a relational structure A such that deciding if the conjunctive query is satisfied by a relational structure B is equivalent to deciding if there exists a homomorphism from A to B. We phrase our results in terms of homomorphisms. Precisely, we study, for each relational structure A, the testability of homomorphism inadmissibility from A. We consider algorithms that have oracle access to an input relational structure B and that distinguish, with high probability, the case where there is no homomorphism from A to B, from the case where one needs to remove a constant fraction of tuples from B in order to suppress all such homomorphisms. We provide a complete characterization of the structures A from which one can test homomorphism inadmissibility with one-sided error by making a constant number of queries to B. Our characterization shows that homomorphism inadmissibility from A is constant-query testable with one-sided error if and only if the core of A is alpha-acyclic. We also show that the injective version of the problem is constant-query testable with one-sided error if A is alpha-acyclic; this result generalizes existing results for testing subgraph-freeness in the general graph model.",
  "subtype": "PODS Research",
  "authors": [
    {
      "name": "Hubie Chen",
      "affiliation": "Birkbeck, University of London"
    },
    {
      "name": "Yuichi Yoshida",
      "affiliation": "National Institute of Informatics"
    }
   ],
  "type": "PODS",
  "id": "PODS_Research_009"
},
"PODS_Research_013": {
  "title": "Regularizing Conjunctive Features for Classification",
  "abstract": "We consider the feature-generation task wherein we are given a  database with entities labeled as positive and negative examples, and the goal is to find feature queries that allow for a linear  separation between the two sets of examples. We focus on conjunctive feature queries, and explore two fundamental problems:  (a) deciding whether separating feature queries exist (separability), and (b) generating such queries when they exist. In  the approximate versions of these problems, we allow a predefined fraction of the examples to be misclassified. To restrict the  complexity of the generated classifiers, we explore various ways of regularizing (i.e., imposing simplicity constraints on) them by  limiting their dimension, the number of joins in feature queries, and their generalized hypertree width (ghw). Among other results, we  show that the separability problem is tractable in the case of bounded ghw; yet, the generation problem is intractable, simply  because the feature queries might be too large. So, we explore a third problem: classifying new entities without necessarily  generating the feature queries. Interestingly, in the case of bounded ghw we can efficiently classify without ever explicitly generating the feature queries.",
  "subtype": "PODS Research",
  "authors": [
    {
      "name": "Pablo Barceló",
      "affiliation": "University of Chile & IMFD Chile"
    },
    {
      "name": "Alexander Baumgartner",
      "affiliation": "University of Chile & RISC"
    },
    {
      "name": "Victor Dalmau",
      "affiliation": "Universitat Pompeu Fabra"
    },
    {
      "name": "Benny Kimelfeld",
      "affiliation": "Technion"
    }
   ],
  "type": "PODS",
  "id": "PODS_Research_013"
},
"PODS_Research_015": {
  "title": "Probabilistic Databases with an Infinite Open-World Assumption",
  "abstract": "Probabilistic databases (PDBs) introduce uncertainty into relational databases by specifying probabilities for several possible instances. Traditionally, they are finite probability spaces over database instances. Such finite PDBs inherently make a closed-world assumption: non-occurring facts are assumed to be impossible, rather than just unlikely. As convincingly argued by Ceylan et al. (KR '16), this results in implausibilities and clashes with intuition. An open-world assumption, where facts not explicitly listed may have a small positive probability can yield more reasonable results. The corresponding open-world model of Ceylan et al., however, assumes that all entities in the PDB come from a fixed finite universe. In this work, we take one further step and propose a model of truly open-world PDBs with an infinite universe. This is natural when we consider entities from typical domains such as integers, real numbers, or strings. While the probability space might become infinitely large, all instances of a PDB remain finite. We provide a sound mathematical framework for infinite PDBs generalizing the existing theory of finite PDBs. Our main results are concerned with countable, tuple-independent PDBs; we present a generic construction showing that such PDBs exist in the infinite and provide a characterization of their existence. This construction can be used to give an open-world semantics to finite PDBs. The construction can also be extended to so-called block-independent-disjoint probabilistic databases. Algorithmic questions are not the focus of this paper, but we show how query evaluation algorithms can be lifted from finite PDBs to perform approximate evaluation (with an arbitrarily small additive approximation error) in countably infinite tuple-independent PDBs.",
  "subtype": "PODS Research",
  "authors": [
    {
      "name": "Martin Grohe",
      "affiliation": "RWTH Aachen University"
    },
    {
      "name": "Peter Lindner",
      "affiliation": "RWTH Aachen University"
    }
   ],
  "type": "PODS",
  "id": "PODS_Research_015"
},
"PODS_Research_016": {
  "title": "The Selfish Models Property: Bounding the Complexity of Query Containment and Entailment Problems",
  "abstract": "Query containment is the fundamental problem of deciding, given two database queries, if the result of the first query is always contained in the result of the second query. For a number of established query classes, an instance of this problem can be decided by computing a set of models of the first query, and then evaluating the second query on each of the models. We formalize this phenomenon by introducing the selfish models property; this property gives an avenue for establishing both the decidability of and complexity upper bounds for containment problems. Using this property, we show how existing results can be uniformly derived, and we present two significant novel positive results for first-order query containment problems, exhibiting complexity upper bounds for containment problems that were not previously known to be decidable.",
  "subtype": "PODS Research",
  "authors": [
    {
      "name": "Hubie Chen",
      "affiliation": "Birkbeck, University of London"
    }
   ],
  "type": "PODS",
  "id": "PODS_Research_016"
},
"PODS_Research_017": {
  "title": "HyperBench: A Benchmark and Tool for Hypergraphs and Empirical Findings",
  "abstract": "To cope with the intractability of answering Conjunctive Queries (CQs) and solving Constraint Satisfaction Problems (CSPs), several notions of hypergraph decompositions have been proposed - giving rise to different notions of width, noticeably, plain, generalized, and fractional hypertree width (hw, ghw, and fhw). Given the increasing interest in using such decomposition methods in practice, a publicly accessible repository of decomposition software, as well as a large set of benchmarks, and a web-accessible workbench for inserting, analysing, and retrieving hypergraphs are called for. We address this need by providing (i) concrete implementations of hypergraph decompositions (including new practical algorithms), (ii) a new, comprehensive benchmark of hypergraphs stemming from disparate CQ and CSP collections, and (iii) HyperBench, our new web-interface for accessing the benchmark and the results of our analyses. In addition, we describe a number of actual experiments we carried out with this new infrastructure.",
  "subtype": "PODS Research",
  "authors": [
    {
      "name": "Wolfgang Fischl",
      "affiliation": "Vienna University of Technology"
    },
    {
      "name": "Georg Gottlob",
      "affiliation": "University of Oxford"
    },
    {
      "name": "Davide Mario Longo",
      "affiliation": "Vienna University of Technology"
    },
    {
      "name": "Reinhard Pichler",
      "affiliation": "Vienna University of Technology"
    }
   ],
  "type": "PODS",
  "id": "PODS_Research_017"
},
"PODS_Research_024": {
  "title": "Split-Correctness in Information Extraction",
  "abstract": "Programs for extracting structured information from text, namely information extractors, often operate separately on document segments obtained from a generic splitting operation such as sentences, paragraphs, k-grams, HTTP requests, and so on. An automated detection of this behavior of extractors, which we refer to as split-correctness, would allow text analysis systems to devise query plans with parallel evaluation on segments for accelerating the processing of large documents. Other applications include the incremental evaluation on dynamic content, where re-evaluation of information extractors can be restricted to revised segments, and debugging, where developers of information extractors are informed about potential boundary crossing of different semantic components. We propose a new formal framework for split-correctness within the formalism of document spanners. Our preliminary analysis studies the complexity of split-correctness over regular spanners. We also discuss different variants of split-correctness, for instance, in the presence of black-box extractors with split constraints.",
  "subtype": "PODS Research",
  "authors": [
    {
      "name": "Johannes Doleschal",
      "affiliation": "University of Bayreuth & Hasselt University"
    },
    {
      "name": "Benny Kimelfeld",
      "affiliation": "Technion"
    },
    {
      "name": "Wim Martens",
      "affiliation": "University of Bayreuth"
    },
    {
      "name": "Yoav Nahshon",
      "affiliation": "Technion"
    },
    {
      "name": "Frank Neven",
      "affiliation": "Hasselt University & Transnational University of Limburg"
    }
   ],
  "type": "PODS",
  "id": "PODS_Research_024"
},
"PODS_Research_030": {
  "title": "Decidable XPath Fragments in the Real World",
  "abstract": "XPath is arguably the most popular query language for selecting elements in XML documents. Besides query evaluation, query satisfiability and containment are the main computational problems for XPath; they are useful, for instance, to detect dead code or validate query optimisations. These problems are undecidable in general, but several fragments have been identified over time for which satisfiability (or query containment) is decidable: CoreXPath 1.0 and 2.0 without so-called data joins, fragments with data joins but limited navigation, etc. However, these fragments are often given in a simplified syntax, and sometimes w.r.t. a simplified XPath semantics. Moreover, they have been studied mostly with theoretical motivations, with little consideration for the practically relevant features of XPath. To investigate the practical impact of these theoretical fragments, we design a benchmark compiling thousands of real-world XPath queries extracted from open-source projects, and match them against syntactic fragments from the literature. We investigate how to extend these fragments with seldom-considered features such as free variables, data tests, data joins, and the last () and id () functions, for which we provide both undecidability and decidability results. We analyse the coverage of the original and extended fragments, and further provide a glimpse at which other practical features might be worth investigating in the future.",
  "subtype": "PODS Research",
  "authors": [
    {
      "name": "David Baelde",
      "affiliation": "ENS Paris-Saclay & CNRS, Université Paris-Saclay"
    },
    {
      "name": "Anthony Lick",
      "affiliation": "ENS Paris-Saclay & CNRS, Université Paris-Saclay"
    },
    {
      "name": "Sylvain Schmitz",
      "affiliation": "ENS Paris-Saclay & CNRS, Université Paris-Saclay"
    }
   ],
  "type": "PODS",
  "id": "PODS_Research_030"
},
"PODS_Research_032": {
  "title": "Topology Dependent Bounds For FAQs",
  "abstract": "In this paper, we prove topology dependent bounds on the number of rounds needed to compute Functional Aggregate Queries (FAQs) studied by Abo Khamis et al. [PODS 2016] in a synchronous distributed network under the model considered by Chattopadhyay et al. [FOCS 2014, SODA 2017]. Unlike the recent work on computing database queries in the Massively Parallel Computation model, in the model of Chattopadhyay et al., nodes can communicate only via private point-to-point channels and we are interested in bounds that work over an arbitrary communication topology. This model, which is closer to the well-studied congest model in distributed computing and generalizes Yao's two party communication complexity model, has so far only been studied for problems that are common in the two-party communication complexity literature. This is the first work to consider more practically motivated problems in this distributed model. For the sake of exposition, we focus on two specific problems in this paper: Boolean Conjunctive Query (BCQ) and computing variable/factor marginals in Probabilistic Graphical Models (PGMs). We obtain tight bounds on the number of rounds needed to compute such queries as long as the underlying hypergraph of the query is O(1)-degenerate and has O(1)-arity. In particular, the O(1)-degeneracy condition covers most well-studied queries that are efficiently computable in the centralized computation model like queries with constant treewidth. These tight bounds depend on a new notion of `width' (namely internal-node-width) for Generalized Hypertree Decompositions (GHDs) of acyclic hypergraphs, which minimizes the number of internal nodes in a sub-class of GHDs. To the best of our knowledge, this width has not been studied explicitly in the theoretical database literature. Finally, we consider the problem of computing the product of a vector with a chain of matrices and prove tight bounds on its round complexity (over a finite field of two elements) using a novel min-entropy based argument.",
  "subtype": "PODS Research",
  "authors": [
    {
      "name": "Michael Langberg",
      "affiliation": "University at Buffalo, SUNY"
    },
    {
      "name": "Shi Li",
      "affiliation": "University at Buffalo, SUNY"
    },
    {
      "name": "Sai Vikneshwar Mani Jayaraman",
      "affiliation": "University at Buffalo, SUNY"
    },
    {
      "name": "Atri Rudra",
      "affiliation": "University at Buffalo, SUNY"
    }
   ],
  "type": "PODS",
  "id": "PODS_Research_032"
},
"PODS_Research_033": {
  "title": "Containment of Shape Expression Schemas for RDF",
  "abstract": "We study the problem of containment of shape expression schemas ShEx for RDF graphs. We identify a subclass of ShEx that has a natural graphical  representation in the form of shape graphs and whose semantics is captured with a tractable notion of embedding of an RDF graph in a  shape graph. When applied to pairs of shape graphs, an embedding is a sufficient condition for containment, and for a practical subclass of  deterministic shape graphs, it is also a necessary one, thus yielding a subclass with tractable containment. Containment for general shape graphs is  EXP-complete. Finally, we show that containment for arbitrary ShEx is decidable.",
  "subtype": "PODS Research",
  "authors": [
    {
      "name": "Slawek Staworko",
      "affiliation": "CNRS & University of Lille"
    },
    {
      "name": "Piotr Wieczorek",
      "affiliation": "University of Wroclaw"
    }
   ],
  "type": "PODS",
  "id": "PODS_Research_033"
},
"PODS_Research_041": {
  "title": "The Space-Efficient Core of Vadalog",
  "abstract": "Vadalog is a system for performing complex reasoning tasks such as those required in advanced knowledge graphs. The logical core of the underlying Vadalog language is the warded fragment of tuple-generating dependencies (TGDs). This formalism ensures tractable reasoning in data complexity, while a recent analysis focusing on a practical implementation led to the reasoning algorithm around which the Vadalog system is built. A fundamental question that has emerged in the context of Vadalog is the following: can we limit the recursion allowed by wardedness in order to obtain a formalism that provides a convenient syntax for expressing useful recursive statements, and at the same time achieves space-efficiency? After analyzing several real-life examples of warded sets of TGDs provided by our industrial partners, as well as recent benchmarks, we observed that recursion is often used in a restricted way: the body of a TGD contains at most one atom whose predicate is mutually recursive with a predicate in the head. We show that this type of recursion, known as piece-wise linear in the Datalog literature, is the answer to our main question. We further show that piece-wise linear recursion alone, without the wardedness condition, is not enough as it leads to the undecidability of reasoning. We finally study the relative expressiveness of the query languages based on (piece-wise linear) warded sets of TGDs.",
  "subtype": "PODS Research",
  "authors": [
    {
      "name": "Gerald Berger",
      "affiliation": "Vienna University of Technology"
    },
    {
      "name": "Georg Gottlob",
      "affiliation": "University of Oxford"
    },
    {
      "name": "Andreas Pieris",
      "affiliation": "University of Edinburgh"
    },
    {
      "name": "Emanuel Sallinger",
      "affiliation": "University of Oxford"
    }
   ],
  "type": "PODS",
  "id": "PODS_Research_041"
},
"PODS_Research_043": {
  "title": "Attacking Diophantus: Solving a Special Case of Bag Containment",
  "abstract": "Conjunctive-query containment is the problem of deciding whether the answers of a given conjunctive query on an arbitrary database instance are always contained in the answers of a second query on the same instance. This is a very relevant question in query optimization, data integration, and other data management and artificial intelligence areas. The problem has been deeply studied and understood for the, so-called, set-semantics, i.e., when query answers and database instances are modelled as sets of tuples. In particular, it has been shown by Chandra and Merlin to be NPTIME-COMPLETE. On the contrary, when investigated under bag-semantics, a.k.a. multiset semantics, which allows for replicated tuples both in the underlying instance and in the query answers, it is not even clear whether the problem is decidable. Since this is exactly the standard interpretation for commercial relational database systems, the question turns out to be an important one. Multiple works on variations and restrictions of the bag-containment problem have been reported in the literature and, although the general problem is still open, we contribute with this article by solving a special case that has been identified as a major open problem on its own. More specifically, we study projection-free queries, i.e., queries without existentially quantified variables, and show decidability for the bag-containment problem of a projection-free conjunctive query into a generic conjunctive query. We prove indeed that deciding containment in this setting is in Pi^p_2. Our approach relies on the solution of a special case of the Diophantine inequality problem via a reduction to the linear inequality problem and clearly exposes inherent difficulties in the analysis of the general question.",
  "subtype": "PODS Research",
  "authors": [
    {
      "name": "George Konstantinidis",
      "affiliation": "University of Southampton"
    },
    {
      "name": "Fabio Mogavero",
      "affiliation": "University of Naples Federico II"
    }
   ],
  "type": "PODS",
  "id": "PODS_Research_043"
},
"PODS_Research_060": {
  "title": "Robust Set Reconciliation via Locality Sensitive Hashing",
  "abstract": "We consider variations of set reconciliation problems where two parties, Alice and Bob, each hold a set of points in a metric space, and the goal is for Bob to conclude with a set of points that is close to Alice's set of points in a well-defined way. This setting has been referred to as robust set reconciliation. In one variation, the goal is for Bob to end with a set of points that is close to Alice's in earth mover's distance, and in another the goal is for Bob to have a point that is close to each of Alice's. The first problem has been studied before; while previous results achieved an O(d) approximation, where d is the dimension of the space, we achieve an O(log n) approximation, where n is the number of points.  The second problem appears new, and here we find schemes that, under certain conditions, use sublinear communication. Our primary novelty is utilizing Invertible Bloom Lookup Tables in combination with locality sensitive hashing. This combination allows us to cope with the geometric setting in a communication-efficient manner.",
  "subtype": "PODS Research",
  "authors": [
    {
      "name": "Michael Mitzenmacher",
      "affiliation": "Harvard University"
    },
    {
      "name": "Tom Morgan",
      "affiliation": "Harvard University & Google"
    }
   ],
  "type": "PODS",
  "id": "PODS_Research_060"
},
"PODS_Research_061": {
  "title": "Tight Trade-offs for the Maximum k-Coverage Problem in the General Streaming Model",
  "abstract": "We study the maximum k-coverage problem in the general edge-arrival streaming model: given a collection of m sets F, each subset of a ground set of elements U of size n, the task is to find k sets whose coverage is maximized. The sets are specified as a sequence of (element, set) pairs in an arbitrary order. Our main result is a tight (up to polylogarithmic factors) trade-off between the space complexity and the approximation factor alphain(1/(1-1/e), tildeOmega(sqrtm)] of any single-pass streaming algorithm that estimates the maximum coverage size. Specifically, we show that the optimal space bound is tildeTheta(m/alpha^2). Moreover, we design a single-pass algorithm that reports an alpha-approximate solution in tildeO(m/alpha^2 + k) space. Our algorithm heavily exploits data stream sketching techniques, which could lead to further connections between vector sketching methods and streaming algorithms for combinatorial optimization tasks.",
  "subtype": "PODS Research",
  "authors": [
    {
      "name": "Piotr Indyk",
      "affiliation": "MIT"
    },
    {
      "name": "Ali Vakilian",
      "affiliation": "MIT"
    }
   ],
  "type": "PODS",
  "id": "PODS_Research_061"
},
"PODS_Research_063": {
  "title": "Query Evaluation in Election Databases",
  "abstract": "Election databases are the main elements of a recently introduced framework that aims to create bridges between the computational social choice and the data management communities. An election database consists of incomplete information about the preferences of voters, in the form of partial orders, alongside with standard database relations that provide contextual information. Earlier work in computational social choice focused on the computation of possible winners and necessary winners that are determined by the available incomplete information and the voting rule at hand. The presence of the relational context, however, permits the formulation of sophisticated queries about voting rules, candidates, potential winners, issues, and positions on issues. Such queries can be given possible answer semantics and necessary answer semantics on an election database, where the former means that the query is true on some completion of the given partial orders and the latter means that the query is true on every such completion. %In this paper,   We carry out a systematic investigation of query evaluation on election databases by analyzing how the interaction between the partial preferences, the voting rules and the relational context impacts on the complexity of query evaluation. To this effect, we focus on positional scoring rules and unions of conjunctive queries. We establish a number of results that delineate the complexity of the possible answers and of the necessary answers for different positional scoring rules and for various classes of unions of conjunctive queries. Furthermore, we show that query evaluation is fixed-parameter tractable, where the parameter is the number of candidates in the election.",
  "subtype": "PODS Research",
  "authors": [
    {
      "name": "Benny Kimelfeld",
      "affiliation": "Technion"
    },
    {
      "name": "Phokion Kolaitis",
      "affiliation": "University of California, Santa Cruz & IBM Almaden Research Center"
    },
    {
      "name": "Muhammad Tibi",
      "affiliation": "Technion"
    }
   ],
  "type": "PODS",
  "id": "PODS_Research_063"
},
"PODS_Research_064": {
  "title": "Compiling Existential Positive Queries to Bounded-Variable Fragments",
  "abstract": "A crucial property of bounded-variable fragments of first-order logic is that they can be evaluated in polynomial time. It is therefore a useful preprocessing step to rewrite, if possible, a first-order query to a logically equivalent one with a minimum number of variables. However, it may occur that reducing the number of variables causes an increase in formula size. We investigate this trade-off for the existential-positive fragment of first-order queries, where variable minimisation is decidable in general. In particular, we study the blow-up in the formula size when compiling existential-positive queries to the bounded variable fragment of positive first-order logic. While the increase of the formula size is always at most exponential, we identify situations (based on the signature and the number of variables) where only a polynomial blow-up is needed. In all other cases, we show that an exponential lower bound on the formula size of the compiled formula that matches the general upper bound. This exponential lower bound is unconditional, and is the first unconditional lower bound for formula size with respect to the studied compilation; it is proved via establishing a novel interface with circuit complexity which may be of future interest.",
  "subtype": "PODS Research",
  "authors": [
    {
      "name": "Christoph Berkholz",
      "affiliation": "Humboldt-Universität"
    },
    {
      "name": "Hubie Chen",
      "affiliation": "Birkbeck, University of London"
    }
   ],
  "type": "PODS",
  "id": "PODS_Research_064"
},
"PODS_Research_072": {
  "title": "On Functional Aggregate Queries with Additive Inequalities",
  "abstract": "Motivated by fundamental applications in databases and relational machine learning, we formulate and study the problem of answering functional aggregate queries (FAQ) in which some of the input factors are defined by a collection of additive inequalities between variables. We refer to these queries as FAQ-AI for short. To answer FAQ-AI in the Boolean semiring, we define relaxed tree decompositions and relaxed submodular and fractional hypertree width parameters. We show that an extension of the InsideOut algorithm using Chazelle's geometric data structure for solving the semigroup range search problem can answer Boolean FAQ-AI in time given by these new width parameters. This new algorithm achieves lower complexity than known solutions for FAQ-AI. It also recovers some known results in database query answering. Our second contribution is a relaxation of the set of polymatroids that gives rise to the counting version of the submodular width, denoted by #subw. This new width is sandwiched between the submodular and the fractional hypertree widths. Any FAQ and FAQ-AI over one semiring can be answered in time proportional to #subw and respectively to the relaxed version of #subw. We present three applications of our FAQ-AI framework to relational machine learning: k-means clustering, training linear support vector machines, and training models using non-polynomial loss. These optimization problems can be solved over a database asymptotically faster than computing the join of the database relations.",
  "subtype": "PODS Research",
  "authors": [
    {
      "name": "Mahmoud Abo Khamis",
      "affiliation": "RelationalAI"
    },
    {
      "name": "Ryan Curtin",
      "affiliation": "RelationalAI"
    },
    {
      "name": "Benjamin Moseley",
      "affiliation": "Carnegie Mellon University"
    },
    {
      "name": "Hung Ngo",
      "affiliation": "RelationalAI"
    },
    {
      "name": "XuanLong Nguyen",
      "affiliation": "University of Michigan"
    },
    {
      "name": "Dan Olteanu",
      "affiliation": "University of Oxford"
    },
    {
      "name": "Maximilian Schleich",
      "affiliation": "University of Oxford"
    }
   ],
  "type": "PODS",
  "id": "PODS_Research_072"
},
"PODS_Research_074": {
  "title": "What Storage Access Privacy is Achievable with Small Overhead?",
  "abstract": "Oblivious RAM (ORAM) and private information retrieval (PIR) are classic cryptographic primitives used to hide the access pattern to data whose storage has been outsourced to an untrusted server. Unfortunately, both primitives require considerable overhead compared to plaintext access. For large-scale storage infrastructure with highly frequent access requests, the degradation in response time and the exorbitant increase in resource costs incurred by either ORAM or PIR prevent their usage. In an ideal scenario, a privacy-preserving storage protocols with small overhead would be implemented for these heavily trafficked storage systems to avoid negatively impacting either performance and/or costs. In this work, we study the problem of the best storage access privacy that is achievable with only small overhead over plaintext access. To answer this question, we consider differential privacy access which is a generalization of the oblivious access security notion that are considered by ORAM and PIR. Quite surprisingly, we present strong evidence that constant overhead storage schemes may only be achieved with privacy budgets of epsilon = Omega(log n). We present asymptotically optimal constructions for differentially private variants of both ORAM and PIR with privacy budgets epsilon = Theta(log n) with only O(1) overhead. In addition, we consider a more complex storage primitive called key-value storage in which data is indexed by keys from a large universe (as opposed to consecutive integers in ORAM and PIR). We present a differentially private key-value storage scheme with epsilon = Theta(log n) and O(loglog n) overhead. This construction uses a new oblivious, two-choice hashing scheme that may be of independent interest.",
  "subtype": "PODS Research",
  "authors": [
    {
      "name": "Sarvar Patel",
      "affiliation": "Google"
    },
    {
      "name": "Giuseppe Persiano",
      "affiliation": "Google & University of Salerno"
    },
    {
      "name": "Kevin Yeo",
      "affiliation": "Google"
    }
   ],
  "type": "PODS",
  "id": "PODS_Research_074"
},
"PODS_Research_075": {
  "title": "Weighted Reservoir Sampling from Distributed Streams",
  "abstract": "We consider message-efficient continuous random sampling from a distributed stream, where the probability of inclusion of an item in the sample is proportional to a weight associated with the item. The unweighted version, where all weights are equal, is well studied, and admits tight upper and lower bounds on message complexity. For weighted sampling with replacement, there is a simple reduction to unweighted sampling with replacement. However, in many applications the stream may have only a few heavy items which may dominate a random sample when chosen with replacement. Weighted sampling without replacement (weighted SWOR) eludes this issue, since such heavy items can be sampled at most once. In this work, we present the first message-optimal algorithm for weighted SWOR from a distributed stream. Our algorithm also has optimal space and time complexity.  As an application of our algorithm for weighted SWOR, we derive the first distributed streaming algorithms for tracking heavy hitters with residual error. Here the goal is to identify stream items that contribute significantly to the residual stream, once the heaviest items are removed. Residual heavy hitters generalize the notion of ell_1 heavy hitters and are important in streams that have a skewed distribution of weights. In addition to the upper bound, we also provide a lower bound on the message complexity that is nearly tight up to a log(1/eps) factor. Finally, we use our weighted sampling algorithm to improve the message complexity of distributed L_1 tracking, also known as count tracking, which is a widely studied problem in distributed streaming. We also derive a tight message lower bound, which closes the message complexity of this fundamental problem.",
  "subtype": "PODS Research",
  "authors": [
    {
      "name": "Rajesh Jayaram",
      "affiliation": "Carnegie Mellon University"
    },
    {
      "name": "Gokarna Sharma",
      "affiliation": "Kent State University"
    },
    {
      "name": "Srikanta Tirthapura",
      "affiliation": "Iowa State University"
    },
    {
      "name": "David Woodruff",
      "affiliation": "Carnegie Mellon University"
    }
   ],
  "type": "PODS",
  "id": "PODS_Research_075"
},
"PODS_Research_077": {
  "title": "Distributed and Streaming Linear Programming in Low Dimensions",
  "abstract": "We study linear programming and general LP-type problems in several big data (streaming and distributed) models. We mainly focus on low dimensional problems in which the number of constraints is much larger than the number of variables. Low dimensional LP-type problems appear frequently in various machine learning tasks such as robust regression, support vector machines, and core vector machines. As supporting large-scale machine learning queries in database systems has become an important direction for database research, obtaining efficient algorithms for low dimensional LP-type problems on massive datasets is of great value. In this paper we give both upper and lower bounds for LP-type problems in distributed and streaming models. Our bounds are almost tight when the dimensionality of the problem is a fixed constant.",
  "subtype": "PODS Research",
  "authors": [
    {
      "name": "Sepehr Assadi",
      "affiliation": "Princeton University"
    },
    {
      "name": "Nikolai Karpov",
      "affiliation": "Indiana University"
    },
    {
      "name": "Qin Zhang",
      "affiliation": "Indiana University"
    }
   ],
  "type": "PODS",
  "id": "PODS_Research_077"
},
"PODS_Research_079": {
  "title": "Instance and Output Optimal Parallel Algorithms for Acyclic Joins",
  "abstract": "Massively parallel join algorithms have received much attention in recent years, while most prior work has focused on worst-optimal algorithms. However, the worst-case optimality of these join algorithms relies on hard instances having very large output sizes, which rarely appear in practice. A stronger notion of optimality is output-optimal, which requires an algorithm to be optimal within the class of all instances sharing the same input and output size.  An even stronger optimality is instance-optimal, i.e., the algorithm is optimal on every single instance, but this may not always be achievable. In the traditional RAM model of computation, the classical Yannakakis algorithm is instance-optimal on any acyclic join. But in the massively parallel computation (MPC) model, the situation becomes much more complicated. We first show that for the class of r-hierarchical joins, instance-optimality can still be achieved in the MPC model. Then, we give a new MPC algorithm for an arbitrary acyclic join with load O (IN over p + sqrtIN cdot OUT over p), where IN,OUT are the input and output sizes of the join, and p is the number of servers in the MPC model. This improves the MPC version of the Yannakakis algorithm by an O (sqrtOUT over IN ) factor. Furthermore, we show that this is output-optimal when OUT = O(p cdot IN), for every acyclic but non-r-hierarchical join. Finally, we give the first output-sensitive lower bound for the triangle join in the MPC model, showing that it is inherently more difficult than acyclic joins.",
  "subtype": "PODS Research",
  "authors": [
    {
      "name": "Xiao Hu",
      "affiliation": "Hong Kong University of Science and Technology"
    },
    {
      "name": "Ke Yi",
      "affiliation": "Hong Kong University of Science and Technology"
    }
   ],
  "type": "PODS",
  "id": "PODS_Research_079"
},
"PODS_Research_083": {
  "title": "Complexity Bounds for Relational Algebra over Document Spanners",
  "abstract": "We investigate the complexity of evaluating queries in Relational Algebra (RA) over the relations extracted by regex formulas (i.e., regular expressions with capture variables) over text documents. Such queries, also known as the regular document spanners, were shown to have an evaluation with polynomial delay for every positive RA expression (i.e., consisting of only natural joins, projections and unions); here, the RA expression is fixed and the input consists of both the regex formulas and the document. In this work, we explore the implication of two fundamental generalizations. The first is adopting the 'schemaless' semantics for spanners, as proposed and studied by Maturana et al. The second is going beyond the positive RA to allowing the difference operator. We show that each of the two generalizations introduces computational hardness: it is intractable to compute the natural join of two regex formulas under the schemaless semantics, and the difference between two regex formulas under both the ordinary and schemaless semantics. Nevertheless, we propose and analyze syntactic constraints, on the RA expression and the regex formulas at hand, such that the expressive power is fully preserved and, yet, evaluation can be done with polynomial delay. Unlike the previous work on RA over regex formulas, our technique is not (and provably cannot be) based on the static compilation of regex formulas, but rather on an ad-hoc compilation into an automaton that incorporates both the query and the document. This approach also allows us to include black-box extractors in the RA expression.",
  "subtype": "PODS Research",
  "authors": [
    {
      "name": "Liat Peterfreund",
      "affiliation": "Technion"
    },
    {
      "name": "Dominik Freydenberger",
      "affiliation": "Loughborough University"
    },
    {
      "name": "Benny Kimelfeld",
      "affiliation": "Technion"
    },
    {
      "name": "Markus Kröll",
      "affiliation": "Vienna University of Technology"
    }
   ],
  "type": "PODS",
  "id": "PODS_Research_083"
},
"PODS_Research_084": {
  "title": "On the Enumeration Complexity of Unions of Conjunctive Queries",
  "abstract": "We study the enumeration complexity of Unions of Conjunctive Queries (UCQs). We aim to identify the UCQs that are tractable in the sense that the answer tuples can be enumerated with a linear preprocessing phase and a constant delay between every successive tuples. It has been established that, in the absence of self joins and under conventional complexity assumptions, the CQs that admit such an evaluation are precisely the free-connex ones. A union of tractable CQs is always tractable. We generalize the notion of free-connexity from CQs to UCQs, thus showing that some unions containing intractable CQs are, in fact, tractable. Interestingly, some unions consisting of only intractable CQs are tractable too. The question of a finding a full characterization of the tractability of UCQs remains open. Nevertheless, we prove that for several classes of queries, free-connexity fully captures the tractable UCQs.",
  "subtype": "PODS Research",
  "authors": [
    {
      "name": "Nofar Carmeli",
      "affiliation": "Technion"
    },
    {
      "name": "Markus Kröll",
      "affiliation": "Vienna University of Technology"
    }
   ],
  "type": "PODS",
  "id": "PODS_Research_084"
},
"PODS_Research_087": {
  "title": "Better Sliding Window Algorithms to Maximize Subadditive and Diversity Objectives",
  "abstract": "The streaming computation model is a standard model for large-scale data analysis: the input arrives one element at a time, and the goal is to maintain an approximately optimal solution using only a constant, or, at worst, polylogarithmic space.</par><par>In practice, however, recency plays a large role, and one often wishes to consider only the last w elements that have arrived, the so-called sliding window problem. A trivial approach is to simply store the last w elements in a buffer; our goal is to develop algorithms with space and update time sublinear in w. In this regime, there are two frameworks: exponential histograms and smooth histograms, which can be used to obtain sliding window algorithms for families of functions satisfying certain properties.</par><par>Unfortunately, these frameworks have limitations and cannot always be applied directly. A prominent example is the problem of maximizing submodular function with cardinality constraints. While some of these difficulties can be rectified on a case-by-case basis, here, we describe an alternative approach to designing efficient sliding window algorithms for maximization problems. Then we instantiate this approach on a wide range of problems, yielding better algorithms for submodular function optimization, diversity optimization and general subadditive optimization. In doing so, we improve state-of-the art results obtained using problem-specific algorithms.",
  "subtype": "PODS Research",
  "authors": [
    {
      "name": "Michele Borassi",
      "affiliation": "Google Research"
    },
    {
      "name": "Alessandro Epasto",
      "affiliation": "Google Research"
    },
    {
      "name": "Silvio Lattanzi",
      "affiliation": "Google Research"
    },
    {
      "name": "Sergei Vassilvitskii",
      "affiliation": "Google Research"
    },
    {
      "name": "Morteza Zadimoghaddam",
      "affiliation": "Google Research"
    }
   ],
  "type": "PODS",
  "id": "PODS_Research_087"
},
"PODS_Research_098": {
  "title": "Enumeration on Trees with Tractable Combined Complexity and Efficient Updates",
  "abstract": "We give an algorithm to enumerate the results on trees of monadic second-order (MSO) queries represented by nondeterministic tree automata. After linear time preprocessing (in the input tree), we can enumerate answers with linear delay (in each answer). We allow updates on the tree to take place at any time, and we can then restart the enumeration after logarithmic time in the tree. Further, all our combined complexities are polynomial in the automaton. Our result follows our previous circuit-based enumeration algorithms based on deterministic tree automata, and is also inspired by our earlier result on words and nondeterministic sequential extended variable-set automata in the context of document spanners. We extend these results and combine them with a recent tree balancing scheme by Niewerth, so that our enumeration structure supports updates to the underlying tree in logarithmic time (with leaf insertions, leaf deletions, and node relabelings). Our result implies that, for MSO queries with free first-order variables, we can enumerate the results with linear preprocessing and constant-delay and update the underlying tree in logarithmic time, which improves on several known results for words and trees. Building on lower bounds from data structure research, we also show unconditionally that up to a doubly logarithmic factor the update time of our algorithm is optimal. Thus, unlike other settings, there can be no algorithm with constant update time.",
  "subtype": "PODS Research",
  "authors": [
    {
      "name": "Antoine Amarilli",
      "affiliation": "LTCI, CNRS, Télécom ParisTech, Université Paris-Saclay"
    },
    {
      "name": "Pierre Bourhis",
      "affiliation": "CRIStAL, CNRS UMR 9189, Inria Lille"
    },
    {
      "name": "Stefan Mengel",
      "affiliation": "CNRS, CRIL UMR 8188"
    },
    {
      "name": "Matthias Niewerth",
      "affiliation": "University of Bayreuth"
    }
   ],
  "type": "PODS",
  "id": "PODS_Research_098"
},
"PODS_Research_100": {
  "title": "Counting Database Repairs under Primary Keys Revisited",
  "abstract": "Consistent query answering (CQA) aims to deliver meaningful answers when queries are evaluated over inconsistent databases. Such answers must be certainly true in all repairs, which are consistent databases whose difference from the inconsistent one is somehow minimal. An interesting task in this context is to count the number of repairs that entail the query. This problem has been already studied for conjunctive queries and primary keys; we know that it is #P-complete in data complexity under polynomial-time Turing reductions (a.k.a. Cook reductions). However, as it has been already observed in the literature of counting complexity, there are problems that are 'hard-to-count-easy-to-decide', which cannot be complete (under reasonable assumptions) for #P under weaker reductions, and, in particular, under standard many-one logspace reductions (a.k.a. parsimonious reductions). For such 'hard-to-count-easy-to-decide' problems, a crucial question is whether we can determine their exact complexity by looking for subclasses of #P to which they belong. Ideally, we would like to show that such a problem is complete for a subclass of #P under many-one logspace reductions. The main goal of this work is to perform such a refined analysis for the problem of counting the number of repairs under primary keys that entail the query.",
  "subtype": "PODS Research",
  "authors": [
    {
      "name": "Marco Calautti",
      "affiliation": "University of Edinburgh"
    },
    {
      "name": "Marco Console",
      "affiliation": "University of Edinburgh"
    },
    {
      "name": "Andreas Pieris",
      "affiliation": "University of Edinburgh"
    }
   ],
  "type": "PODS",
  "id": "PODS_Research_100"
},
"PODS_Research_109": {
  "title": "Efficient Logspace Classes for Enumeration, Counting, and Uniform Generation",
  "abstract": "In this work, we study two simple yet general complexity classes, based on logspace Turing machines, which provide a unifying framework for efficient query evaluation in areas like information extraction and graph databases, among others. We investigate the complexity of three fundamental algorithmic problems for these classes: enumeration, counting and uniform generation of solutions, and show that they have several desirable properties in this respect. Both complexity classes are defined in terms of nondeterministic logspace transducers (NL transducers). For the first class, we consider the case of unambiguous NL transducers, and we prove constant delay enumeration, and both counting and uniform generation of solutions in polynomial time. For the second class, we consider unrestricted NL transducers, and we obtain polynomial delay enumeration, approximate counting in polynomial time, and polynomial-time randomized algorithms for uniform generation. More specifically, we show that each problem in this second class admits a fully polynomial-time randomized approximation scheme (FPRAS) and a polynomial-time Las Vegas algorithm for uniform generation. Interestingly, the key idea to prove these results is to show that the fundamental problem #NFA admits an FPRAS, where #NFA is the problem of counting the number of strings of length n accepted by a nondeterministic finite automaton (NFA). While this problem is known to be #P-complete and, more precisely, SpanL-complete, it was open whether this problem admits an FPRAS. In this work, we solve this open problem, and obtain as a welcome corollary that every function in SpanL admits an FPRAS.",
  "subtype": "PODS Research",
  "authors": [
    {
      "name": "Marcelo Arenas",
      "affiliation": "Pontificia Universidad Católica de Chile"
    },
    {
      "name": "Luis Alberto Croquevielle",
      "affiliation": "Pontificia Universidad Católica de Chile"
    },
    {
      "name": "Rajesh Jayaram",
      "affiliation": "Carnegie Mellon University"
    },
    {
      "name": "Cristian Riveros",
      "affiliation": "Pontificia Universidad Católica de Chile"
    }
   ],
  "type": "PODS",
  "id": "PODS_Research_109"
},
"PODS_Research_112": {
  "title": "Reachability in Database-driven Systems with Numerical Attributes under Recency Bounding",
  "abstract": "A prominent research direction of the database theory community is to develop techniques for verification of database-driven systems operating over relational and numerical data. Along this line, we lift the framework of database manipulating systems citeAbdullaAAMR-pods-16 which handle relational data to also accommodate numerical data and the natural order on them. We study an under-approximation called recency bounding under which the most basic verification problem --reachability, is decidable. Even under this under-approximation the reachability space is infinite in multiple dimensions -- owing to the unbounded sizes of the active domain, the unbounded numerical domain it has access to, and the unbounded length of the executions. We show that, nevertheless, reachability is ExpTime complete. Going beyond reachability to LTL model checking renders verification undecidable.",
  "subtype": "PODS Research",
  "authors": [
    {
      "name": "Parosh Aziz Abdulla",
      "affiliation": "Uppsala University"
    },
    {
      "name": "C. Aiswarya",
      "affiliation": "Chennai Mathematical Institute"
    },
    {
      "name": "Mohamed Faouzi Atig",
      "affiliation": "Uppsala University"
    },
    {
      "name": "Marco Montali",
      "affiliation": "KRDB Research Centre, Free University of Bozen-Bolzano"
    }
   ],
  "type": "PODS",
  "id": "PODS_Research_112"
},
"PODS_Research_114": {
  "title": "The Complexity of Counting Cycles in the Adjacency List Streaming Model",
  "abstract": "We study the problem of counting cycles in the adjacency list streaming model, fully resolving in which settings there exist sublinear space algorithms. Our main upper bound is a two-pass algorithm for estimating triangles that uses wtO(m/T^2/3) space, where m is the edge count and T is the triangle count of the graph. On the other hand, we show that no sublinear space multipass algorithm exists for counting ell-cycles for ell geq 5. Finally, we show that counting 4-cycles is intermediate: sublinear space algorithms exist in multipass but not single-pass settings.",
  "subtype": "PODS Research",
  "authors": [
    {
      "name": "John Kallaugher",
      "affiliation": "University of Texas at Austin"
    },
    {
      "name": "Andrew McGregor",
      "affiliation": "University of Massachusetts Amherst"
    },
    {
      "name": "Eric Price",
      "affiliation": "University of Texas at Austin"
    },
    {
      "name": "Sofya Vorotnikova",
      "affiliation": "University of Massachusetts Amherst"
    }
   ],
  "type": "PODS",
  "id": "PODS_Research_114"
},
"PODS_Research_119k": {
  "title": "Differential Privacy and the US Census",
  "abstract": "Differential privacy is a mathematically rigorous definition of privacy tailored to statistical analysis of large datasets. Differentially private systems simultaneously provide useful statistics to the well-intentioned data analyst and strong protection against arbitrarily powerful adversarial system users -- without needing to distinguish between the two. Differentially private systems 'don't care' what the adversary knows, now or in the future. Finally, differentially private systems can rigorously bound and control the cumulative privacy loss that accrues over many interactions with the confidential data. These unique properties, together with the abundance of auxiliary data sources and the ease with which they can be deployed by a privacy adversary, led the US Census Bureau to adopt differential privacy as the disclosure avoidance methodology of the 2020 decennial census. This talk will motivate the definition of differential privacy, reflect on the theory-meets-practice experiences of the decennial census, and highlight a few pressing challenges in the field.",
  "subtype": "PODS Keynote",
  "authors": [
    {
      "name": "Cynthia Dwork",
      "affiliation": "Harvard University"
    }
   ],
  "type": "PODS",
  "id": "PODS_Research_119k"
},
"PODS_Research_125g": {
  "title": "Database Repairs and Consistent Query Answering: Origins and Further Developments",
  "abstract": "In this article we review the main concepts around database repairs and consistent query answering, with emphasis on tracing back the origin, motivation, and early developments. We also describe some research directions that has spun from those main concepts and the original line of research. We emphasize, in particular, fruitful and recent connections between repairs and causality in databases.",
  "subtype": "Gems of PODS",
  "authors": [
    {
      "name": "Leopoldo Bertossi",
      "affiliation": "RelationalAI & Carleton University"
    }
   ],
  "type": "PODS",
  "id": "PODS_Research_125g"
},
"PODS_Research_128t": {
  "title": "Making Consistency Protocols Serializable",
  "abstract": "A tutorial given at PODS 2019, focussed on several research agendas in the past decade or so, that examine weak isolation levels, and obtain many (or all) of the benefits for application integrity, traditionally achieved by serializable concurrency control. The tutorial presents both the mechanisms and the reasoning approaches from these research works.",
  "subtype": "PODS Tutorial",
  "authors": [
    {
      "name": "Alan Fekete",
      "affiliation": "University of Sydney"
    }
   ],
  "type": "PODS",
  "id": "PODS_Research_128t"
},
"PODS_Research_131t": {
  "title": "Algorithmic Fairness: Measures, Methods and Representations",
  "abstract": "What happens when we replace — or augment — human decision-making with algorithms? This is a simple question, but the answers now define a new field of study — a field that I call algorithmic fairness, and that spans issues of fairness, discrimination, accountability, transparency, interpretability and responsibility, and so much more. While some of the early work in the area came out of data mining and machine learning, the field is now truly transdisciplinary, with contributions from all across computer science, as well as from all disciplines that touch on aspects of society - whether it be economics, philosophy, sociology, political science, or communication. In this tutorial, I’ll try to do three things: I’ll survey the main questions and some of the key insights we’ve developed over the years. I’ll explain the web of connections between the technical and the social disciplines that make up this area, and I’ll point to exciting directions that remain to be explored in both technical and social dimensions. Along the way I hope to illustrate what I think are some interesting “collisions” between computer science and the social sciences, and call for a reimagining of core ideas in our field, including the very idea of how we think about data representation.",
  "subtype": "PODS Tutorial",
  "authors": [
    {
      "name": "Suresh Venkatasubramanian",
      "affiliation": "University of Utah"
    }
   ],
  "type": "PODS",
  "id": "PODS_Research_131t"
},
"PODS_Research_134m": {
  "title": "2019 ACM PODS Alberto O. Mendelzon Test-of-Time Award",
  "abstract": "",
  "subtype": "PODS Award Talk",
  "authors": [
    {
      "name": "Jianwen Su",
      "affiliation": "University of California, Santa Barbara"
    },
    {
      "name": "Dirk Van Gucht",
      "affiliation": "Indiana University"
    },
    {
      "name": "Victor Vianu",
      "affiliation": "University of California, San Diego"
    }
   ],
  "type": "PODS",
  "id": "PODS_Research_134m"
},
"PODS_Research_tot": {
  "title": "A General Datalog-based Framework for Tractable Query Answering",
  "abstract": "",
  "subtype": "PODS Tutorial",
  "authors": [
    {
      "name": "Andrea Cali",
      "affiliation": "Birkbeck College"
    },
    {
      "name": "Georg Gottlob",
      "affiliation": "University of Oxford"
    },
    {
      "name": "Thomas Lukasiewicz",
      "affiliation": "University of Oxford"
    }
   ],
  "type": "PODS",
  "id": "PODS_Research_tot"
},
"PODS_Research_122g": {
  "title": "Remembering the Probabilistic Analysis of Latent Semantic Indexing",
  "abstract": "",
  "subtype": "Gems of PODS",
  "authors": [
    {
      "name": "Christos Papadimitriou",
      "affiliation": "Columbia University"
    }
   ],
  "type": "PODS",
  "id": "PODS_Research_122g"
},
"SIGMOD_Industrial_640": {
  "title": "QuickInsights: Quick and Automatic Discovery of Insights from Multi-Dimensional Data",
  "abstract": "Discovering interesting data patterns is a common and important analytical need in data, with increasing user demand for automated discovery abilities. However, automatically discovering interesting patterns from multi-dimensional data remains challenging. Existing techniques focus on mining individual types of patterns. There is a lack of unified formulation for different pattern types, as well as general mining frameworks to derive them effectively and efficiently. We present a novel technique QuickInsights, which quickly and automatically discovers interesting patterns from multi-dimensional data. QuickInsights proposes a unified formulation of interesting patterns, called insights, and designs a systematic mining framework to discover high-quality insights efficiently. We demonstrate the effectiveness and efficiency of QuickInsights through our evaluation on 447 real datasets as well as user studies on both expert users and non-expert users. QuickInsights is released in Microsoft Power BI.",
  "subtype": "SIGMOD Industrial",
  "authors": [
    {
      "name": "Rui Ding",
      "affiliation": "Microsoft Research"
    },
    {
      "name": "Shi Han",
      "affiliation": "Microsoft Research"
    },
    {
      "name": "Yong Xu",
      "affiliation": "Microsoft Research"
    },
    {
      "name": "Haidong Zhang",
      "affiliation": "Microsoft Research"
    },
    {
      "name": "Dongmei Zhang",
      "affiliation": "Microsoft Research"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Industrial_640"
},
"SIGMOD_Industrial_642": {
  "title": "CFS: A Distributed File System for Large Scale Container Platforms",
  "abstract": "We propose CFS, a distributed file system for large scale container platforms. CFS supports both sequential and random file accesses with optimized storage for both large files and small files, and adopts different replication protocols for different write scenarios to improve the replication performance. It employs a metadata subsystem to store and distribute the file metadata across different storage nodes based on the memory usage. This metadata placement strategy avoids the need of data rebalancing during capacity expansion. CFS also provides POSIX-compliant APIs with relaxed semantics and metadata atomicity to improve the system performance. We performed a comprehensive comparison with Ceph, a widely-used distributed file system on container platforms. Our experimental results show that, in testing 7 commonly used metadata operations, CFS gives around 3 times performance boost on average. In addition, CFS exhibits better random-read/write performance in highly concurrent environments with multiple clients and processes.",
  "subtype": "SIGMOD Industrial",
  "authors": [
    {
      "name": "Haifeng Liu",
      "affiliation": "University of Science and Technology of China"
    },
    {
      "name": "Wei Ding",
      "affiliation": "JD.com"
    },
    {
      "name": "Yuan Chen",
      "affiliation": "JD.com"
    },
    {
      "name": "Weilong Guo",
      "affiliation": "JD.com"
    },
    {
      "name": "Shuoran Liu",
      "affiliation": "JD.com"
    },
    {
      "name": "Tianpeng Li",
      "affiliation": "JD.com"
    },
    {
      "name": "Mofei Zhang",
      "affiliation": "JD.com"
    },
    {
      "name": "Jianxing Zhao",
      "affiliation": "JD.com"
    },
    {
      "name": "Hongyin Zhu",
      "affiliation": "JD.com"
    },
    {
      "name": "Zhengyi Zhu",
      "affiliation": "JD.com"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Industrial_642"
},
"SIGMOD_Industrial_649": {
  "title": "ExplainIt! – A Declarative Root-cause Analysis Engine for Time Series Data",
  "abstract": "We present sys, a declarative, unsupervised root-cause analysis engine that uses time series monitoring data from large complex  systems such as data centres. sys empowers operators to succinctly specify a large number of causal hypotheses to search for causes of  interesting events. sys then ranks these hypotheses, reducing the number of causal dependencies from hundreds of thousands to a  handful for human understanding. We show how a declarative language, such as SQL, can be effective in declaratively enumerating  hypotheses that probe the structure of an unknown probabilistic  graphical causal model of the underlying system. Our thesis is that  databases are in a unique position to enable users to rapidly  explore the possible causal mechanisms in data collected from  diverse sources. We empirically demonstrate how sys had helped us  resolve over 30~performance issues in a commercial product since  late 2014, of which we discuss a few cases in detail.",
  "subtype": "SIGMOD Industrial",
  "authors": [
    {
      "name": "Vimalkumar Jeyakumar",
      "affiliation": "Cisco Tetration Analytics"
    },
    {
      "name": "Omid Madani",
      "affiliation": "Cisco Tetration Analytics"
    },
    {
      "name": "Ali Parandeh",
      "affiliation": "Cisco Tetration Analytics"
    },
    {
      "name": "Ashutosh Kulshreshtha",
      "affiliation": "Cisco Tetration Analytics"
    },
    {
      "name": "Weifei Zeng",
      "affiliation": "Cisco Tetration Analytics"
    },
    {
      "name": "Navindra Yadav",
      "affiliation": "Cisco Tetration Analytics"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Industrial_649"
},
"SIGMOD_Industrial_651": {
  "title": "Automatically Indexing Millions of Databases in Microsoft Azure SQL Database",
  "abstract": "An appropriate set of indexes can result in orders of magnitude better query performance. Index management is a challenging task even for expert human administrators. Fully automating this process is of significant value. We describe the challenges, architecture, design choices, implementation, and learnings from building an industrial-strength auto-indexing service for Microsoft Azure SQL Database, a relational database service. Our service has been generally available for more than two years, generating index recommendations for every database in Azure SQL Database, automatically implementing them for a large fraction, and significantly improving performance of hundreds of thousands of databases. We also share our experience from experimentation at scale with production databases which gives us confidence in our index recommendation quality for complex real applications.",
  "subtype": "SIGMOD Industrial",
  "authors": [
    {
      "name": "Sudipto Das",
      "affiliation": "Microsoft"
    },
    {
      "name": "Miroslav Grbic",
      "affiliation": "Microsoft"
    },
    {
      "name": "Igor Ilic",
      "affiliation": "Microsoft"
    },
    {
      "name": "Isidora Jovandic",
      "affiliation": "Microsoft"
    },
    {
      "name": "Andrija Jovanovic",
      "affiliation": "Microsoft"
    },
    {
      "name": "Vivek Narasayya",
      "affiliation": "Microsoft"
    },
    {
      "name": "Miodrag Radulovic",
      "affiliation": "Microsoft"
    },
    {
      "name": "Maja Stikic",
      "affiliation": "Microsoft"
    },
    {
      "name": "Gaoxiang Xu",
      "affiliation": "Microsoft"
    },
    {
      "name": "Surajit Chaudhuri",
      "affiliation": "Microsoft"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Industrial_651"
},
"SIGMOD_Industrial_653": {
  "title": "Socrates: The New SQL Server in the Cloud",
  "abstract": "The database-as-a-service paradigm in the cloud (DBaaS) is becoming increasingly popular. Organizations adopt this paradigm because they expect higher security, higher availability, and lower and more flexible cost with high performance. It has become clear, however, that these expectations cannot be met in the cloud with the traditional, monolithic database architecture. This paper presents a novel DBaaS architecture, called Socrates. Socrates has been implemented in Microsoft SQL Server and is available in Azure as SQL DB Hyperscale. This paper describes the key ideas and features of Socrates, and it compares the performance of Socrates with the previous SQL DB offering in Azure.",
  "subtype": "SIGMOD Industrial",
  "authors": [
    {
      "name": "Panagiotis Antonopoulos",
      "affiliation": "Microsoft"
    },
    {
      "name": "Alex Budovski",
      "affiliation": "Microsoft"
    },
    {
      "name": "Cristian Diaconu",
      "affiliation": "Microsoft"
    },
    {
      "name": "Alejandro Hernandez",
      "affiliation": "Microsoft"
    },
    {
      "name": "Jack Hu",
      "affiliation": "Microsoft"
    },
    {
      "name": "Hanuma Kodavalla",
      "affiliation": "Microsoft"
    },
    {
      "name": "Donald Kossmann",
      "affiliation": "Microsoft Research"
    },
    {
      "name": "Umar Farooq Minhas",
      "affiliation": "Microsoft Research"
    },
    {
      "name": "Naveen Prakash",
      "affiliation": "Microsoft"
    },
    {
      "name": "Vijendra Purohit",
      "affiliation": "Microsoft"
    },
    {
      "name": "Hugh Qu",
      "affiliation": "Microsoft"
    },
    {
      "name": "Chaitanya Sreenivas Ravella",
      "affiliation": "Microsoft"
    },
    {
      "name": "Krystyna Reisteter",
      "affiliation": "Microsoft"
    },
    {
      "name": "Sheetal Shrotri",
      "affiliation": "Microsoft"
    },
    {
      "name": "Dixin Tang",
      "affiliation": "University of Chicago"
    },
    {
      "name": "Vikram Wakade",
      "affiliation": "Microsoft"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Industrial_653"
},
"SIGMOD_Industrial_655": {
  "title": "Implementation of Cluster-wide Logical Clock and Causal Consistency in MongoDB",
  "abstract": "MongoDB is a distributed database that supports replication and horizontal partitioning (sharding). MongoDB replica sets consist of a primary that accepts all client writes and then propagates those writes to the secondaries. Each member of the replica set contains the same set of data. For horizontal partitioning, each shard (or partition) is a replica set. This paper discusses the design and rationale behind MongoDB’s implementation of a cluster-wide logical clock and causal consistency. The design leveraged ideas from across the research community to ensure that the implementation adds minimal processing overhead, tolerates possible operator errors, and gives protection against non-trusted client attacks. While the goal of the team was not to discover or test new algorithms, the practical implementation necessitated a novel combination of ideas from the research community on causal consistency, security, and minimal performance overhead at scale. This paper describes a large scale, practical implementation of causal consistency using a hybrid logical clock, adding the signing of logical time ranges to the protocol, and introducing performance optimizations necessary for systems at scale. The implementation seeks to define an event as a state change and as such must make forward progress guarantees even during periods of no state changes for a partition of data.",
  "subtype": "SIGMOD Industrial",
  "authors": [
    {
      "name": "Misha Tyulenev",
      "affiliation": "MongoDB, Inc"
    },
    {
      "name": "Andy Schwerin",
      "affiliation": "MongoDB, Inc"
    },
    {
      "name": "Asya Kamsky",
      "affiliation": "MongoDB, Inc"
    },
    {
      "name": "Randolph Tan",
      "affiliation": "MongoDB, Inc"
    },
    {
      "name": "Alyson Cabral",
      "affiliation": "MongoDB, Inc"
    },
    {
      "name": "Jack Mulrow",
      "affiliation": "MongoDB, Inc"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Industrial_655"
},
"SIGMOD_Industrial_657": {
  "title": "Nanosecond Indexing of Graph Data With Hash Maps and VLists",
  "abstract": "We introduce a wait-free, multi-reader, single-writer, kill -9 durable, indexing structure for in-memory social graph databases. This structure requires no communication from the readers back to the writer, allowing for trivial read scalability and isolation. We support online updates without compromising availability or read performance. Our structure supports looking up small subgraphs in 80 nanoseconds and a materialization rate of 12 nanoseconds per edge. Storage takes 7 bytes per edge per index and supports almost 1 million online writes per second.",
  "subtype": "SIGMOD Industrial",
  "authors": [
    {
      "name": "Andrew Carter",
      "affiliation": "LinkedIn Corporation"
    },
    {
      "name": "Andrew Rodriguez",
      "affiliation": "LinkedIn Corporation"
    },
    {
      "name": "Yiming Yang",
      "affiliation": "LinkedIn Corporation"
    },
    {
      "name": "Scott Meyer",
      "affiliation": "LinkedIn Corporation"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Industrial_657"
},
"SIGMOD_Industrial_661": {
  "title": "Automatically Generating Interesting Facts from Wikipedia Tables",
  "abstract": "Modern search engines provide contextual information surrounding query entities beyond ten blue links in the form of information cards.  Among the various attributes displayed about entities there has been recent interest in providing fun facts. Obtaining such trivia at a large scale is, however, non-trivial: hiring professional content creators is expensive and extracting statements from the Web is prone to uninteresting, out-of-context and/or unreliable facts. In this paper we show how fun facts can be mined from superlative tables in Wikipedia, whose rows are ranked according to some statistics, to provide a large volume of reliable and interesting content. We employ a template-based approach to semi-automatically generate natural language statements as fun facts. We show how to bootstrap and streamline the process for faster and cheaper task completion.  However, the content contained in these tables is dynamic. Therefore, we address the problem of automatically maintaining the pairing of templates to tables as the tables are updated over time. Fun facts produced by our work is now part of Google's production search results.",
  "subtype": "SIGMOD Industrial",
  "authors": [
    {
      "name": "Flip Korn",
      "affiliation": "Google Research"
    },
    {
      "name": "Xuezhi Wang",
      "affiliation": "Google Research"
    },
    {
      "name": "You Wu",
      "affiliation": "Google Research"
    },
    {
      "name": "Cong Yu",
      "affiliation": "Google Research"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Industrial_661"
},
"SIGMOD_Industrial_665": {
  "title": "X-Engine: An Optimized Storage Engine for Large-scale E-commerce Transaction Processing",
  "abstract": "Alibaba runs the largest e-commerce platform in the world serving more than 600 million customers, with a GMV (gross merchandise value) exceeding USD 768 billion in FY2018. Online e-commerce transactions have three notable characteristics: (1) drastic increase of transactions per second with the kickoff of major sales and promotion events, (2) a large number of hot records that can easily overwhelm system buffers, and (3) quick shift of the 'temperature' (hot v.s. warm v.s. cold) of different records due to the availability of promotions on different categories over different short time periods. For example, Alibaba's OLTP database clusters experienced a 122 times increase of transactions on the start of the Singles' Day Global Shopping Festival in 2018, processing up to 491,000 sales transactions per second which translate to more than 70 million database transactions per second. To address these challenges, we introduce X-Engine, a write-optimized storage engine of POLARDB built at Alibaba, which utilizes a tiered storage architecture with the LSM-tree (log-structured merge tree) to leverage hardware acceleration such as FPGA-accelerated compactions, and a suite of optimizations including asynchronous writes in transactions, multi-staged pipelines and incremental cache replacement during compactions. Evaluation results show that X-Engine has outperformed other storage engines under such transactional workloads.",
  "subtype": "SIGMOD Industrial",
  "authors": [
    {
      "name": "Gui Huang",
      "affiliation": "Alibaba Group"
    },
    {
      "name": "Xuntao Cheng",
      "affiliation": "Alibaba Group"
    },
    {
      "name": "Jianying Wang",
      "affiliation": "Alibaba Group"
    },
    {
      "name": "Yujie Wang",
      "affiliation": "Alibaba Group"
    },
    {
      "name": "Dengcheng He",
      "affiliation": "Alibaba Group"
    },
    {
      "name": "Tieying Zhang",
      "affiliation": "Alibaba Group"
    },
    {
      "name": "Feifei Li",
      "affiliation": "Alibaba Group"
    },
    {
      "name": "Sheng Wang",
      "affiliation": "Alibaba Group"
    },
    {
      "name": "Wei Cao",
      "affiliation": "Alibaba Group"
    },
    {
      "name": "Qiang Li",
      "affiliation": "Alibaba Group"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Industrial_665"
},
"SIGMOD_Industrial_674": {
  "title": "FoundationDB Record Layer: A Multi-Tenant Structured Datastore",
  "abstract": "The FoundationDB Record Layer is an open source library that provides a record-oriented data store with semantics similar to a relational database implemented on top of FoundationDB, an ordered, transactional key-value store. The Record Layer provides a lightweight, highly extensible way to store structured data. It offers schema management and a rich set of query and indexing facilities, some of which are not usually found in traditional relational databases, such as nested record types, indexes on commit versions, and indexes that span multiple record types. The Record Layer is stateless and built for massive multi-tenancy, encapsulating and isolating all of a tenant's state, including indexes, into a separate logical database. We demonstrate how the Record Layer is used by CloudKit, Apple's cloud backend service, to provide powerful abstractions to applications serving hundreds of millions of users. CloudKit uses the Record Layer to host billions of independent databases, many with a common schema. Features provided by the Record Layer enable CloudKit to provide richer APIs and stronger semantics with reduced maintenance overhead and improved scalability.",
  "subtype": "SIGMOD Industrial",
  "authors": [
    {
      "name": "Christos Chrysafis",
      "affiliation": "Apple"
    },
    {
      "name": "Ben Collins",
      "affiliation": "Apple"
    },
    {
      "name": "Scott Dugas",
      "affiliation": "Apple"
    },
    {
      "name": "Jay Dunkelberger",
      "affiliation": "Apple"
    },
    {
      "name": "Moussa Ehsan",
      "affiliation": "Apple"
    },
    {
      "name": "Scott Gray",
      "affiliation": "Apple"
    },
    {
      "name": "Alec Grieser",
      "affiliation": "Apple"
    },
    {
      "name": "Ori Herrnstadt",
      "affiliation": "Apple"
    },
    {
      "name": "Kfir Lev-Ari",
      "affiliation": "Apple"
    },
    {
      "name": "Tao Lin",
      "affiliation": "Apple"
    },
    {
      "name": "Mike McMahon",
      "affiliation": "Apple"
    },
    {
      "name": "Nicholas Schiefer",
      "affiliation": "Apple"
    },
    {
      "name": "Alexander Shraer",
      "affiliation": "Apple"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Industrial_674"
},
"SIGMOD_Industrial_676": {
  "title": "Snorkel DryBell: A Case Study in Deploying Weak Supervision at Industrial Scale",
  "abstract": "Labeling training data is one of the most costly bottlenecks in developing machine learning-based applications. We present a first-of-its-kind study showing how existing knowledge resources from across an organization can be used as weak supervision in order to bring development time and cost down by an order of magnitude, and introduce Snorkel DryBell, a new weak supervision management system for this setting. Snorkel DryBell builds on the Snorkel framework, extending it in three critical aspects: flexible, template-based ingestion of diverse organizational knowledge, cross-feature production serving, and scalable, sampling-free execution. On three classification tasks at Google, we find that Snorkel DryBell creates classifiers of comparable quality to ones trained with tens of thousands of hand-labeled examples, converts non-servable organizational resources to servable models for an average 52% performance improvement, and executes over millions of data points in tens of minutes.",
  "subtype": "SIGMOD Industrial",
  "authors": [
    {
      "name": "Stephen Bach",
      "affiliation": "Brown University"
    },
    {
      "name": "Daniel Rodriguez",
      "affiliation": "Google"
    },
    {
      "name": "Yintao Liu",
      "affiliation": "Google"
    },
    {
      "name": "Chong Luo",
      "affiliation": "Google"
    },
    {
      "name": "Haidong Shao",
      "affiliation": "Google"
    },
    {
      "name": "Cassandra Xia",
      "affiliation": "Google"
    },
    {
      "name": "Souvik Sen",
      "affiliation": "Google"
    },
    {
      "name": "Alex Ratner",
      "affiliation": "Stanford University"
    },
    {
      "name": "Braden Hancock",
      "affiliation": "Stanford University"
    },
    {
      "name": "Houman Alborzi",
      "affiliation": "Google"
    },
    {
      "name": "Rahul Kuchhal",
      "affiliation": "Google"
    },
    {
      "name": "Chris Ré",
      "affiliation": "Stanford University"
    },
    {
      "name": "Rob Malkin",
      "affiliation": "Google"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Industrial_676"
},
"SIGMOD_Industrial_679": {
  "title": "PS2: Parameter Server on Spark",
  "abstract": "Most of the data is extracted and processed by Spark in Tencent Machine Learning Platform. However, seldom of them use Spark MLlib, an official machine learning (ML) library on top of Spark due to its inefficiency. In contrast, systems like parameter servers, XGBoost and TensorFlow are more used, which incur expensive cost of transferring data in and out of Spark ecosystem. In this paper, we identify the causes of inefficiency in Spark MLlib and solve the problem by building parameter servers on top of Spark. We propose PS2, a parameter server architecture that integrates Spark without hacking the core of Spark. With PS2, we leverage the power of Spark for data processing and ML training, and parameter servers for maintaining ML models. By carefully analyzing Tencent ML workloads, we figure out a widely existing computation pattern for ML models-element-wise operations among multiple high dimensional vectors. Based on this observation, we propose a new data abstraction, called Dimension Co-located Vector (DCV) for efficient model management in PS2. A DCV is a distributed vector that considers locality in parameter servers and enables efficient computation with multiple co-located distributed vectors. For ease-of-use, we also design a wide variety of advanced operators for operating DCVs. Finally, we carefully implement the PS2 system and evaluate it against existing systems on both public and Tencent workloads. Empirical results demonstrate that PS2 can outperform Spark MLlib by up to 55.6X and specialized ML systems like Petuum by up to 3.7X.",
  "subtype": "SIGMOD Industrial",
  "authors": [
    {
      "name": "Zhipeng Zhang",
      "affiliation": "Peking University & Tencent Inc."
    },
    {
      "name": "Bin Cui",
      "affiliation": "Peking University"
    },
    {
      "name": "Yingxia Shao",
      "affiliation": "Beijing University of Posts and Telecommunications"
    },
    {
      "name": "Lele Yu",
      "affiliation": "Tencent Inc."
    },
    {
      "name": "Jiawei Jiang",
      "affiliation": "Tencent Inc."
    },
    {
      "name": "Xupeng Miao",
      "affiliation": "Peking University & Tencent Inc."
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Industrial_679"
},
"SIGMOD_Industrial_689": {
  "title": "One SQL to Rule Them All - an Efficient and Syntactically Idiomatic Approach to Management of Streams and Tables",
  "abstract": "Real-time data analysis and management are increasingly critical for today’s businesses. SQL is the de facto lingua franca for these endeavors, yet support for robust streaming analysis and management with SQL remains limited. Many approaches restrict semantics to a reduced subset of features and/or require a suite of non-standard constructs. Additionally, use of event timestamps to provide native support for analyzing events according to when they actually occurred is not pervasive, and often comes with important limitations. We present a three-part proposal for integrating robust streaming into SQL, namely: (1) time-varying relations as a foundation for classical tables as well as streaming data, (2) event time semantics, (3) a limited set of optional keyword extensions to control the materialization of time-varying query results. We show how with these minimal additions it is possible to utilize the complete suite of standard SQL semantics to perform robust stream processing. We motivate and illustrate these concepts using examples and describe lessons learned from implementations in Apache Calcite, Apache Flink, and Apache Beam. We conclude with syntax and semantics of a concrete proposal for extensions of the SQL standard and note further areas of exploration.",
  "subtype": "SIGMOD Industrial",
  "authors": [
    {
      "name": "Edmon Begoli",
      "affiliation": "Oak Ridge National Laboratory"
    },
    {
      "name": "Tyler Akidau",
      "affiliation": "Google"
    },
    {
      "name": "Fabian Hueske",
      "affiliation": "Ververica"
    },
    {
      "name": "Julian Hyde",
      "affiliation": "Looker Inc."
    },
    {
      "name": "Kathryn Knight",
      "affiliation": "Oak Ridge National Laboratory"
    },
    {
      "name": "Kenneth Knowles",
      "affiliation": "Google"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Industrial_689"
},
"SIGMOD_Industrial_691": {
  "title": "Apache Hive: From MapReduce to Enterprise-grade Big Data Warehousing",
  "abstract": "Apache Hive is an open-source relational database system for analytic big-data workloads. In this paper we describe the key innovations on the journey from batch tool to fully fledged enterprise data warehousing system. We present a hybrid architecture that combines traditional MPP techniques with more recent big data and cloud concepts to achieve the scale and performance required by today's analytic applications. We explore the system by detailing enhancements along four main axis: Transactions, optimizer, runtime, and federation. We then provide experimental results to demonstrate the performance of the system for typical workloads and conclude with a look at the community roadmap.",
  "subtype": "SIGMOD Industrial",
  "authors": [
    {
      "name": "Jesús Camacho-Rodríguez",
      "affiliation": "Hortonworks"
    },
    {
      "name": "Ashutosh Chauhan",
      "affiliation": "Hortonworks"
    },
    {
      "name": "Alan Gates",
      "affiliation": "Hortonworks"
    },
    {
      "name": "Eugene Koifman",
      "affiliation": "Hortonworks"
    },
    {
      "name": "Owen O'Malley",
      "affiliation": "Hortonworks"
    },
    {
      "name": "Vineet Garg",
      "affiliation": "Hortonworks"
    },
    {
      "name": "Zoltan Haindrich",
      "affiliation": "Hortonworks"
    },
    {
      "name": "Sergey Shelukhin",
      "affiliation": "Hortonworks"
    },
    {
      "name": "Prasanth Jayachandran",
      "affiliation": "Hortonworks"
    },
    {
      "name": "Siddharth Seth",
      "affiliation": "Hortonworks"
    },
    {
      "name": "Deepak Jaiswal",
      "affiliation": "Hortonworks"
    },
    {
      "name": "Slim Bouguerra",
      "affiliation": "Hortonworks"
    },
    {
      "name": "Nishant Bangarwa",
      "affiliation": "Hortonworks"
    },
    {
      "name": "Sankar Hariappan",
      "affiliation": "Hortonworks"
    },
    {
      "name": "Anishek Agarwal",
      "affiliation": "Hortonworks"
    },
    {
      "name": "Jason Dere",
      "affiliation": "Hortonworks"
    },
    {
      "name": "Daniel Dai",
      "affiliation": "Hortonworks"
    },
    {
      "name": "Thejas Nair",
      "affiliation": "Hortonworks"
    },
    {
      "name": "Nita Dembla",
      "affiliation": "Hortonworks"
    },
    {
      "name": "Gopal Vijayaraghavan",
      "affiliation": "Hortonworks"
    },
    {
      "name": "Günther Hagleitner",
      "affiliation": "Hortonworks"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Industrial_691"
},
"SIGMOD_Industrial_693": {
  "title": "Entity Matching Meets Data Science: A Progress Report from the Magellan Project",
  "abstract": "Entity matching (EM) finds data instances that refer to the same real-world entity. In 2015, we started the Magellan project at  UW-Madison, joint with industrial partners, to build EM systems. Most current EM systems are stand-alone monoliths. In  contrast, Magellan borrows ideas from the field of data science (DS), to build a new kind of EM systems, which is an ecosystem of  interoperable tools. This paper provides a progress report on  the past 3.5 years of Magellan, focusing on the system aspects and   on how ideas from the field of data science have been adapted to   the EM context. We argue why EM can be viewed as a  special class of DS problems, and thus can benefit from system  building ideas in DS. We discuss how these ideas have been adapted to build pymatcher and cloudmatcher, EM tools for power users and  lay users. These tools have been successfully used in 21 EM tasks at 12 companies and domain science groups, and have been pushed into  production for many customers. We report on the lessons learned, and  outline a new envisioned Magellan ecosystem, which consists of not  just on-premise Python tools, but also interoperable microservices deployed, executed, and scaled out on the cloud, using tools such as Dockers and Kubernetes.",
  "subtype": "SIGMOD Industrial",
  "authors": [
    {
      "name": "Yash Govind",
      "affiliation": "University of Wisconsin, Madison"
    },
    {
      "name": "Pradap Konda",
      "affiliation": "University of Wisconsin, Madison"
    },
    {
      "name": "Paul Suganthan G.C.",
      "affiliation": "Google"
    },
    {
      "name": "Philip Martinkus",
      "affiliation": "University of Wisconsin, Madison"
    },
    {
      "name": "Palaniappan Nagarajan",
      "affiliation": "University of Wisconsin, Madison"
    },
    {
      "name": "Aravind Soundararajan",
      "affiliation": "University of Wisconsin, Madison"
    },
    {
      "name": "Han Li",
      "affiliation": "University of Wisconsin, Madison"
    },
    {
      "name": "Sidharth Mudgal",
      "affiliation": "University of Wisconsin, Madison"
    },
    {
      "name": "Jeff Ballard",
      "affiliation": "University of Wisconsin, Madison"
    },
    {
      "name": "Haojun Zhang",
      "affiliation": "University of Wisconsin, Madison"
    },
    {
      "name": "Adel Ardalan",
      "affiliation": "University of Wisconsin, Madison"
    },
    {
      "name": "Sanjib Das",
      "affiliation": "University of Wisconsin, Madison"
    },
    {
      "name": "Derek Paulsen",
      "affiliation": "University of Wisconsin, Madison"
    },
    {
      "name": "Amanpreet Singh Saini",
      "affiliation": "University of Wisconsin, Madison"
    },
    {
      "name": "Erik Paulson",
      "affiliation": "University of Wisconsin, Madison"
    },
    {
      "name": "Youngchoon Park",
      "affiliation": "Johnson Controls"
    },
    {
      "name": "Marshall Carter",
      "affiliation": "American Family Insurance"
    },
    {
      "name": "Mingju Sun",
      "affiliation": "American Family Insurance"
    },
    {
      "name": "Glenn Fung",
      "affiliation": "American Family Insurance"
    },
    {
      "name": "AnHai Doan",
      "affiliation": "University of Wisconsin, Madison"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Industrial_693"
},
"SIGMOD_Industrial_694": {
  "title": "Data Platform for Machine Learning",
  "abstract": "In this paper, we present a purpose-built data management system, MLdp, for all machine learning (ML) datasets. ML applications pose some unique requirements different from common conventional data processing applications, including but not limited to: data lineage and provenance tracking, rich data semantics and formats, integration with diverse ML frameworks and access patterns, trial-and-error driven data exploration and evolution, rapid experimentation, reproducibility of the model training, strict compliance and privacy regulations, etc. Current ML systems/services, often named MLaaS, to-date focus on the ML algorithms, and offer no integrated data management system. Instead, they require users to bring their own data and to manage their own data on either blob storage or on file systems. The burdens of data management tasks, such as versioning and access control, fall onto the users, and not all compliance features, such as terms of use, privacy measures, and auditing, are available. MLdp offers a minimalist and flexible data model for all varieties of data, strong version management to guarantee re-producibility of ML experiments, and integration with major ML frameworks. MLdp also maintains the data provenance to help users track lineage and dependencies among data versions and models in their ML pipelines. In addition to table-stake features, such as security, availability and scalability, MLdp's internal design choices are strongly influenced by the goal to support rapid ML experiment iterations, which cycle through data discovery, data exploration, feature engineering, model training, model evaluation, and back to data discovery. The contributions of this paper are: 1) to recognize the needs and to call out the requirements of an ML data platform, 2) to share our experiences in building MLdp by adopting existing database technologies to the new problem as well as by devising new solutions, and 3) to call for actions from our communities on future challenges.",
  "subtype": "SIGMOD Industrial",
  "authors": [
    {
      "name": "Pulkit Agrawal",
      "affiliation": "Apple"
    },
    {
      "name": "Rajat Arya",
      "affiliation": "Apple"
    },
    {
      "name": "Aanchal Bindal",
      "affiliation": "Apple"
    },
    {
      "name": "Sandeep Bhatia",
      "affiliation": "Apple"
    },
    {
      "name": "Anupriya Gagneja",
      "affiliation": "Apple"
    },
    {
      "name": "Joseph Godlewski",
      "affiliation": "Apple"
    },
    {
      "name": "Yucheng Low",
      "affiliation": "Apple"
    },
    {
      "name": "Timothy Muss",
      "affiliation": "Apple"
    },
    {
      "name": "Mudit Manu Paliwal",
      "affiliation": "Apple"
    },
    {
      "name": "Sethu Raman",
      "affiliation": "Apple"
    },
    {
      "name": "Vishrut Shah",
      "affiliation": "Apple"
    },
    {
      "name": "Bochao Shen",
      "affiliation": "Apple"
    },
    {
      "name": "Laura Sugden",
      "affiliation": "Apple"
    },
    {
      "name": "Kaiyu Zhao",
      "affiliation": "Apple"
    },
    {
      "name": "Ming-Chuan Wu",
      "affiliation": "Apple"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Industrial_694"
},
"SIGMOD_Demo_696": {
  "title": "Peering through the Dark: An Owl's View of Inter-job Dependencies and Jobs' Impact in Shared Clusters",
  "abstract": "Shared multi-tenant infrastructures have enabled companies to consolidate workloads and data, increasing data-sharing and cross-organizational re-use of job outputs. This same resource- and work-sharing has also increased the risk of missed deadlines and diverging priorities as recurring jobs and workflows developed by different teams evolve independently. To prevent incidental business disruptions, identifying and managing job dependencies with clarity becomes increasingly important. Owl is a cluster log analysis and visualization tool that (i) extracts and visualizes job dependencies derived from historical job telemetry and data provenance data sets, and (ii) introduces a novel job valuation algorithm estimating the impact of a job on dependent users and jobs. This demonstration showcases Owl's features that can help users identify critical job dependencies and quantify job importance based on jobs' impact.",
  "subtype": "SIGMOD Demo",
  "authors": [
    {
      "name": "Andrew Chung",
      "affiliation": "Carnegie Mellon University"
    },
    {
      "name": "Carlo Curino",
      "affiliation": "Microsoft"
    },
    {
      "name": "Subru Krishnan",
      "affiliation": "Microsoft"
    },
    {
      "name": "Konstantinos Karanasos",
      "affiliation": "Microsoft"
    },
    {
      "name": "Panagiotis Garefalakis",
      "affiliation": "Imperial College London"
    },
    {
      "name": "Gregory Ganger",
      "affiliation": "Carnegie Mellon University"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Demo_696"
},
"SIGMOD_Demo_697": {
  "title": "Pivotal Greenplum© for Kubernetes: Demonstration of Managing Greenplum Database on Kubernetes",
  "abstract": "Greenplum Database (GPDB) has many features designed to enable data scientists. Before a data scientist can use GPDB, a database administrator (DBA) must provision a cluster and install any required data science packages. Provisioning a GPDB cluster on bare metal requires a lengthy setup process. Scaling, recovering, and securing the cluster post-deployment are also complex. Greenplum for Kubernetes (GP4K) abstracts away these complexities, simplifying and automating the process for users. In this demonstration, we introduce GP4K with an opinionated deployment and a declarative manifest. We provide a brief overview of GP4K’s architecture and discuss its implementation. We also demonstrate a full life cycle of managing a cluster from birth to retirement, including scale-up and self-healing all with minimal DBA inputs.",
  "subtype": "SIGMOD Demo",
  "authors": [
    {
      "name": "Jemish Patel",
      "affiliation": "Pivotal Software Inc"
    },
    {
      "name": "Goutam Tadi",
      "affiliation": "Pivotal Software Inc"
    },
    {
      "name": "Oz Basarir",
      "affiliation": "Pivotal Software Inc"
    },
    {
      "name": "Lawrence Hamel",
      "affiliation": "Pivotal Software Inc"
    },
    {
      "name": "David Sharp",
      "affiliation": "Pivotal Software Inc"
    },
    {
      "name": "Fei Yang",
      "affiliation": "Pivotal Software Inc"
    },
    {
      "name": "Xin Zhang",
      "affiliation": "Pivotal Software Inc"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Demo_697"
},
"SIGMOD_Demo_700": {
  "title": "FindYourFavorite: An Interactive System for Finding the User's Favorite Tuple in the Database",
  "abstract": "When faced with a database containing millions of tuples, an end user might be only interested in finding his/her favorite tuple in the database. In this paper, we study how to help an end user to find such a favorite tuple with a few user interactions. In each interaction, a user is presented with a small number of tuples (which can be artificial tuples outside the database or true tuples inside the database) and s/he is asked to indicate the tuple s/he favors the most among them. Different from the previous work which displays artificial tuples to users during the interaction and requires heavy user interactions, we achieve a stronger result. Specifically, we use a concept, called the utility hyperplane, to model the user preference and an effective pruning strategy to locate the favorite tuple for a user in the whole database. Based on these techniques, we developed an interactive system, called FindYourFavorite, and demonstrate that the system could identify the favorite tuple for a user with a few user interactions by always displaying true tuples in the database.",
  "subtype": "SIGMOD Demo",
  "authors": [
    {
      "name": "Min Xie",
      "affiliation": "Hong Kong University of Science and Technology"
    },
    {
      "name": "Tianwen Chen",
      "affiliation": "Hong Kong University of Science and Technology"
    },
    {
      "name": "Raymond Chi-Wing Wong",
      "affiliation": "Hong Kong University of Science and Technology"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Demo_700"
},
"SIGMOD_Demo_703": {
  "title": "Large Scale Graph Mining with G-Miner",
  "abstract": "This Demo presents G-Miner, a distributed system for graph mining. The take-aways for Demo attendees are: (1) a good understanding of the challenges of various graph mining workloads; (2) useful insights on how to design a good system for graph mining by comparing G-Miner with existing systems on performance, expressiveness and user-friendliness; and (3) how to use G-Miner for interactive graph analytics.",
  "subtype": "SIGMOD Demo",
  "authors": [
    {
      "name": "Hongzhi Chen",
      "affiliation": "The Chinese University of Hong Kong"
    },
    {
      "name": "Xiaoxi Wang",
      "affiliation": "The Chinese University of Hong Kong"
    },
    {
      "name": "Chenghuan Huang",
      "affiliation": "The Chinese University of Hong Kong"
    },
    {
      "name": "Juncheng Fang",
      "affiliation": "The Chinese University of Hong Kong"
    },
    {
      "name": "Yifan Hou",
      "affiliation": "The Chinese University of Hong Kong"
    },
    {
      "name": "Changji Li",
      "affiliation": "The Chinese University of Hong Kong"
    },
    {
      "name": "James Cheng",
      "affiliation": "The Chinese University of Hong Kong"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Demo_703"
},
"SIGMOD_Demo_708": {
  "title": "Visual Exploration of Time Series Anomalies with Metro-Viz",
  "abstract": "This demo presents a novel data visualization solution for exploring the results of time series anomaly detection systems. When anomalies are reported, there is a need to reason about the results. We introduce Metro-Viz -- a visual tool to assist data scientists in performing this analysis. Metro-Viz offers a rich set of interaction features (e.g., comparative analysis, what-if testing) backed by data management strategies specifically tailored to the workload. We show our tool in action via multiple time series datasets and anomaly detectors.",
  "subtype": "SIGMOD Demo",
  "authors": [
    {
      "name": "Philipp Eichmann",
      "affiliation": "Brown University"
    },
    {
      "name": "Franco Solleza",
      "affiliation": "Brown University"
    },
    {
      "name": "Nesime Tatbul",
      "affiliation": "Intel Labs and MIT"
    },
    {
      "name": "Stan Zdonik",
      "affiliation": "Brown University"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Demo_708"
},
"SIGMOD_Demo_709": {
  "title": "ANMAT: Automatic Knowledge Discovery and Error Detection through Pattern Functional Dependencies",
  "abstract": "Knowledge discovery is critical to successful data analytics. We propose a new type of meta-knowledge, namely pattern functional dependencies (PFDs), that combine patterns (or regex-like rules) and integrity constraints (ICs) to model the dependencies (or meta-knowledge) between partial values (or patterns) across different attributes in a table. PFDs go beyond the classical functional dependencies and their extensions. For instance, in an employee table, ID 'F-9-107', 'F' determines the finance department. Moreover, a key application of PFDs is to use them to identify erroneous data; tuples that violate some PFDs. In this demonstration, attendees will experience the following features: (i) PFD discovery -- automatically discover PFDs from (dirty) data in different domains; and (ii) Error detection with PFDs -- we will show errors that are detected by PFDs but cannot be captured by existing approaches.",
  "subtype": "SIGMOD Demo",
  "authors": [
    {
      "name": "Abdulhakim Qahtan",
      "affiliation": "QCRI, HBKU"
    },
    {
      "name": "Nan Tang",
      "affiliation": "QCRI, HBKU"
    },
    {
      "name": "Mourad Ouzzani",
      "affiliation": "QCRI, HBKU"
    },
    {
      "name": "Yang Cao",
      "affiliation": "University of Edinburgh"
    },
    {
      "name": "Michael Stonebraker",
      "affiliation": "MIT"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Demo_709"
},
"SIGMOD_Demo_711": {
  "title": "Estimating Cardinalities with Deep Sketches",
  "abstract": "We introduce Deep Sketches, which are compact models of databases that allow us to estimate the result sizes of SQL queries. Deep Sketches are powered by a new deep learning approach to cardinality estimation that can capture correlations between columns, even across tables. Our demonstration allows users to define such sketches on the TPC-H and IMDb datasets, monitor the training process, and run ad-hoc queries against trained sketches. We also estimate query cardinalities with HyPer and PostgreSQL to visualize the gains over traditional cardinality estimators.",
  "subtype": "SIGMOD Demo",
  "authors": [
    {
      "name": "Andreas Kipf",
      "affiliation": "Technische Universität München"
    },
    {
      "name": "Dimitri Vorona",
      "affiliation": "Technische Universität München"
    },
    {
      "name": "Jonas Müller",
      "affiliation": "Technische Universität München"
    },
    {
      "name": "Thomas Kipf",
      "affiliation": "University of Amsterdam"
    },
    {
      "name": "Bernhard Radke",
      "affiliation": "Technische Universität München"
    },
    {
      "name": "Viktor Leis",
      "affiliation": "Technische Universität München"
    },
    {
      "name": "Peter Boncz",
      "affiliation": "CWI"
    },
    {
      "name": "Thomas Neumann",
      "affiliation": "Technische Universität München"
    },
    {
      "name": "Alfons Kemper",
      "affiliation": "Technische Universität München"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Demo_711"
},
"SIGMOD_Demo_712": {
  "title": "Unit Testing Data with Deequ",
  "abstract": "Modern companies and institutions rely on data to guide every single decision. Missing or incorrect information seriously compromises any decision process. We demonstrate Deequ, an Apache Spark-based library for automating the verification of data quality at scale. This library provides a declarative API, which combines common quality constraints with user-defined validation code, and thereby enables unit tests for data. Deequ is available as open source, meets the requirements of production use cases at Amazon, and scales to datasets with billions of records if the constraints to evaluate are chosen carefully. Our demonstration walks attendees through a fictitious business use case of validating daily product reviews from a public dataset, and is executed in a proprietary interactive notebook environment. We show attendees how to define data unit tests from automatically suggested constraints and how to create customized tests. Additionally, we demonstrate how to apply Deequ to validate incrementally growing datasets, and give examples of how to configure anomaly detection algorithms on time series of data quality metrics to further automate the data validation.",
  "subtype": "SIGMOD Demo",
  "authors": [
    {
      "name": "Sebastian Schelter",
      "affiliation": "Amazon Research"
    },
    {
      "name": "Felix Biessmann",
      "affiliation": "Amazon Research"
    },
    {
      "name": "Dustin Lange",
      "affiliation": "Amazon Research"
    },
    {
      "name": "Tammo Rukat",
      "affiliation": "Amazon Research"
    },
    {
      "name": "Phillipp Schmidt",
      "affiliation": "Amazon Research"
    },
    {
      "name": "Stephan Seufert",
      "affiliation": "Amazon Research"
    },
    {
      "name": "Pierre Brunelle",
      "affiliation": "Amazon Research"
    },
    {
      "name": "Andrey Taptunov",
      "affiliation": "Amazon Research"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Demo_712"
},
"SIGMOD_Demo_715": {
  "title": "Data Debugging and Exploration with Vizier",
  "abstract": "We present Vizier, a multi-modal data exploration and debugging tool. The system supports a wide range of operations by seamlessly integrating Python, SQL, and automated data curation and debugging methods. Using Spark as an execution backend, Vizier handles large datasets in multiple formats. Ease-of-use is attained through integration of a notebook with a spreadsheet-style interface and with visualizations that guide and support the user in the loop. In addition, native support for provenance and versioning enable collaboration and uncertainty management. In this demonstration we will illustrate the diverse features of the system using several realistic data science tasks based on real data.",
  "subtype": "SIGMOD Demo",
  "authors": [
    {
      "name": "Mike Brachmann",
      "affiliation": "University at Buffalo"
    },
    {
      "name": "Carlos Bautista",
      "affiliation": "New York University"
    },
    {
      "name": "Sonia Castelo",
      "affiliation": "New York University"
    },
    {
      "name": "Su Feng",
      "affiliation": "Illinois Institute of Technology"
    },
    {
      "name": "Juliana Freire",
      "affiliation": "New York University"
    },
    {
      "name": "Boris Glavic",
      "affiliation": "Illinois Institute of Technology"
    },
    {
      "name": "Oliver Kennedy",
      "affiliation": "University of Buffalo"
    },
    {
      "name": "Heiko M&#252;eller",
      "affiliation": "New York University"
    },
    {
      "name": "R&#233;mi Rampin",
      "affiliation": "New York University"
    },
    {
      "name": "William Spoth",
      "affiliation": "University at Buffalo"
    },
    {
      "name": "Ying Yang",
      "affiliation": "Oracle"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Demo_715"
},
"SIGMOD_Demo_718": {
  "title": "DuckDB: an Embeddable Analytical Database",
  "abstract": "The immense popularity of SQLite shows that there is a need for unobtrusive in-process data management solutions. However, there is no such system yet geared towards analytical workloads. We demonstrate DuckDB, a novel data management system designed to execute analytical SQL queries while embedded in another process. In our demonstration, we pit DuckDB against other data management solutions to showcase its performance in the embedded analytics scenario. DuckDB is available as Open Source software under a permissive license.",
  "subtype": "SIGMOD Demo",
  "authors": [
    {
      "name": "Mark Raasveldt",
      "affiliation": "CWI"
    },
    {
      "name": "Hannes Mühleisen",
      "affiliation": "CWI"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Demo_718"
},
"SIGMOD_Demo_719": {
  "title": "CLASH: A High-Level Abstraction for Optimized, Multi-Way Stream Joins over Apache Storm",
  "abstract": "We propose the demonstration of CLASH, a high-level abstraction on top of Apache Storm. CLASH is designed around MultiStream, a novel join operator designed for native support of distributed, multi-way stream joins. MultiStream allows trading off materialization of intermediate results versus communication load. With this demonstration, we invite the audience to explore the full potential of CLASH: multi-way stream joins, creation of complex join plans and their automated optimization, and ultimately the hassle-free SQL-style user/application interface and the translation of the optimized query plans to deployable Storm topologies that are executed on our local compute cluster.",
  "subtype": "SIGMOD Demo",
  "authors": [
    {
      "name": "Manuel Dossinger",
      "affiliation": "TU Kaiserslautern"
    },
    {
      "name": "Sebastian Michel",
      "affiliation": "TU Kaiserslautern"
    },
    {
      "name": "Constantin Roudsarabi",
      "affiliation": "TU Kaiserslautern"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Demo_719"
},
"SIGMOD_Demo_721": {
  "title": "PgCuckoo: Laying Plan Eggs in PostgreSQL's Nest",
  "abstract": "We demonstrate how to use PostgreSQL's planner hook to open a side entrance through which we can pass plan trees for immediate execution. Since this reaches deep into PostgreSQL, we implement plan detail inference and decoration to ensure that externally crafted trees perfectly mimic regular plans. Plan trees may then (1) be generated by external code generators that want to use PostgreSQL as a reliable and efficient back-end for new (maybe even non-relational) languages, or (2) stem from experimental rewrites of SQL plans that PostgreSQL itself does not implement (yet). The demonstration provides a live account of what becomes possible once we let PostgreSQL hatch foreign plan eggs.",
  "subtype": "SIGMOD Demo",
  "authors": [
    {
      "name": "Denis Hirn",
      "affiliation": "Universität Tübingen"
    },
    {
      "name": "Torsten Grust",
      "affiliation": "Universität Tübingen"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Demo_721"
},
"SIGMOD_Demo_723": {
  "title": "CrowdGame: A Game-Based Crowdsourcing System for Cost-Effective Data Labeling",
  "abstract": "Large-scale data labeling has become a major bottleneck for many applications, such as machine learning and data integration. This paper presents CrowdGame, a crowdsourcing system that harnesses the crowd to gather data labels in a cost-effective way. CrowdGame focuses on generating high-quality labeling rules to largely reduce the labeling cost while preserving quality. It first generates candidate rules, and then devises a game-based crowdsourcing approach to select rules with high coverage and accuracy. CrowdGame applies the generated rules for effective data labeling. We have implemented CrowdGame and provided a user-friendly interface for users to deploy their labeling applications. We will demonstrate CrowdGame in two representative data labeling scenarios, entity matching and relation extraction.",
  "subtype": "SIGMOD Demo",
  "authors": [
    {
      "name": "Tongyu Liu",
      "affiliation": "Renmin University of China"
    },
    {
      "name": "Jingru Yang",
      "affiliation": "Renmin University of China"
    },
    {
      "name": "Ju Fan",
      "affiliation": "Renmin University of China"
    },
    {
      "name": "Zhewei Wei",
      "affiliation": "Renmin University of China"
    },
    {
      "name": "Guoliang Li",
      "affiliation": "Tsinghua University"
    },
    {
      "name": "Xiaoyong Du",
      "affiliation": "Renmin University of China"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Demo_723"
},
"SIGMOD_Demo_724": {
  "title": "Ursprung: Provenance for Large-Scale Analytics Environments",
  "abstract": "Modern analytics has produced wonders, but reproducing and verifying these wonders is difficult. Data provenance helps to solve this problem by collecting information on how data is created and accessed. Although provenance collection techniques have been used successfully on a smaller scale, tracking provenance in large-scale analytics environments is challenging due to the scale of provenance generated and the heterogeneous domains. Without provenance, analysts struggle to keep track of and reproduce their analyses. We demonstrate Ursprung, a provenance collection system specifically targeted at such environments. Ursprung transparently collects the minimal set of system-level provenance required to track the relationships between data and processes. To collect domain specific provenance, Usprung enables users to specify capture rules to curate application-specific logs, intermediate results etc. To reduce storage overhead and accelerate queries, it uses event hierarchies to synthesize raw provenance into compact summaries.",
  "subtype": "SIGMOD Demo",
  "authors": [
    {
      "name": "Lukas Rupprecht",
      "affiliation": "IBM Almaden Research Center"
    },
    {
      "name": "James Davis",
      "affiliation": "Virginia Tech & IBM Systems"
    },
    {
      "name": "Constantine Arnold",
      "affiliation": "IBM Almaden Research Center"
    },
    {
      "name": "Alexander Lubbock",
      "affiliation": "Vanderbilt University"
    },
    {
      "name": "Darren Tyson",
      "affiliation": "Vanderbilt University"
    },
    {
      "name": "Deepavali Bhagwat",
      "affiliation": "IBM Almaden Research Center"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Demo_724"
},
"SIGMOD_Demo_725": {
  "title": "Demonstration of ModelarDB: Model-Based Management of Dimensional Time Series",
  "abstract": "Due to the big amounts of sensor data produced, it is infeasible to store all of the data points collected and practitioners currently hide outliers by storing simple aggregates instead. As a remedy, we demonstrate sys, a model-based actsms for time series with dimensions and possibly gaps. In this demonstration, participants can ingest data sets from multiple domains and experience how sys provides fast ingestion and a high compression ratio by adaptively compressing time series using a set of models to accommodate changes in the structure of each time series over time. Models approximate time series within a user-defined error bound (possibly zero). Participants can also experience how the compression ratio can be improved by ingesting correlated time series in groups created by sys from user-hints. Participants provide these using primitives for describing correlation. Last, participants can execute SQL queries on the ingested data sets and see how the system optimizes queries directly on models.",
  "subtype": "SIGMOD Demo",
  "authors": [
    {
      "name": "Søren Kejser Jensen",
      "affiliation": "Aalborg University"
    },
    {
      "name": "Torben Bach Pedersen",
      "affiliation": "Aalborg University"
    },
    {
      "name": "Christian Thomsen",
      "affiliation": "Aalborg University"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Demo_725"
},
"SIGMOD_Demo_729": {
  "title": "NEURON: Query Execution Plan Meets Natural Language Processing For Augmenting DB Education",
  "abstract": "A core component of a database systems course at the undergraduate level is the design and implementation of the query optimizer in an textscrdbms. The query optimization process produces a query execution plan (textscqep), which represents an execution strategy for an textscsql query. Unfortunately, in practice, it is often difficult for a student to comprehend a query execution strategy by perusing its textscqep, hindering her learning process. In this demonstration, we present a novel system called textscneuron that facilitates natural language interaction with textscqeps to enhance its understanding. textscneuron accepts an textscsql query (which may include joins, aggregation, nesting, among other things) as input, executes it, and generates a simplified natural language description (both in text and voice form) of the execution strategy deployed by the underlying textscrdbms. Furthermore, it facilitates understanding of various features related to a textscqep through a natural language question answering (textscnlqa) framework. We advocate that such tool, world's first of its kind, can greatly enhance students' learning of the query optimization topic.",
  "subtype": "SIGMOD Demo",
  "authors": [
    {
      "name": "Siyuan Liu",
      "affiliation": "Nanyang Technological University"
    },
    {
      "name": "Sourav Bhowmick",
      "affiliation": "Nanyang Technological University"
    },
    {
      "name": "Wanlu Zhang",
      "affiliation": "Nanyang Technological University"
    },
    {
      "name": "Shu Wang",
      "affiliation": "Nanyang Technological University"
    },
    {
      "name": "Wanyi Huang",
      "affiliation": "Nanyang Technological University"
    },
    {
      "name": "Shafiq Joty",
      "affiliation": "Nanyang Technological University"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Demo_729"
},
"SIGMOD_Demo_730": {
  "title": "PIClean: A Probabilistic and Interactive Data Cleaning System",
  "abstract": "With the dramatic increasing interest in data analysis, ensuring data quality becomes one of the most important topics in data science. Data Cleaning, the process of ensuring data quality, is composed of two stages: error detection and error repair. Despite decades of research in data cleaning, existing cleaning systems still have limitations in terms of usability and error coverage. We propose PIClean, a probabilistic and interactive data cleaning system that aims at addressing the aforementioned limitations. PIClean produces probabilistic errors and probabilistic fixes using low-rank approximation, which implicitly discovers and uses relationships between columns of a dataset for cleaning. The probabilistic errors and fixes are confirmed or rejected by users, and the user feedbacks are constantly incorporated by PIClean to produce more accurate and higher-coverage cleaning results.",
  "subtype": "SIGMOD Demo",
  "authors": [
    {
      "name": "Zhuoran Yu",
      "affiliation": "Georgia Institute of Technology"
    },
    {
      "name": "Xu Chu",
      "affiliation": "Georgia Institute of Technology"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Demo_730"
},
"SIGMOD_Demo_733": {
  "title": "Demonstration of SpeakQL: Speech-driven Multimodal Querying of Structured Data",
  "abstract": "In this demonstration, we present SpeakQL, a speech-driven query system and interface for structured data. SpeakQL supports a tractable and practically useful subset of regular SQL, allowing users to query in any domain with unbounded vocabulary with the help of speech/touch based user-in-the-loop mechanisms for correction. When querying in such domains, automatic speech recognition introduces countless forms of errors in transcriptions, presenting us with a technical challenge. We characterize such errors and leverage our observations along with SQL's unambiguous context-free grammar to first correct the query structure. We then exploit phonetic representation of the queried database to identify the correct Literals, hence delivering the corrected transcribed query. In this demo, we show that SpeakQL helps users reduce time and effort in specifying SQL queries significantly. In addition, we show that SpeakQL, unlike Natural Language Interfaces and conversational assistants, allows users to query over any arbitrary database schema. We allow the audience to explore SpeakQL using an easy-to-use web-based interface to compose SQL queries.",
  "subtype": "SIGMOD Demo",
  "authors": [
    {
      "name": "Vraj Shah",
      "affiliation": "University of California, San Diego"
    },
    {
      "name": "Side Li",
      "affiliation": "University of California, San Diego"
    },
    {
      "name": "Kevin Yang",
      "affiliation": "University of California, San Diego"
    },
    {
      "name": "Arun Kumar",
      "affiliation": "University of California, San Diego"
    },
    {
      "name": "Lawrence Saul",
      "affiliation": "University of California, San Diego"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Demo_733"
},
"SIGMOD_Demo_734": {
  "title": "Ratel: Interactive Analytics for Large Scale Trajectories",
  "abstract": "Trajectory data analytics plays an important role in many applications, such as transportation optimization, urban planning, taxi scheduling, and so on. However, trajectory data analytics has a great challenge that the time cost for processing queries is too high on big datasets. In this paper, we demonstrate a distributed in-memory framework Ratel base on Spark for analyzing large scale trajectories. Ratel groups trajectories into partitions by considering the data locality and load balance. We build R-Tree based global indexes to prune partitions when applying trajectory search and join. For each partition, Ratel uses a filter-refinement method to efficiently find similar trajectories. We show three kinds of scenarios - bus station planning, route recommendation, and transportation analytics. Demo attendees can interact with a web UI, pose different queries on the dataset, and navigate the query result.",
  "subtype": "SIGMOD Demo",
  "authors": [
    {
      "name": "Haoda Li",
      "affiliation": "Tsinghua University"
    },
    {
      "name": "Guoliang Li",
      "affiliation": "Tsinghua University"
    },
    {
      "name": "Jiayang Liu",
      "affiliation": "Tsinghua University"
    },
    {
      "name": "Haitao Yuan",
      "affiliation": "Tsinghua University"
    },
    {
      "name": "Haiquan Wang",
      "affiliation": "Tsinghua University"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Demo_734"
},
"SIGMOD_Demo_746": {
  "title": "MigCast: Putting a Price Tag on Data Model Evolution in NoSQL Data Stores",
  "abstract": "We demonstrate MigCast, a tool-based advisor for exploring data migration strategies in the context of developing NoSQL-backed applications. Users of MigCast can consider their options for evolving their data model along with legacy data already persisted in the cloud-hosted production database. They can explore alternative actions as the financial costs are predicted respective to the cloud provider chosen. Thereby they are better equipped to assess potential consequences of imminent data migration decisions. To this end, MigCast maintains an internal cost model, taking into account characteristics of the data instance, expected workload, data model changes, and cloud provider pricing models. Hence, MigCast enables software project stakeholders to remain in control of the operative costs and to make informed decisions evolving their applications.",
  "subtype": "SIGMOD Demo",
  "authors": [
    {
      "name": "Andrea Hillenbrand",
      "affiliation": "Darmstadt University of Applied Sciences"
    },
    {
      "name": "Maksym Levchenko",
      "affiliation": "Darmstadt University of Applied Sciences"
    },
    {
      "name": "Uta Störl",
      "affiliation": "Darmstadt University of Applied Sciences"
    },
    {
      "name": "Stefanie Scherzinger",
      "affiliation": "OTH Regensburg"
    },
    {
      "name": "Meike Klettke",
      "affiliation": "University of Rostock"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Demo_746"
},
"SIGMOD_Demo_747": {
  "title": "Apollo: A Dataset Profiling and Operator Modeling System",
  "abstract": "The rapidly increasing amount of available data has created invaluable business opportunities but also new challenges. The focus on content-driven analytics is shifting attention from optimizing operators and systems to handle massive data sizes, to intelligent selection of those datasets that   maximize the business competitive advantage. To date, there exists no   efficient method to quantify the impact of numerous available datasets over different analytics tasks - a thorough execution over every input would be prohibitively expensive. In this demonstration, we present Apollo,   a data profiling and operator modeling system that tackles this   challenge. Our system quantifies dataset similarities and projects them into a low-dimensional space. Operator outputs are then estimated over the   entire dataset, utilizing similarity information with Machine Learning and a small sample of actual executions. During the demo, attendees will be   able to model and visualize multiple analytics operators over datasets from the domains of machine learning and graph analytics.",
  "subtype": "SIGMOD Demo",
  "authors": [
    {
      "name": "Tasos Bakogiannis",
      "affiliation": "National Technical University of Athens"
    },
    {
      "name": "Ioannis Giannakopoulos",
      "affiliation": "National Technical University of Athens"
    },
    {
      "name": "Dimitrios Tsoumakos",
      "affiliation": "Ionian University"
    },
    {
      "name": "Nectarios Koziris",
      "affiliation": "National Technical University of Athens"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Demo_747"
},
"SIGMOD_Demo_748": {
  "title": "NeMeSys - A Showcase of Data Oriented Near Memory Graph Processing",
  "abstract": "NeMeSys is a NUMA-aware graph pattern processing engine, which uses the Near Memory Processing paradigm to allow for high scalability. With modern server systems incorporating an increasing amount of main memory, we can store graphs and compute analytical graph algorithms like graph pattern matching completely in-memory. Our system blends state-of-the-art approaches from the transactional database world together with graph processing principles. We demonstrate, that graph pattern processing - standalone and workloads - can be controlled by leveraging different partitioning strategies, applying Bloom filter based messaging optimization and, given performance constraints, can save energy by applying frequency scaling of CPU cores.",
  "subtype": "SIGMOD Demo",
  "authors": [
    {
      "name": "Alexander Krause",
      "affiliation": "Technische Universität Dresden"
    },
    {
      "name": "Thomas Kissinger",
      "affiliation": "Technische Universität Dresden"
    },
    {
      "name": "Dirk Habich",
      "affiliation": "Technische Universität Dresden"
    },
    {
      "name": "Wolfgang Lehner",
      "affiliation": "Technische Universität Dresden"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Demo_748"
},
"SIGMOD_Demo_750": {
  "title": "Low-latency Spark Queries on Updatable Data",
  "abstract": "As data science gets deployed more and more into operational applications, it becomes important for data science frameworks to be able to perform computations in interactive, sub-second time. Indexing and caching are two key techniques that can make interactive query processing on large datasets possible. In this demo, we show the design, implementation and performance of a new indexing abstraction in Apache Spark, called the Indexed DataFrame. This is a cached DataFrame that incorporates an index to support fast lookup and join operations, and supports updates with multi-version concurrency. We demonstrate the Indexed Dataframe on a social network dataset using microbenchmarks and real-world graph processing queries, in datasets that are continuously growing.",
  "subtype": "SIGMOD Demo",
  "authors": [
    {
      "name": "Alexandru Uta",
      "affiliation": "Vrije Universiteit Amsterdam"
    },
    {
      "name": "Bogdan Ghit",
      "affiliation": "Databricks"
    },
    {
      "name": "Ankur Dave",
      "affiliation": "University of California, Berkeley"
    },
    {
      "name": "Peter Boncz",
      "affiliation": "CWI"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Demo_750"
},
"SIGMOD_Demo_752": {
  "title": "BlockchainDB - Towards a Shared Database on Blockchains",
  "abstract": "In this demo we present BlockchainDB, which leverages blockchains as storage layer and introduces a database layer on top that extends blockchains by classical data management techniques (e.g., sharding). Further, BlockchainDB provides a standardized key/value-based query interface to facilitate the adoption of blockchains for data sharing use cases. With BlockchainDB we can thus not only improve the performance and scalability of blockchains for data sharing but also decrease the complexity for organizations intending to use blockchains for this use case.",
  "subtype": "SIGMOD Demo",
  "authors": [
    {
      "name": "Muhammad El-Hindi",
      "affiliation": "TU Darmstadt"
    },
    {
      "name": "Martin Heyden",
      "affiliation": "TU Darmstadt"
    },
    {
      "name": "Carsten Binnig",
      "affiliation": "TU Darmstadt"
    },
    {
      "name": "Ravi Ramamurthy",
      "affiliation": "Microsoft Research"
    },
    {
      "name": "Arvind Arasu",
      "affiliation": "Microsoft Research"
    },
    {
      "name": "Donald Kossmann",
      "affiliation": "Microsoft Research"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Demo_752"
},
"SIGMOD_Demo_753": {
  "title": "Fluid: A Blockchain based Framework for Crowdsourcing",
  "abstract": "Recently, crowdsourcing has emerged as a new computing paradigm to solve problems that need human intrinsic, such as image annotation. However, there are two limitations in existing crowdsourcing platforms, i.e. non-transparent incentive mechanism and isolated profiles of workers, which harms the interests of both requesters and workers. Meanwhile, Blockchain technology introduces a solution to build a transparent, immutable data model in the Byzantine environment. Moreover, Blockchain systems (e.g. Ethereum) can also support the Tuning-complete script called smart contracts. Thus, we are motivated to use the feature of the transparent data model and smart contract in Blockchain to address the two limitations. Based on the proposed solutions, we have designed a Blockchain based framework which supports foundations of general crowdsourcing platforms. In addition, our framework also has following novel features: (1) it provides the transparent incentive mechanisms; (2) it supports a trusted worker's profile sharing in a cross-platform mode.",
  "subtype": "SIGMOD Demo",
  "authors": [
    {
      "name": "Siyuan Han",
      "affiliation": "Hong Kong University of Science and Technology"
    },
    {
      "name": "Zihuan Xu",
      "affiliation": "Hong Kong University of Science and Technology"
    },
    {
      "name": "Yuxiang Zeng",
      "affiliation": "Hong Kong University of Science and Technology"
    },
    {
      "name": "Lei Chen",
      "affiliation": "Hong Kong University of Science and Technology"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Demo_753"
},
"SIGMOD_Demo_761": {
  "title": "Demonstration of Nimbus: Model-based Pricing for Machine Learning in a Data Marketplace",
  "abstract": "Various domains such as business intelligence and journalism have made many achievements with help of data analytics based on machine learning (ML). While a lot of work has studied how to reduce the cost of training, storing, and deploying ML models, there is little work on eliminating the data collection and purchase cost. Existing data markets provide only simplistic mechanism allowing the sale of fixed datasets with fixed price, which potentially hurts not only ML model availability to buyers with limited budget, but market expansion and thus sellers' revenue as well. In this work, we demonstrate Nimbus, a data market framework for ML model exchange. Instead of pricing data, Nimbus prices ML models directly, which we call model-based pricing (MBP). Through interactive interfaces, the audience can play the role of sellers to vend their own ML models with different price requirements, as well as the role of buyers to purchase ML model instances with different accuracy/budget constraints. We will further demonstrate how much gain of sellers' revenue and buyers' affordability Nimbus can achieve with low runtime cost via both real time and offline results.",
  "subtype": "SIGMOD Demo",
  "authors": [
    {
      "name": "Lingjiao Chen",
      "affiliation": "University of Wisconsin, Madison"
    },
    {
      "name": "Hongyi Wang",
      "affiliation": "University of Wisconsin, Madison"
    },
    {
      "name": "Leshang Chen",
      "affiliation": "University of Pennsylvania"
    },
    {
      "name": "Paraschos Koutris",
      "affiliation": "University of Wisconsin, Madison"
    },
    {
      "name": "Arun Kumar",
      "affiliation": "University of California, San Diego"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Demo_761"
},
"SIGMOD_Demo_764": {
  "title": "Capturing and Querying Structural Provenance in Spark with Pebble",
  "abstract": "Analyzing and debugging Spark processing pipelines is a tedious task which typically involves a lot of engineering effort. The task becomes even more complex when the pipelines process nested data. Provenance solutions that track the derivation process of individual data items assist data engineers while debugging these pipelines. However, state-of-the-art solutions do not precisely track nested data items.</par><par>We demonstrate Pebble, a system for capturing and querying a new type of provenance on nested data in Spark called structural provenance. It captures access and modification of top-level as well as nested data items, and allows querying the provenance of nested items based on tree-pattern-matching. Implemented as a standalone library on top of Apache Spark, it seamlessly leverages the underlying infrastructure for scalability. Through the graphical user interface implemented in a Jupyter notebook we showcase ten debugging scenarios of Spark programs on real-world datasets.",
  "subtype": "SIGMOD Demo",
  "authors": [
    {
      "name": "Ralf Diestelkämper",
      "affiliation": "Universität Stuttgart"
    },
    {
      "name": "Melanie Herschel",
      "affiliation": "Universität Stuttgart"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Demo_764"
},
"SIGMOD_Demo_767": {
  "title": "MorphStore - In-Memory Query Processing based on Morphing Compressed Intermediates LIVE",
  "abstract": "In this demo, we present MorphStore, an in-memory column store with a novel compression-aware query processing concept. Basically, compression using lightweight integer compression algorithms already plays an important role in existing in-memory column stores, but mainly for base data. The continuous handling of compression from the base data to the intermediate results during query processing has already been discussed, but not investigated in detail since the computational effort for compression as well as decompression is often assumed to exceed the benefits of a reduced transfer cost between CPU and main memory. However, this argument increasingly loses its validity as we are going to show in our demo. Generally, our novel compression-aware query processing concept is characterized by the fact that we are able to speed up the query execution by morphing compressed intermediate results from one scheme to another scheme to dynamically adapt to the changing data characteristics during query processing. Our morphing decisions are made using a cost-based approach.",
  "subtype": "SIGMOD Demo",
  "authors": [
    {
      "name": "Dirk Habich",
      "affiliation": "Technische Universität Dresden"
    },
    {
      "name": "Patrick Damme",
      "affiliation": "Technische Universität Dresden"
    },
    {
      "name": "Annett Ungethüm",
      "affiliation": "Technische Universität Dresden"
    },
    {
      "name": "Johannes Pietrzyk",
      "affiliation": "Technische Universität Dresden"
    },
    {
      "name": "Alexander Krause",
      "affiliation": "Technische Universität Dresden"
    },
    {
      "name": "Juliana Hildebrandt",
      "affiliation": "Technische Universität Dresden"
    },
    {
      "name": "Wolfgang Lehner",
      "affiliation": "Technische Universität Dresden"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Demo_767"
},
"SIGMOD_Demo_771": {
  "title": "MapRepair: Mapping and Repairing under Policy Views",
  "abstract": "Mapping design is overwhelming for end users, who have to check at par the correctness of the mappings and the possible information disclosure over the exported source instance. In this demonstration, we focus on the latter problem by proposing a novel practical solution to ensure that a mapping faithfully complies with a set of privacy restrictions specified as source policy views. We showcase MapRepair, that guides the user through the tasks of visualizing the results of the data exchange process with and without the privacy restrictions. MapRepair leverages formal privacy guarantees and is inherently data-independent, i.e. if a set of criteria are satisfied by the mapping statement, then it guarantees that both the mapping and the underlying instances do not leak sensitive information. Furthermore, MapRepair also allows to automatically repair an input mapping w.r.t. a set of policy views in case of information leakage. We build on various demonstration scenarios, including synthetic and real-world instances and mappings.",
  "subtype": "SIGMOD Demo",
  "authors": [
    {
      "name": "Angela Bonifati",
      "affiliation": "Lyon 1 University &amp; Liris CNRS"
    },
    {
      "name": "Ugo Comignani",
      "affiliation": "Lyon 1 University &amp; Liris CNRS"
    },
    {
      "name": "Efthymia Tsamoura",
      "affiliation": "University of Oxford"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Demo_771"
},
"SIGMOD_Demo_777": {
  "title": "SVQ: Streaming Video Queries",
  "abstract": "Recent advances in video processing utilizing deep learning primitives achieved breakthroughs in fundamental problems in video analysis such as frame classification and object detection enabling an array of new applications.</par><par>In this demo we present SVQ a system capable of executing declarative queries on streaming video. The system utilizes a set of approximate filters to speed up queries that involve objects of specific type (e.g., cars, trucks, etc.) on video frames with associated spatial relationships among them (e.g., car left of truck). The resulting filters are able to assess quickly if the query predicates are true to proceed with further analysis of the frame or otherwise not consider the frame further avoiding costly object detection and localization operations. The filters utilize extensible deep neural architectures and are easy to deploy and utilize.</par><par>We demonstrate that the application of our filtering techniques in the context of SVQ enable declarative queries on video streams increasing dramatically the frame processing rate and speed up query processing by at least two orders of magnitude depending on the query.",
  "subtype": "SIGMOD Demo",
  "authors": [
    {
      "name": "Ioannis Xarchakos",
      "affiliation": "University of Toronto"
    },
    {
      "name": "Nick Koudas",
      "affiliation": "University of Toronto"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Demo_777"
},
"SIGMOD_Demo_778": {
  "title": "RATest: Explaining Wrong Relational Queries Using Small Examples",
  "abstract": "We present a system called textscRATest, designed to help debug relational queries against reference queries and test database instances. In many applications, e.g., classroom learning and regression testing, we test the correctness of a user query Q by evaluating it over a test database instance D and comparing its result with that of evaluating a reference (correct) query Q_0 over D. If Q(D) differs from Q_0(D), the user knows Q is incorrect. However, D can be large (often by design), which makes debugging Q difficult. The key idea behind textscRATest is to show the user a much smaller database instance D' subseteq D, which we call a counterexample, such that Q(D') neq Q_0(D'). textscRATest builds on data provenance and constraint solving, and employs a suite of techniques to support, at interactive speed, complex queries involving differences and group-by aggregation. We demonstrate an application of textscRATest in learning: it has been used successfully by a large undergraduate database course in a university to help students with a relational algebra assignment.",
  "subtype": "SIGMOD Demo",
  "authors": [
    {
      "name": "Zhengjie Miao",
      "affiliation": "Duke University"
    },
    {
      "name": "Sudeepa Roy",
      "affiliation": "Duke University"
    },
    {
      "name": "Jun Yang",
      "affiliation": "Duke University"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Demo_778"
},
"SIGMOD_Demo_779": {
  "title": "NAVIGATE: Explainable Visual Graph Exploration by Examples",
  "abstract": "We demonstrate NAVIGATE, an explaiunderlinetextbfNAble query engine for underlinetextbfVIsual underlinetextbfGrunderlinetextbfAph exploraunderlinetextbfTion by underlinetextbfExamples. NAVIGATE interleaves query rewriting and query answering to help users (1) search graphs textslwithout writing complex queries, and (2) understand answers by providing intuitive explanations. Users can visually construct queries and specify missing or unwanted example entities to guide the exploration towards desired answers. NAVIGATE can rewrite queries with answers close to examples, by minimally altering their topological and semantic constraints. Another unique feature is its ability to explain query results by tracing the query manipulation operators that are responsible for transforming the original answers to desirable ones.  In addition, NAVIGATE optimizes system response time by referring to dynamically cached star views to reduce both query evaluation and rewriting cost at run time. We also demonstrate its ease-of-use, efficiency, and explainable exploration in applications such as recommendation and knowledge base search.",
  "subtype": "SIGMOD Demo",
  "authors": [
    {
      "name": "Mohammad Hossein Namaki",
      "affiliation": "Washington State University"
    },
    {
      "name": "Qi Song",
      "affiliation": "Washington State University"
    },
    {
      "name": "Yinghui Wu",
      "affiliation": "Washington State University"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Demo_779"
},
"SIGMOD_Demo_781": {
  "title": "C2Metadata: Automating the Capture of Data Transformations from Statistical Scripts in Data Documentation",
  "abstract": "Datasets are often derived by manipulating raw data with statistical software packages. The derivation of a dataset must be recorded in terms of both the raw input and the manipulations applied to it. Statistics packages typically provide limited help in documenting provenance for the resulting derived data. At best, the operations performed by the statistical package are described in a script. Disparate representations make these scripts hard to understand for users. To address these challenges, we created Continuous Capture of Metadata (C2Metadata), a system to capture data transformations in scripts for statistical packages and represent it as metadata in a standard format that is easy to understand. We do so by devising a Structured Data Transformation Algebra (SDTA), which uses a small set of algebraic operators to express a large fraction of data manipulation performed in practce. We then implement SDTA, inspired by relational algebra, in a data transformation specification language we call SDTL. In this demonstration, we showcase C2metadata's capture of data transformations from a pool of sample transformation scripts in at least two languages: SPSS and Stata (SAS and R are under development), for social science data in a large academic repository. We will allow the audience to explore C2Metadata using a web-based interface, visualize the intermediate steps and trace the provenance and changes of data at different levels for better understanding of the process.",
  "subtype": "SIGMOD Demo",
  "authors": [
    {
      "name": "Jie Song",
      "affiliation": "University of Michigan"
    },
    {
      "name": "George Alter",
      "affiliation": "University of Michigan"
    },
    {
      "name": "H. V. Jagadish",
      "affiliation": "University of Michigan"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Demo_781"
},
"SIGMOD_Demo_782": {
  "title": "GraphWrangler: An Interactive Graph View on Relational Data",
  "abstract": "Existing data stores of enterprises are full of connected data and users are increasingly finding value in performing graph querying, analytics and visualization on this data. This process involves a labor-intensive ETL pipeline, where users write scripts to extract graphs from data stored in legacy stores, often an RDBMS, and import these graphs into a graph-specific software. We demonstrate GraphWrangler, a system that allows users to connect to an RDBMS and within a few clicks extract graphs out of their tabular data, visualize and explore these graphs, and automatically generate scripts for their ETL pipelines. GraphWrangler adopts the predictive interaction framework and internally uses a data transformation language that is a limited subset of SQL. Our demonstration video can be found here: https://youtu.be/k92Qk6vuIsU",
  "subtype": "SIGMOD Demo",
  "authors": [
    {
      "name": "Nafisa Anzum",
      "affiliation": "University of Waterloo"
    },
    {
      "name": "Semih Salihoglu",
      "affiliation": "University of Waterloo"
    },
    {
      "name": "Daniel Vogel",
      "affiliation": "University of Waterloo"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Demo_782"
},
"SIGMOD_Demo_783": {
  "title": "MithraRanking: A System for Responsible Ranking Design",
  "abstract": "Items from a database are often ranked based on a combination of criteria. The weight given to each criterion in the combination can greatly affect the ranking produced. Often, a user may have a general sense of the relative importance of the different criteria, but beyond this may have the flexibility, within limits, to choose combinations that weigh these criteria differently with an acceptable region.  We demonstrate MithraRanking, a system that helps users choose criterion weights that lead to 'better' rankings in terms of having desirable properties while remaining within the acceptable region. The goodness properties we focus on are stability and fairness.",
  "subtype": "SIGMOD Demo",
  "authors": [
    {
      "name": "Yifan Guan",
      "affiliation": "University of Michigan"
    },
    {
      "name": "Abolfazl Asudeh",
      "affiliation": "University of Michigan"
    },
    {
      "name": "Pranav Mayuram",
      "affiliation": "University of Michigan"
    },
    {
      "name": "H. V. Jagadish",
      "affiliation": "University of Michigan"
    },
    {
      "name": "Julia Stoyanovich",
      "affiliation": "New York University"
    },
    {
      "name": "Gerome Miklau",
      "affiliation": "University of Massachusetts Amherst"
    },
    {
      "name": "Gautam Das",
      "affiliation": "University of Texas at Arlington"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Demo_783"
},
"SIGMOD_Demo_785": {
  "title": "Coconut Palm: Static and Streaming Data Series Exploration Now in your Palm",
  "abstract": "Many modern applications produce massive streams of data series and maintain them in indexes to be able to explore them through nearest neighbor search. Existing data series indexes, however, are expensive to operate as they issue many random I/Os to storage. To address this problem, we recently proposed Coconut, a new infrastructure that organizes data series based on a new sortable format. In this way, Coconut is able to leverage state-of-the-art indexing techniques that rely on sorting for the first time to build, maintain and query data series indexes using fast sequential I/Os. In this demonstration, we present Coconut Palm, a new exploration tool that allows to interactively combine different indexing techniques from within the Coconut infrastructure and to thereby seamlessly explore data series from across various scientific domains. We highlight the rich indexing design choices that Coconut opens up, and we present a new recommender tool that allows users to intelligently navigate them for both static and streaming data exploration scenarios.",
  "subtype": "SIGMOD Demo",
  "authors": [
    {
      "name": "Haridimos Kondylakis",
      "affiliation": "FORTH-ICS"
    },
    {
      "name": "Niv Dayan",
      "affiliation": "Harvard University"
    },
    {
      "name": "Kostas Zoumpatianos",
      "affiliation": "Harvard University"
    },
    {
      "name": "Themis Palpanas",
      "affiliation": "Paris Descartes University"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Demo_785"
},
"SIGMOD_Demo_787": {
  "title": "NEWS: News Event Walker and Summarizer",
  "abstract": "Most news summarization techniques are static, and thus do not satisfy user needs in having summaries with specific structures or details. Meanwhile, existing dynamic techniques such as query-based summarization fail to handle content-independent queries that target the type of summary information such as time, location, reasons, and consequences of reported events. The NEWS system supports multi-granular summarization along two dimensions: the level of detail and type of information. The system employs fine-grained information extraction to extract facts and their facets with type tagging. The extracted information is then modeled as a graph used to create summaries. The system incrementally expands summaries based on the nodes visited by users, folding related events into the search space.",
  "subtype": "SIGMOD Demo",
  "authors": [
    {
      "name": "Radityo Eko Prasojo",
      "affiliation": "Free University of Bozen-Bolzano"
    },
    {
      "name": "Mouna Kacimi",
      "affiliation": "Free University of Bozen-Bolzano"
    },
    {
      "name": "Werner Nutt",
      "affiliation": "Free University of Bozen-Bolzano"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Demo_787"
},
"SIGMOD_Demo_792": {
  "title": "Cost-Effective, Workload-Adaptive Migration of Big Data Applications to the Cloud",
  "abstract": "More than 10,000 enterprises worldwide use the big data stack composed of multiple distributed systems. At Unravel, we build the next-generation APM platform for the big data stack, and we have worked with a representative sample of these enterprises that covers most industry verticals. This sample covers the spectrum of choices for deploying the big data stack across on-premises datacenters, private and public cloud deployments, and hybrid combinations of these. In this paper, we present a solution for assisting enterprises planning the migration of their big data stacks from on-premises deployments to the cloud. Our solution is goal driven and adapts to various migration scenarios. We present the system architecture we built and several cloud mapping options. We also describe a demonstration script that involves practical, real-world use-cases of the path to cloud adoption.",
  "subtype": "SIGMOD Demo",
  "authors": [
    {
      "name": "Victor Giannakouris",
      "affiliation": "Unravel Data Systems"
    },
    {
      "name": "Alejandro Fernandez",
      "affiliation": "Unravel Data Systems"
    },
    {
      "name": "Alkis Simitsis",
      "affiliation": "Unravel Data Systems"
    },
    {
      "name": "Shivnath Babu",
      "affiliation": "Unravel Data Systems"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Demo_792"
},
"SIGMOD_Demo_796": {
  "title": "ChronosDB in Action: Manage, Process, and Visualize Big Geospatial Arrays in the Cloud",
  "abstract": "Immense volumes of geospatial arrays are generated daily. Examples of such include satellite imagery, numerical simulation, and derivative data avalanche. Array DBMS are one of the prominent tools for working with large geospatial arrays. Usually the arrays natively come as raster files. ChronosDB is a novel distributed, file based, geospatial array DBMS: http://chronosdb.gis.land/ ChronosDB operates directly on raster files, delegates array processing to existing elaborate command line tools, and outperforms SciDB by up to 75x on average. This demonstration will showcase three new components of ChronosDB enabling users to interact with the system and appreciate its benefits: (i) a Web GUI (edit, submit queries and get the output), (ii) an execution plan explainer (investigate the generated DAG), and (iii) a dataset visualizer (display ChronosDB arrays on an interactive web map).",
  "subtype": "SIGMOD Demo",
  "authors": [
    {
      "name": "Ramon Antonio Rodriges Zalipynis",
      "affiliation": "National Research University Higher School of Economics"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Demo_796"
},
"SIGMOD_Demo_800": {
  "title": "Natural Language Querying of Complex Business Intelligence Queries",
  "abstract": "Natural Language Interface to Database (NLIDB) eliminates the need for an end user to use complex query languages like SQL by translating the input natural language statements to SQL automatically. Although NLIDB systems have seen rapid growth of interest recently, the current state-of-the-art systems can at best handle point queries to retrieve certain column values satisfying some filters, or aggregation queries involving basic SQL aggregation functions. In this demo, we showcase our NLIDB system with extended capabilities for business applications that require complex nested SQL queries without prior training or feedback from human in-the-loop. In particular, our system uses novel algorithms that combine linguistic analysis with deep domain reasoning for solving core challenges in handling nested queries. To demonstrate the capabilities, we propose a new benchmark dataset containing realistic business intelligence queries, conforming to an ontology derived from FIBO and FRO financial ontologies. In this demo, we will showcase a wide range of complex business intelligence queries against our benchmark dataset, with increasing level of complexity. The users will be able to examine the SQL queries generated, and also will be provided with an English description of the interpretation.",
  "subtype": "SIGMOD Demo",
  "authors": [
    {
      "name": "Jaydeep Sen",
      "affiliation": "IBM Research AI"
    },
    {
      "name": "Fatma Ozcan",
      "affiliation": "IBM Research AI"
    },
    {
      "name": "Abdul Quamar",
      "affiliation": "IBM Research AI"
    },
    {
      "name": "Greg Stager",
      "affiliation": "IBM Canada"
    },
    {
      "name": "Ashish Mittal",
      "affiliation": "IBM Research AI"
    },
    {
      "name": "Manasa Jammi",
      "affiliation": "IBM Research AI"
    },
    {
      "name": "Chuan Lei",
      "affiliation": "IBM Research AI"
    },
    {
      "name": "Diptikalyan Saha",
      "affiliation": "IBM Research AI"
    },
    {
      "name": "Karthik Sankaranarayanan",
      "affiliation": "IBM Research AI"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Demo_800"
},
"SIGMOD_Keynote_015": {
  "title": "State of Public and Private Blockchains: Myths and Reality",
  "abstract": "It has been a decade since the concept of blockchain was invented as the underlying core data structure of the permissionless or public Bitcoin cryptocurrency network. Since then, several cryptocurrencies, tokens and ICOs have emerged. After much speculation and hype, significant number of them have become problematic or worthless! The public blockchain system Ethereum emerged by generalizing the use of blockchains to manage any kind of asset, be it physical or purely digital, with the introduction of Smart Contracts. Over the years, numerous myths have developed with respect to the purported utility and the need for public blockchains. The adoption and further adaptation of blockchains and smart contracts for use in the permissioned or private environments is what I consider to be useful and of practical consequence. Hence, the technical aspects of only private blockchain systems will be the focus of my SIGMOD 2019 keynote. Along the way, I will bust many myths associated with public blockchains. I will also compare traditional database technologies with blockchain systems’ features and identify desirable future research topics.",
  "subtype": "SIGMOD Keynote",
  "authors": [
    {
      "name": "C. Mohan",
      "affiliation": "IBM Almaden Research Center"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Keynote_015"
},
"SIGMOD_Keynote_025": {
  "title": "Responsible Data Science",
  "abstract": "Data science is an emerging discipline that offers both promise and peril. Responsible data science refers to efforts that address both the technical and societal issues in emerging data-driven technologies. How can machine learning and database systems reason effectively about complex dependencies and uncertainty? Furthermore, how do we understand the ethical and societal issues involved in data-driven decision-making? There is a pressing need to integrate algorithmic and statistical principles, social science theories, and basic humanist concepts so that we can think critically and constructively about the socio-technical systems we are building. In this talk, I will overview this emerging area, with an emphasis on relational learning.",
  "subtype": "SIGMOD Keynote",
  "authors": [
    {
      "name": "Lise Getoor",
      "affiliation": "University of California, Santa Cruz"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Keynote_025"
},
"SIGMOD_Research Competition_01": {
  "title": "SpeakQL: Towards Speech-driven Multimodal Querying",
  "abstract": "Speech-based inputs have become popular in many applications on constrained device environments such as smartphones and tablets, and even personal conversational assistants such as Siri, Alexa, and Cortana. Inspired by this recent success of speech-driven interfaces, in this work, we consider an important fundamental question: How should one design a speech-driven system to query structured data? Recent works have studied new querying modalities like visual [4, 8], touch-based [3, 7], and natural language interfaces (NLIs) [5, 6], especially for constrained querying environments such as tablets, smartphones, and conversational assistants. The commands given by the user are then translated to the Structured Query Language (SQL). But conspicuous by its absence is a speech-driven interface for regular SQL or other structured querying. One might wonder: Why dictate structured queries and not just use NLIs or visual interfaces? From a practical standpoint, many users, including in the C-suite, enterprise, Web, and other domains are already familiar with SQL (even if only a subset of it) and use it routinely. A spoken SQL interface could help them speed up query specification, especially in constrained settings such as smartphones and tablets, where typing SQL would be painful. More fundamentally, there is a trade-off inherent in any query interface, as illustrated in Figure 1(A).",
  "subtype": "SIGMOD Research Competition",
  "authors": [
    {
      "name": "Vraj Shah",
      "affiliation": "University of California, San Diego"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Research Competition_01"
},
"SIGMOD_Research Competition_02": {
  "title": "Fingerprints for Compressed Columnar Data Search",
  "abstract": "To enhance performance in main memory databases, compression techniques have been suggested to keep large volume of data in-memory, as opposed to loading data on demand from slower media storage. High compression ratio, however, comes with both memory and performance overhead for queries; packed data needs to be decompressed into vectors before applying optimized scan algorithms. In this work, we propose data summaries at column block level. Our preliminary experimental studies on TPC-H data confirm that under the same memory budget used for MinMax synopsis, our block headers can lower the false positive rates by up to 30% for compressed data scans and can reduce the overhead of employing advanced compression schemes.",
  "subtype": "SIGMOD Research Competition",
  "authors": [
    {
      "name": "Carmen Kwan",
      "affiliation": "University of Waterloo"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Research Competition_02"
},
"SIGMOD_Research Competition_03": {
  "title": "CAvSAT: A System for Query Answering over Inconsistent Databases",
  "abstract": "Managing inconsistencies in databases is an old, but recurring, problem. An inconsistent database is a database that violates one or more integrity constraints. In the real-world, inconsistent databases arise in several different contexts, including data warehousing and information integration. The framework of database repairs and consistent query answering (CQA) is a principled way of handling inconsistencies. In this work, we propose a novel approach that has a potential to build a comprehensive and scalable CQA system. We report preliminary experimental results on a prototype CQA system CAvSAT (Consistent Answering via Satisfiability), implemented using this approach.",
  "subtype": "SIGMOD Research Competition",
  "authors": [
    {
      "name": "Akhil Dixit",
      "affiliation": "University of California, Santa Cruz"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Research Competition_03"
},
"SIGMOD_Research Competition_05": {
  "title": "Scalable Reservoir Sampling on Many-Core CPUs",
  "abstract": "Database systems need to be able to convert queries to efficient execution plans. As recent research has shown, correctly estimating cardinalities of subqueries is an important factor in the efficiency of the resulting plans [7, 8]. Many algorithms have been proposed in literature that utilize a random sample to estimate cardinalities [6, 9, 13]. Thus, some modern database systems choose to store a materialized uniformly random sample for their relations [3, 6]. Such samples are built and refreshed when statistics are gathered, by loading uniformly random tuples from the relation in disk using random IO.",
  "subtype": "SIGMOD Research Competition",
  "authors": [
    {
      "name": "Altan Birler",
      "affiliation": "Technische Universität München"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Research Competition_05"
},
"SIGMOD_Research Competition_06": {
  "title": "LSM-Trees and B-Trees: The Best of Both Worlds",
  "abstract": "LSM-Trees and B-Trees are the two primary data structures used as storage engines in modern key-value (KV) stores. These two structures are optimal for different workloads; LSM-Trees perform better on update queries, whereas B-Trees are preferable for short range lookups. KV stores today use one or the other. However, for modern applications with increasingly diverse workloads, limiting KV stores to utilize only one of the two designs leads to a significant loss in performance. We propose a novel method of online transitioning a KV store from an LSM-Tree to a B-Tree and vice versa. This allows KV stores to smoothly adapt to changing workloads and use the optimal data structure as the workload changes.",
  "subtype": "SIGMOD Research Competition",
  "authors": [
    {
      "name": "Varun Jain",
      "affiliation": "Harvard University"
    },
    {
      "name": "James Lennon",
      "affiliation": "Harvard University"
    },
    {
      "name": "Harshita Gupta",
      "affiliation": "Harvard University"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Research Competition_06"
},
"SIGMOD_Research Competition_08": {
  "title": "Generating Selective Filters for Access Method and PhysicalDesign Evaluation",
  "abstract": "It is a challenge for researchers and system developers to evaluate the impacts of new access methods and physical designs on query performance. One can evaluate these by executing selective filters over differing data distributions. For example, an index lookup may give better performance than a full table scan for a highly selective query [4]. However, generating a filter workload on nonsynthetic datasets to evaluate new techniques currently involves manually coming up with workloads of varying selectivities, which can be cumbersome. Automatically generating workloads with given selectivities over any dataset can facilitate a systematic study of performance of data storage and query optimization methods that also allows researchers to leverage interesting datasets. Toward this goal, this paper describes a new query generation method that, given a table T in a database D, and a selectivity constraint (L, R), where 0 ? L < R ? 1, generates filter queries (i.e., queries with predicates in WHERE-clauses) whose output size is between L ? |T | and R ? |T |. We first show that generating filter queries with selectivities satisfying the given constraint requires solving the subset sum problem for each integral value in the range L ? |T | and R ? |T |. We then present a polynomial-time heuristic for the same and discuss the performance of the heuristic on a large data set.",
  "subtype": "SIGMOD Research Competition",
  "authors": [
    {
      "name": "Pranav Subramaniam",
      "affiliation": "University of Chicago"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Research Competition_08"
},
"SIGMOD_Research Competition_10": {
  "title": "Interactive Visualization For Big Spatial Data",
  "abstract": "The significance of spatial data is undeniable in present world. Starting from satellite data to GPS locations, from Facebook tags to Yelp check-ins, spatial data has become an intricate part of our daily life. Interactive visualization of these datasets can be of immense help to the scientific community for exploratory analytics which in turn helps to identify unique patterns and trends.",
  "subtype": "SIGMOD Research Competition",
  "authors": [
    {
      "name": "Saheli Ghosh",
      "affiliation": "University of California, Riverside"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Research Competition_10"
},
"SIGMOD_Research Competition_12": {
  "title": "Learning to Generate Questions with Adaptive Copying Neural Networks",
  "abstract": "Automatic question generation is an important problem in natural language processing. In this paper, we propose a novel adaptive copying recurrent neural network model to tackle the problem of question generation from sentences and paragraphs. The proposed model adds a copying mechanism component onto a bidirectional LSTM architecture to generate more suitable questions adaptively from the input data. Our experimental results show the proposed model can outperform the state-of-the-art question generation methods in terms of BLEU and ROUGE evaluation scores.",
  "subtype": "SIGMOD Research Competition",
  "authors": [
    {
      "name": "Xinyuan Lu",
      "affiliation": "Carleton University"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Research Competition_12"
},
"SIGMOD_Research Competition_13": {
  "title": "Query-Driven Learning for Next Generation Predictive Modeling & Analytics",
  "abstract": "As data-size is increasing exponentially, new paradigm shifts have to emerge allowing fast exploitation of data by every- body. Large-scale predictive analytics is restricted to wealthy organizations as small-scale enterprises (SMEs) struggle to compete and are inundated by the sheer monetary cost of either procuring data infrastructures or analyzing datasets over the Cloud. The aim of this work is to study mechanisms which can democratize analytics, in the sense of making them affordable, while at the same time ensuring high efficiency, scalability, and accuracy. The crux of this proposal lies in developing query-driven solutions that can be used off the Cloud thus minimizing costs. Our query-driven approach will learn and adapt on-the-fly machine learning models, based solely on query-answer interactions, which can be used for answering analytical queries. In this abstract we describe the methodology followed for the implementation and evaluation of the system designed.",
  "subtype": "SIGMOD Research Competition",
  "authors": [
    {
      "name": "Fotis Savva",
      "affiliation": "University of Glasgow"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Research Competition_13"
},
"SIGMOD_Research Competition_14": {
  "title": "Answering Range Queries Under Local Differential Privacy",
  "abstract": "Counting the fraction of a population having an input within a specified interval i.e. range count query is a fundamental database operation. Range count queries can also be used to compute other interesting statistics such as quantiles. The framework of differential privacy [6] (DP) is becoming a standard for privacy-preserving data analysis [1]. While many works address the problem of range counting queries in the trusted aggregation model, surprisingly, this problem has not been addressed specifically under untrusted aggregation (local DP [10]). In this work we study the problem of answering 1?dimensional range count queries under the constraint of LDP.",
  "subtype": "SIGMOD Research Competition",
  "authors": [
    {
      "name": "Tejas Kulkarni",
      "affiliation": "University of Warwick"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Research Competition_14"
},
"SIGMOD_Research Competition_15": {
  "title": "Helios: An Adaptive and Query Workload-driven Partitioning Framework for Distributed Graph Stores",
  "abstract": "In this paper, we present a novel adaptive and workload-driven partitioning framework, named Helios, aiming to achieve low-latency and high-throughput online queries in distributed graph stores. As each workload typically contains popular or similar queries, our partitioning method uses the existing workload to capture active vertices and edges which are frequently visited and traversed respectively. This information is used to heuristically improve the quality of partitions either by avoiding the concentration of active vertices in a few partitions proportional to their visit frequencies or by reducing the probability of the cut of active edges proportional to their traversal frequencies. In order to assess the impact of Helios on a graph store, and to show how easily the approach can be plugged on top of the system, we exploit it in a distributed, graph-based RDF store. The query engine of the store exploits Helios to reduce or eliminate data communication for future queries and balance the load among nodes. We evaluate the store by using realistic query workloads over an RDF dataset. Our results demonstrate the ability of Helios to handle varying query workloads with minimum overhead while maintaining the quality of partitions over time, along with being scalable by increasing either the data size or the number of computing nodes.",
  "subtype": "SIGMOD Research Competition",
  "authors": [
    {
      "name": "Ali Davoudian",
      "affiliation": "Carleton University"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Research Competition_15"
},
"SIGMOD_Research Competition_16": {
  "title": "Deep Query Optimization",
  "abstract": "In recent decades, we observed the rapid growth of several big data platforms. Each of them is designed for specific demands. For instance, Spark can efficiently process iterative queries, while Storm is designed for in-memory processing. In this context, the complexity of these distributed systems make it much harder to develop rigorous cost models for query optimization problems. This paper aims to address two problems of the query optimization process: cost estimation and index selection. The cost estimation problem predicts the best execution plan by measuring the cost of alternative query plans. The index selection problem determines the most suitable indexing method with a given dataset. Both problems require the development of a complex function that measures the cost or suitability of alternatives to a specific dataset. Therefore, we employ deep learning to solve those problems due to its capability of learning complicated models. We first address a simple form of cost estimation problem: selectivity estimation. Our preliminary results show that our deep learning models work efficiently with the accuracy of selectivity estimation up to 97%.",
  "subtype": "SIGMOD Research Competition",
  "authors": [
    {
      "name": "Tin Vu",
      "affiliation": "University of California, Riverside"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Research Competition_16"
},
"SIGMOD_Research Competition_17": {
  "title": "Bootstrapping an End-to-End Natural Language Interface for Databases",
  "abstract": "The ability to extract insights from data is critical for decision making. Intuitive natural language interfaces to databases provide non-technical users with an effective way to formulate complex questions and information needs efficiently and effectively. A recent trend in the area of Natural Language Interfaces for Databases (NLIDBs) has been the use of neural machine translation models to synthesize executable Structured Query Language (SQL) queries from natural language utterances. The main bottleneck in this type of approach is the acquisition of examples for training the model. Recent work has assumed access to a rich manually-curated training set for a given target database. However, this assumption ignores the large manual overhead required to curate the training set for any new database. As a result, NLIDB systems that can simply ‘plug in’ to any new database and perform effectively for naive users have yet to make their way into commercial products. Here we present DBPal, an end-to-end NLIDB framework in which a neural translation model is trained for any new database schema with minimal manual overhead. In addition to being the first off-the-shelf, neural machine translationbased system of its kind, the contributions of our project are 1) its use of a synthetic training set generation pipeline used to bootstrap a translation model without requiring manually curated data, and 2) its use of state-of-the-art multi-task and cross-domain learning techniques that increases the robustness of the translation model towards unseen linguistic phenomena in new domains. In experiments we show that our system can achieve competitive performance on the recently released benchmarks for nl-to-sql translation. Through ablation experiments we show the benefit of using cross-domain learning techniques on the performance of the system. In a user study we show that DBPal outperforms a well-known rule-based NLIDB and performs comparably to an approach using a similar neural model that relies on manually curated data.",
  "subtype": "SIGMOD Research Competition",
  "authors": [
    {
      "name": "Nathaniel Weir",
      "affiliation": "Brown University"
    },
    {
      "name": "Prasetya Utama",
      "affiliation": "TU Darmstadt"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Research Competition_17"
},
"SIGMOD_Research Competition_18": {
  "title": "Recommending Deployment Strategies in Crowdsourcing Platforms",
  "abstract": "We initiate the study of how to recommend deployment strategies to the task designers in crowdsourcing platforms. The work proposes the first ever optimization based formalism of the task deployment strategies based on different parameters that the task designers have in mind during deployment and present principled algorithms that are designed using computational geometry techniques.",
  "subtype": "SIGMOD Research Competition",
  "authors": [
    {
      "name": "Dong Wei",
      "affiliation": "New Jersey Institute of Technology"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Research Competition_18"
},
"SIGMOD_Research Competition_19": {
  "title": "Towards Understanding Data Analysis Workflows using a Large Notebook Corpus",
  "abstract": "The advent of big data analysis as a profession as well as a hobby has brought an increase in novel forms of data exploration and analysis, particularly ad-hoc analysis. Analysis of raw datasets using frameworks such as pandas and R have become very popular[8]. Typically these types of workflows are geared towards ingesting and transforming data in an exploratory fashion in order to derive knowledge while minimizing time-to-insight. However, there exists very little work studying usability and performance concerns of such unstructured workflows.",
  "subtype": "SIGMOD Research Competition",
  "authors": [
    {
      "name": "Mohammed Suhail Rehman",
      "affiliation": "University of Chicago"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Research Competition_19"
},
"SIGMOD_Research Competition_20": {
  "title": "Arachnid: Generalized Visual Data Cleaning",
  "abstract": "Data cleaning is an inherently exploratory and visual process. Visualizations help analysts spot errors or surprising patterns/relationships that would otherwise go unnoticed. Once identified, the user will want to execute transformations in an attempt to correct these errors. However, alternating between the contexts of transforming and visualizing the data can be tedious. As a result, there is a need for data to be visualized and cleaned on the fly. Current solutions address this issue by allowing users to specify data cleaning transformations through a limited set of interactions that can directly manipulate visualizations pre-defined by the cleaning system itself. These visualizations are either generated as a table (OpenRefine, Wrangler, Microsoft Excel), or as bar graphs (Tableau Prep). By mapping a narrow set of mouse-based interactions to data cleaning specifications, these systems allow users to intuitively and quickly transform data within a constrained set of use cases. However, in order to gain a comprehensive visual understanding of the data and to quickly clean it for additional use cases, analysts often need to generate multiple visualizations from a set of common types and interactively execute data transformations beyond those supported by current cleaning software. Thus, we propose Arachnid as a novel system that builds upon existing work in generalized selection and direct manipulation to introduce a model for translating mouse-based interactions on common types of user-defined visualizations into an enhanced set of data cleaning transformations.",
  "subtype": "SIGMOD Research Competition",
  "authors": [
    {
      "name": "Conder Shou",
      "affiliation": "Columbia University"
    },
    {
      "name": "Amita Shukla",
      "affiliation": "Columbia University"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Research Competition_20"
},
"SIGMOD_Tutorial_01": {
  "title": "Data Pipelines for User Group Analytics",
  "abstract": "User data is becoming increasingly available in various domains ranging from the social Web to electronic patient health records (EHRs). User data is characterized by a combination of demographics (e.g., age, gender, life status) and user actions (e.g., posting a tweet, following a diet). Domain experts rely on user data to conduct large-scale population studies. Information consumers, on the other hand, rely on user data for routine tasks such as finding a book club and getting advice from look-alike patients. User data analytics is usually based on identifying group-level behaviors such as “teenage females who watch Titanic” and “old male patients in Paris who suffer from Bronchitis.” In this tutorial, we review data pipelines for User Group Analytics (UGA). These pipelines admit raw user data as input and return insights in the form of user groups. We review research on UGA pipelines and discuss approaches and open challenges for discovering, exploring, and visualizing user groups. Throughout the tutorial, we will illustrate examples in two key domains: “the social Web” and “health-care”.",
  "subtype": "SIGMOD Tutorial",
  "authors": [
    {
      "name": "Behrooz Omidvar-Tehrani",
      "affiliation": "University of Grenoble Alpes"
    },
    {
      "name": "Sihem Amer-Yahia",
      "affiliation": "University of Grenoble Alpes and CNRS"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Tutorial_01"
},
"SIGMOD_Tutorial_03": {
  "title": "Towards Democratizing Relational Data Visualization",
  "abstract": "The problem of data visualization is to transform data into a visual context such that people can easily understand the significance of data. Nowadays, data visualization becomes especially important, because it is the de facto standard for modern business intelligence and successful data science. This tutorial will cover three specific topics: visualization languages define how the users can interact with various visualization systems; efficient data visualization processes the data and produces visualizations based on well-specified user queries; smart data visualization recommends data visualizations based on underspecified user queries. In this tutorial, we will go logically through these prior art, paying particular attentions on problems that may attract the interest from the database community.",
  "subtype": "SIGMOD Tutorial",
  "authors": [
    {
      "name": "Nan Tang",
      "affiliation": "Qatar Foundation"
    },
    {
      "name": "Eugene Wu",
      "affiliation": "Columbia University"
    },
    {
      "name": "Guoliang Li",
      "affiliation": "Tsinghua University"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Tutorial_03"
},
"SIGMOD_Tutorial_05": {
  "title": "Database and Distributed Computing Foundations of Blockchains",
  "abstract": "The uprise of Bitcoin and other peer-to-peer cryptocurrencies has opened many interesting and challenging problems in cryptography, distributed systems, and databases. The main underlying data structure is blockchain, a scalable fully replicated structure that is shared among all participants and guarantees a consistent view of all user transactions by all participants in the system. In this tutorial, we discuss the basic protocols used in blockchain, and elaborate on its main advantages and limitations. To overcome these limitations, we provide the necessary distributed systems background in managing large scale fully replicated ledgers, using Byzantine Agreement protocols to solve the consensus problem. Finally, we expound on some of the most recent proposals to design scalable and efficient blockchains in both permissionless and permissioned settings. The focus of the tutorial is on the distributed systems and database aspects of the recent innovations in blockchains.",
  "subtype": "SIGMOD Tutorial",
  "authors": [
    {
      "name": "Sujaya Maiyya",
      "affiliation": "University of California, Santa Barbara"
    },
    {
      "name": "Victor Zakhary",
      "affiliation": "University of California, Santa Barbara"
    },
    {
      "name": "Mohammad Javad Amiri",
      "affiliation": "University of California, Santa Barbara"
    },
    {
      "name": "Divyakant Agrawal",
      "affiliation": "University of California, Santa Barbara"
    },
    {
      "name": "Amr El Abbadi",
      "affiliation": "University of California, Santa Barbara"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Tutorial_05"
},
"SIGMOD_Tutorial_06": {
  "title": "Exploring the Data Wilderness through Examples",
  "abstract": "Exploration is one of the primordial ways to accrue knowledge about the world and its nature. As we accumulate, mostly automatically, data at unprecedented volumes and speed, our datasets have become complex and hard to understand. In this context exploratory search provides a handy tool for progressively gather the necessary knowledge by starting from a tentative query that hopefully leads to answers at least partially relevant and that can provide cues about the next queries to issue. Recently, we have witnessed a rediscovery of the so-called example-based methods, in which the user or the analyst circumvent query languages by using examples as input. This shift in semantics has led to a number of methods receiving as query a set of example members of the answer set. The search system then infers the entire answer set based on the given examples and any additional information provided by the underlying database. In this tutorial, we present an excursus over the main example-based methods for exploratory analysis, show techniques tailored to different data types, and provide a unifying view of the problem. We show how different data types require different techniques, and present algorithms that are specifically designed for relational, textual, and graph data.",
  "subtype": "SIGMOD Tutorial",
  "authors": [
    {
      "name": "Davide Mottin",
      "affiliation": "Aarhus University"
    },
    {
      "name": "Matteo Lissandrini",
      "affiliation": "Aalborg University"
    },
    {
      "name": "Yannis Velegrakis",
      "affiliation": "Utrecht University"
    },
    {
      "name": "Themis Palpanas",
      "affiliation": "Paris Descartes University"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Tutorial_06"
},
"SIGMOD_Tutorial_08": {
  "title": "Schemas and Types for JSON Data: From Theory to Practice",
  "abstract": "The last few years have seen the fast and ubiquitous diffusion of JSON as one of the most widely used formats for publishing and interchanging data, as it combines the flexibility of semistructured data models with well-known data structures like records and arrays. The user willing to effectively manage JSON data collections can rely on several schema languages, like JSON Schema, JSound, and Joi, as well as on the type abstractions offered by modern programming and scripting languages like Swift or TypeScript. The main aim of this tutorial is to provide the audience (both researchers and practitioners) with the basic notions for enjoying all the benefits that schema and types can offer while processing and manipulating JSON data. This tutorial focuses on four main aspects of the relation between JSON and schemas: (1) we survey existing schema language proposals and discuss their prominent features; (2) we analyze tools that can infer schemas from data, or that exploit schema information for improving data parsing and management; and (3) we discuss some open research challenges and opportunities related to JSON data.",
  "subtype": "SIGMOD Tutorial",
  "authors": [
    {
      "name": "Mohamed-Amine Baazizi",
      "affiliation": "Sorbonne Université, LIP6 UMR 7606"
    },
    {
      "name": "Dario Colazzo",
      "affiliation": "Université Paris-Dauphine, PSL Research University"
    },
    {
      "name": "Giorgio Ghelli",
      "affiliation": "Università di Pisa"
    },
    {
      "name": "Carlo Sartiani",
      "affiliation": "Università della Basilicata"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Tutorial_08"
},
"SIGMOD_Tutorial_09": {
  "title": "Classical and Contemporary Approaches to Big Time Series Forecasting",
  "abstract": "Time series forecasting is a key ingredient in the automation and optimization of business processes: in retail, deciding which products to order and where to store them depends on the forecasts of future demand in different regions; in cloud computing, the estimated future usage of services and infrastructure components guides capacity planning; and workforce scheduling in warehouses and factories requires forecasts of the future workload. Recent years have witnessed a paradigm shift in forecasting techniques and applications, from computer-assisted model- and assumption-based to data-driven and fully-automated. This shift can be attributed to the availability of large, rich, and diverse time series corpora and result in a set of challenges that need to be addressed such as the following. How can we build statistical models to efficiently and effectively learn to forecast from large and diverse data sources? How can we leverage the statistical power of 'similar' time series to improve forecasts in the case of limited observations? What are the implications for building forecasting systems that can handle large data volumes? The objective of this tutorial is to provide a concise and intuitive overview of the most important methods and tools available for solving large-scale forecasting problems. We review the state of the art in three related fields: (1) classical modeling of time series, (2) scalable tensor methods, and (3) deep learning for forecasting. Further, we share lessons learned from building scalable forecasting systems. While our focus is on providing an intuitive overview of the methods and practical issues which we will illustrate via case studies, we also present some technical details underlying these powerful tools.",
  "subtype": "SIGMOD Tutorial",
  "authors": [
    {
      "name": "Christos Faloutsos",
      "affiliation": "Carnegie Mellon University & Amazon"
    },
    {
      "name": "Jan Gasthaus",
      "affiliation": "AWS AI Labs"
    },
    {
      "name": "Tim Januschowski",
      "affiliation": "AWS AI Labs"
    },
    {
      "name": "Yuyang Wang",
      "affiliation": "AWS AI Labs"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Tutorial_09"
},
"SIGMOD_Tutorial_10": {
  "title": "From Auto-tuning One Size Fits All to Self-designed and Learned Data-intensive Systems",
  "abstract": "We survey new opportunities to design data systems, data structures and algorithms that can adapt to both data and query workloads. Data keeps growing, hardware keeps changing and new applications appear ever more frequently. One size does not fit all, but data-intensive applications would like to balance and control memory requirements, read costs, write costs, as well as monetary costs on the cloud. This calls for tailored data systems, storage, and computation solutions that match the exact requirements of the scenario at hand. Such systems should be 'synthesized' quickly and nearly automatically, removing the human system designers and administrators from the loop as much as possible to keep up with the quick evolution of applications and workloads. In addition, such systems should 'learn' from both past and current system performance and workload patterns to keep adapting their design. We survey new trends in 1) self-designed, and 2) learned data systems and how these technologies can apply to relational, NoSQL, and big data systems as well as to broad data science applications. We focus on both recent research advances and practical applications of this technology, as well as numerous open research opportunities that come from their fusion. We specifically highlight recent work on data structures, algorithms, and query optimization, and how machine learning inspired designs as well as a detailed mapping of the possible design space of solutions can drive innovation to create tailored systems. We also position and connect with past seminal system designs and research in auto-tuning, modular/extensible, and adaptive data systems to highlight the new challenges as well as the opportunities to combine past and new technologies.",
  "subtype": "SIGMOD Tutorial",
  "authors": [
    {
      "name": "Stratos Idreos",
      "affiliation": "Harvard University"
    },
    {
      "name": "Tim Kraska",
      "affiliation": "MIT"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Tutorial_10"
},
"SIGMOD_Workshop_10": {
  "title": "aIDM 19: Second International Workshop on Exploiting Artificial Intelligence Techniques for Data Management",
  "abstract": "The goal of the workshop is to take a holistic view of various AI technologies and investigate how they can be applied to different component of an end-to-end data management pipeline. Special emphasis would be given to how AI techniques in data management systems, e.g., enabling natural language interfaces to relational databases and applying machine learning techniques for query optimizations. However, a lot more needs to done to fully exploit the power of AI for data management workloads.",
  "subtype": "SIGMOD Workshop",
  "authors": [
    {
      "name": "Rajesh Bordawekar",
      "affiliation": "IBM T.J. Watson Research Center"
    },
    {
      "name": "Oded Shmueli",
      "affiliation": "Computer Science Department, Technion"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Workshop_10"
},
"TODS_Poster_1": {
  "title": "Representations and Optimizations for Embedded Parallel Dataflow Languages",
  "acm_link": "https://doi.org/10.1145/3281629",
  "abstract": "Parallel dataflow engines such as Apache Hadoop, Apache Spark, and Apache Flink are an established alternative to relational databases for modern data analysis applications. A characteristic of these systems is a scalable programming model based on distributed collections and parallel transformations expressed by means of second-order functions such as map and reduce. Notable examples are Flink’s DataSet and Spark’s RDD programming abstractions. These programming models are realized as EDSLs—domain specific languages embedded in a general-purpose host language such as Java, Scala, or Python. This approach has several advantages over traditional external DSLs such as SQL or XQuery. First, syntactic constructs from the host language (e.g., anonymous functions syntax, value definitions, and fluent syntax via method chaining) can be reused in the EDSL. This eases the learning curve for developers already familiar with the host language. Second, it allows for seamless integration of library methods written in the host language via the function parameters passed to the parallel dataflow operators. This reduces the effort for developing analytics dataflows that go beyond pure SQL and require domain-specific logic.  At the same time, however, state-of-the-art parallel dataflow EDSLs exhibit a number of shortcomings. First, one of the main advantages of an external DSL such as SQL—the high-level, declarative Select-From-Where syntax—is either lost completely or mimicked in a non-standard way. Second, execution aspects such as caching, join order, and partial aggregation have to be decided by the programmer. Optimizing them automatically is very difficult due to the limited program context available in the intermediate representation of the DSL.  In this article, we argue that the limitations listed above are a side effect of the adopted type-based embedding approach. As a solution, we propose an alternative EDSL design based on quotations. We present a DSL embedded in Scala and discuss its compiler pipeline, intermediate representation, and some of the enabled optimizations. We promote the algebraic type of bags in union representation as a model for distributed collections and its associated structural recursion scheme and monad as a model for parallel collection processing. At the source code level, Scala’s comprehension syntax over a bag monad can be used to encode Select-From-Where expressions in a standard way. At the intermediate representation level, maintaining comprehensions as a first-class citizen can be used to simplify the design and implementation of holistic dataflow optimizations that accommodate for nesting and control-flow. The proposed DSL design therefore reconciles the benefits of embedded parallel dataflow DSLs with the declarativity and optimization potential of external DSLs like SQL.",
  "subtype": "TODS Poster",
  "authors": [
    {
      "name": "Alexander Alexandrov",
      "affiliation": "TU Berlin"
    },
    {
      "name": "Georgi Krastev",
      "affiliation": "TU Berlin"
    },
    {
      "name": "Volker Markl",
      "affiliation": "TU Berlin"
    }
   ],
  "type": "TODS",
  "id": "TODS_Poster_1"
},
"TODS_Poster_2": {
  "title": "A Survey of Spatial Crowdsourcing",
  "acm_link": "https://doi.org/10.1145/3291933",
  "abstract": "Widespread use of advanced mobile devices has led to the emergence of a new class of crowdsourcing called spatial crowdsourcing. Spatial crowdsourcing advances the potential of a crowd to perform tasks related to real-world scenarios involving physical locations, which were not feasible with conventional crowdsourcing methods. The main feature of spatial crowdsourcing is the presence of spatial tasks that require workers to be physically present at a particular location for task fulfillment. Research related to this new paradigm has gained momentum in recent years, necessitating a comprehensive survey to offer a bird’s-eye view of the current state of spatial crowdsourcing literature. In this article, we discuss the spatial crowdsourcing infrastructure and identify the fundamental differences between spatial and conventional crowdsourcing. Furthermore, we provide a comprehensive view of the existing literature by introducing a taxonomy, elucidate the issues/challenges faced by different components of spatial crowdsourcing, and suggest potential research directions for the future.",
  "subtype": "TODS Poster",
  "authors": [
    {
      "name": "Srinivasa Raghavendra",
      "affiliation": "Aalborg University"
    },
    {
      "name": "Bhuvan Gummidi",
      "affiliation": "Aalborg University"
    },
    {
      "name": "Xike Xie",
      "affiliation": "University of Science and Technology of China"
    },
    {
      "name": "Torben Bach Pedersen",
      "affiliation": "Aalborg University"
    }
   ],
  "type": "TODS",
  "id": "TODS_Poster_2"
},
"TODS_Poster_3": {
  "title": "K-Regret Queries Using Multiplicative Utility Functions",
  "acm_link": "https://doi.org/10.1145/3230634",
  "abstract": "The k-regret query aims to return a size-k subset S of a database D such that, for any query user that selects a data object from this size-k subset S rather than from database D, her regret ratio is minimized. The regret ratio here is modeled by the relative difference in the optimality between the locally optimal object in S and the globally optimal object in D. The optimality of a data object in turn is modeled by a utility function of the query user. Unlike traditional top-k queries, the k-regret query does not minimize the regret ratio for a specific utility function. Instead, it considers a family of infinite utility functions F, and aims to find a size-k subset that minimizes the maximum regret ratio of any utility function in F.  Studies on k-regret queries have focused on the family of additive utility functions, which have limitations in modeling individuals’ preferences and decision-making processes, especially for a common observation called the diminishing marginal rate of substitution (DMRS). We introduce k-regret queries with multiplicative utility functions, which are more expressive in modeling the DMRS, to overcome those limitations. We propose a query algorithm with bounded regret ratios. To showcase the applicability of the algorithm, we apply it to a special family of multiplicative utility functions, the Cobb-Douglas family of utility functions, and a closely related family of utility functions, the Constant Elasticity of Substitution family of utility functions, both of which are frequently used utility functions in microeconomics. After a further study of the query properties, we propose a heuristic algorithm that produces even smaller regret ratios in practice. Extensive experiments on the proposed algorithms confirm that they consistently achieve small maximum regret ratios.",
  "subtype": "TODS Poster",
  "authors": [
    {
      "name": "Jianzhong Qi",
      "affiliation": "The University of Melbourne"
    },
    {
      "name": "Fei Zuo",
      "affiliation": "The University of Melbourne"
    },
    {
      "name": "Hanan Samet",
      "affiliation": "University of Maryland"
    },
    {
      "name": "Jia Cheng Yao",
      "affiliation": "The University of Melbourne"
    }
   ],
  "type": "TODS",
  "id": "TODS_Poster_3"
},
"TODS_Poster_4": {
  "title": "Historic Moments Discovery in Sequence Data.",
  "acm_link": "https://doi.org/10.1145/3276975",
  "abstract": "Many emerging applications are based on finding interesting subsequences from sequence data. Finding “prominent streaks,” a set of the longest contiguous subsequences with values all above (or below) a certain threshold, from sequence data is one of that kind that receives much attention. Motivated from real applications, we observe that prominent streaks alone are not insightful enough but require the discovery of something we coined as “historic moments” as companions. In this article, we present an algorithm to efficiently compute historic moments from sequence data. The algorithm is incremental and space optimal, meaning that when facing new data arrival, it is able to efficiently refresh the results by keeping minimal information. Case studies show that historic moments can significantly improve the insights offered by prominent streaks alone. Furthermore, experiments show that our algorithm can outperform the baseline in both time and space.",
  "subtype": "TODS Poster",
  "authors": [
    {
      "name": "Ran Bai",
      "affiliation": "The Hong Kong Polytechnic University"
    },
    {
      "name": "Wing-Kai Hon",
      "affiliation": "National Tsing Hua University, Taiwan"
    },
    {
      "name": "Eric Lo",
      "affiliation": "Chinese University of Hong Kong"
    },
    {
      "name": "Zhian He",
      "affiliation": "University of Hong Kong"
    },
    {
      "name": "Kenny Q. Zhu",
      "affiliation": "Shanghai Jiao Tong University"
    }
   ],
  "type": "TODS",
  "id": "TODS_Poster_4"
},
"SIGMOD_Posters_1": {
  "title": "One poster for each SIGMOD and PODS paper presented on Tuesday. Plus 5 Programming Contest demos.",
  "abstract": "",
  "subtype": "SIGMOD Posters",
  "authors": [
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Posters_1"
},
"SIGMOD_Posters_2": {
  "title": "One poster for each SIGMOD and PODS paper presented on Wednesday.",
  "abstract": "",
  "subtype": "SIGMOD Posters",
  "authors": [
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Posters_2"
},
"SIGMOD_Posters_3": {
  "title": "One poster for each SIGMOD paper presented on Thursday.",
  "abstract": "",
  "subtype": "SIGMOD Posters",
  "authors": [
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Posters_3"
},
"SIGMOD_Workshop_01": {
  "title": "aiDM 2019: the 2nd International Workshop on Exploiting Artificial Intelligence Techniques for Data Management",
  "acm_link": "http://www.aidm-conf.org/",
  "abstract": "The goal of the workshop is to take a holistic view of various AI technologies and investigate how they can be applied to different component of an end-to-end data management pipeline. Special emphasis would be given to how AI techniques could be used for enhancing user experience by reducing complexity in tools, or providing newer insights, or providing better user interfaces.",
  "subtype": "SIGMOD Workshop",
  "authors": [
    {
      "name": "Rajesh Bordawekar",
      "affiliation": "IBM T. J. Watson Research Center"
    },
    {
      "name": "Oded Shmueli",
      "affiliation": "Computer Science Department, Technion"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Workshop_01"
},
"SIGMOD_Workshop_02": {
  "title": "DaMoN 2019: the 15th International Workshop on Data Management on New Hardware",
  "acm_link": "https://sites.google.com/view/damon2019/home-damon-2019",
  "abstract": "The 15th International Workshop on Data Management on New Hardware is held in Amsterdam, The Netherlands on July 1th, 2019, co-located with the ACM Conference on Management of Data (SIGMOD). The focus of this workshop is to strengthen the communication between the database community and broader computer systems communities, specifically the computer architecture, compiler, operating systems, and storage communities.",
  "subtype": "SIGMOD Workshop",
  "authors": [
    {
      "name": "Thomas Neumann",
      "affiliation": "Technische Universität München"
    },
    {
      "name": "Ken Salem",
      "affiliation": "University of Waterloo"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Workshop_02"
},
"SIGMOD_Workshop_03": {
  "title": "DEEM 2019: Workshop on Data Management for End-to-End Machine Learning",
  "acm_link": "http://deem-workshop.org/",
  "abstract": "The DEEM workshop brings together researchers and practitioners at the intersection of applied machine learning, data management and systems research, with the goal to discuss the arising data management issues in machine learning application scenarios.",
  "subtype": "SIGMOD Workshop",
  "authors": [
    {
      "name": "Sebastian Schelter",
      "affiliation": "New York University"
    },
    {
      "name": "Neoklis Polyzotis",
      "affiliation": "Google"
    },
    {
      "name": "Manasi Vartak",
      "affiliation": "Massachusetts Institute of Technology"
    },
    {
      "name": "Stephan Seufert",
      "affiliation": "Amazon Research"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Workshop_03"
},
"SIGMOD_Workshop_04": {
  "title": "DSMM 2019: the 5th Workshop on Data Science for Macro-modeling with Financial and Economic Datasets",
  "acm_link": "https://wiki.umiacs.umd.edu/clip/datascience/index.php/DSMM:_Data_Science_for_Macro-Modeling_with_Financial_and_Economic_Datasets",
  "abstract": "DSMM 2019 will explore the challenges of macro-modeling with financial and economic datasets. The workshop will also showcase the 2019 Financial Entity Identification and Information Integration (FEIII) Challenge which involves two challenge tasks, one over small business data and the other over customs data for shipping manifests.",
  "subtype": "SIGMOD Workshop",
  "authors": [
    {
      "name": "Douglas Burdick",
      "affiliation": "IBM Almaden Research Center"
    },
    {
      "name": "Rajasekar Krishnamurthy",
      "affiliation": "IBM T. J. Watson Research Center"
    },
    {
      "name": "Louiqa Raschid",
      "affiliation": "University of Maryland"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Workshop_04"
},
"SIGMOD_Workshop_05": {
  "title": "GRADES-NDA 2019: Joint International Workshop on Graph Data Management Experiences & Systems and Network Data Analytics",
  "acm_link": "https://sites.google.com/site/gradesnda2019/",
  "abstract": "GRADES-NDA 2019 is the second joint meeting of the GRADES and NDA workshops, which were each independently organized at previous SIGMOD-PODS meetings, GRADES since 2013 and NDA since 2016. The focus of GRADES-NDA is the application areas, usage scenarios, and open challenges in managing large-scale graph-shaped data. To summarize, GRADES-NDA aims to present technical contributions inside graph, RDF, and other data management systems on massive graphs.",
  "subtype": "SIGMOD Workshop",
  "authors": [
    {
      "name": "Akhil Arora",
      "affiliation": "EPFL"
    },
    {
      "name": "Arnab Bhattacharya",
      "affiliation": "IIT Kanpur"
    },
    {
      "name": "George Fletcher",
      "affiliation": "TU Eindhoven"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Workshop_05"
},
"SIGMOD_Workshop_06": {
  "title": "HILDA 2019: the International Workshop on Human-In-the-Loop Data Analytics",
  "acm_link": "http://hilda.io/2019/",
  "abstract": "The Human In the Loop Data Analytics (HILDA) workshop aims to foster interdisciplinary efforts that tackle important challenges in better supporting humans in the loop in the context of data-intensive computations, such as interactive data exploration, integration, analytics, and machine learning. Over the past several years, HILDA has brought together DB researchers interested in the distinctive ways that people impact data management tasks, as well as like-minded researchers in other communities, such as Information Visualization, Data Mining/Machine Learning, and HCI. The work presented at HILDA covers a broad range of topics, from algorithmic, interface, and system design to the user's cognitive, physical, and goal-seeking perspectives when managing and exploring data as well as notions of approximation/prediction. This year, we continued to encourage submissions for initial ideas and visions, early reports of work in progress, as well as reflections on completed projects.",
  "subtype": "SIGMOD Workshop",
  "authors": [
    {
      "name": "Leilani Battle",
      "affiliation": "University of Maryland"
    },
    {
      "name": "Surajit Chaudhuri",
      "affiliation": "Microsoft"
    },
    {
      "name": "Arnab Nandi",
      "affiliation": "The Ohio State University"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Workshop_06"
},
"SIGMOD_Workshop_07": {
  "title": "SBD 2019: the Fourth International Workshop on Semantic Big Data",
  "acm_link": "https://www.ifis.uni-luebeck.de/~groppe/sbd",
  "abstract": "The International Workshop on Semantic Big Data (SBD) is already in the fourth year in conjunction with the ACM SIGMOD Conference. By focusing on topics related to Semantic Web and Big Data, it highlights these important research areas to all participants of the SBD workshop and the main conference. Hence, the workshop offers an ideal forum for open discussions and establishment of professional relationships between academic researchers and industrial members of the Semantic Web and database communities.",
  "subtype": "SIGMOD Workshop",
  "authors": [
    {
      "name": "Sven Groppe",
      "affiliation": "University of Lübeck"
    },
    {
      "name": "Le Gruenwald",
      "affiliation": "University of Oklahoma"
    }
   ],
  "type": "SIGMOD",
  "id": "SIGMOD_Workshop_07"
},
"PODS_Special_Event_for_Phokion_Kolaitis": {
  "title": "Phokion Kolaitis Special Event",
  "acm_link": "https://sigmod2019.org/Kolaitis_Event",
  "abstract": "An event in honor of Phokion Kolaitis will take place on Sunday 30 June 2019. We will celebrate Phokion for his fundamental contributions of lasting value to the principles of database systems and computational logic, as well as to mathematical logic and computer science at large. The event comprises talks by some of his closest collaborators.",
  "subtype": "PODS Special_Event",
  "authors": [
    {
      "name": "Georg Gottlob",
      "affiliation": "University of Oxford"
    },
    {
      "name": "Wang-Chiew Tan",
      "affiliation": "Megagon Labs"
    }
   ],
  "type": "PODS",
  "id": "PODS_Special_Event_for_Phokion_Kolaitis"
},
"ADS__Event": {
  "title": "ADS Event",
  "acm_link": "https://amsterdamdatascience.nl",
  "abstract": "Amsterdam Data Science (ADS) is a virtual organization founded by CWI and the two Amsterdam universities (.vu.nl .uva.nl) to further research, education, and application of data science in the Amsterdam area. ADS regularly organizes meetups that draw researchers, educators, but most prominently practitioners. This event will bring the local ADS community into contact with data management research. This meetup will be open to the ADS community as well as to the whole SIGMOD/PODS audience.",
  "subtype": "ADS",
  "authors": [
    {
      "name": "Jeanne Kroeger",
      "affiliation": "ADS"
    }
   ],
  "type": "ADS",
  "id": "ADS__Event"
},
"LDBC_TUC_Meeting": {
  "title": "LDBC Technical User Community Meeting",
  "acm_link": "http://wiki.ldbcouncil.org/pages/viewpage.action?pageId=106233859",
  "abstract": "The Linkded Data Benchmark Council (ldbcouncil.org) is a non-profit organization furthering graph data management, whose members are primarily graph data management software companies. LDBC has defined a number of graph benchmarks (the Social Network Benchmark, and Graphalytics), but has recently also ventured in the area of collaboration towards a standard property graph data model and query language. Technical Community Meetings such as these are open to everyone (though prior registration is required, see the link). In these meetings, LDBC members present the latest developments on LDBC activities and obtain feedback from practitioners; graph data management companies present obout their products and users present about their challenges.",
  "subtype": "LDBC TUC",
  "authors": [
    {
      "name": "Peter Boncz",
      "affiliation": "LDBC (CWI)"
    },
    {
      "name": "Alastair Green",
      "affiliation": "LDBC (Neo4j)"
    }
   ],
  "type": "LDBC",
  "id": "LDBC_TUC_Meeting"
}}
